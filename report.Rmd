---
title: "Report"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide

---


```{r setup, include=FALSE}
library(tidyverse)
library(patchwork)
library(knitr)
library(dplyr)
library(gganimate)
library(gifski)
library(png)
library(plotly)
library(ggridges)
library(xgboost)
library(readr)
library(stringr)
library(caret)
library(leaps)
library(modelr)
library(pROC)
library(glmnet)
library(corrplot)
library(DiagrammeR)
knitr::opts_chunk$set(
  fig.height = 6,
  fig.width = 8,
  message = F,
  echo = T,
  warning = F
)
```

```{r, include=FALSE}
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

### Interested physiological risk factors
`ARTHRITIS`: Arthritis among adults aged >=18 Years\
`BPHIGH`: High blood pressure among adults aged >=18 Years\
`CANCER`: Cancer (excluding skin cancer) among adults aged >=18 Years\
`CASTHMA`: Asthma among adults aged >=18 Years\
`CHD`: Coronary heart disease among adults aged >=18 Years\
`COPD`: Chronic obstructive pulmonary diseases among adults aged >=18 Years\
`DEPRESSION`: Depression among adults aged >=18 Years\
`DIABETES`: Diagnosed diabetes among adults aged >=18 Years\
`HIGHCHOL`: High cholesterol among adults aged >=18 Years who have been screened in the past 5 Years\
`KIDNEY`: Chronic kidney disease among adults aged >=18 Years\
`OBESITY`: Obesity among adults aged >=18 Years

### Outcome measured
`SLEEP`: Sleeping less than 7 hours among adults aged >=18 Years

## Select interested physiological diseases and sample state
```{r import_data}
data_df = read_csv('PLACES__Census_Tract_Data__GIS_Friendly_Format___2022_release.csv') %>% 
  janitor::clean_names() %>% 
  select(county_name,total_population, state_abbr, 
         starts_with('arthritis'), starts_with('bphigh'), 
         starts_with('cancer'), starts_with('casthma'), starts_with('chd'),
         starts_with('copd'),starts_with('depression'), starts_with('diabetes'),
         starts_with('highchol'), starts_with('kidney'), starts_with('obesity'),
         starts_with('sleep')) 
data_df %>% 
  group_by(state_abbr) %>% 
  dplyr::summarize(
    number_of_regions = n(),
    population = sum(total_population)
  ) %>% 
  arrange(-population) %>% 
  head() %>% knitr::kable()
```

The dataset has `r nrow(na.omit(data_df))` observations across the country, each represents a summarized estimate of a district. We select New York state as our sample data to build the model as it has third largest population with sufficient amount of sample. 

### Extract NY state as target sample
```{r}
ny_data = data_df %>% 
  filter(state_abbr == 'NY') %>% 
  select(ends_with('prev')) %>% 
  rename_with(~str_remove(., '_crude_prev'))
```
## Show the statistics of risk factors prevalence

The following is a boxplot of the prevalences of all of the interested variables. The risk factors with highest prevalence involve screening and medication (HIGHCHOL, BPHIGH and OBESITY). The risk factors with the lowest prevlances are diseases (KIDNEY, CHD, COPD). There is also a general trend that risk factors with higher prevalences have higher variance. 

```{r}
ny_data %>% select(-sleep) %>% 
  pivot_longer(
    everything(), 
    names_to = "risk_factor",
    values_to = "prevelance"
  ) %>% 
  ggplot(aes(x = reorder(risk_factor, -prevelance), y = prevelance, 
             color = risk_factor)) + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 60, hjust = 1),
        legend.position = "none") +
  labs(
      title = "Physiological risk Factor Prevalence in NY State 2020",
      x = "Risk Factor",
      y = "Prevalence (%)",
      caption = "Data from the CDC"
    ) + 
  theme(plot.title = element_text(hjust = 0.5))
```


## Examine correlation of chosen factors
```{r}
par(mfrow = c(1, 1))
ny_data %>% select(-sleep) %>% cor() %>% corrplot(
  method = "circle", addCoef.col = "black", tl.col="black", tl.srt=90, 
  insig = "blank", diag=FALSE, number.cex = .5)
```

## Build model with elastic net

Since some of the variables have relatively high correlation with each other according to the correlation matrix, the process of model selection starts by using elastic net regression in order to remedy for potential multicollinearity. We find the best lambda with lowest se through cross validation, and then fit an elastic net model with coefficients shown below. 

```{r}
set.seed(2023)
predictors = data.matrix(ny_data %>% select(-sleep))
outcome = ny_data %>% pull(sleep)
# 5 folds cv is used 
cv_object = cv.glmnet(predictors, outcome, nfolds = 5)
# Find the best lambda with lowest se
best_lambda = cv_object %>% broom::glance() %>% pull(lambda.min)
# build elastic net model
elastic_model = glmnet(predictors, outcome, lambda = best_lambda, alpha = 0.5)
model_coef = coef.glmnet(elastic_model) %>% as.matrix() %>% as.data.frame()
model_coef[model_coef == 0] = NA
plot(cv_object)
model_coef 
```

## Build model with multiple linear regression

A multiple linear regression is then built with all of the variables selected above. Since the variable of obesity does not show significant influence in predicting prevalance of insufficient sleep, another multiple linear regression excluding obesity is built. The anova result tells that the second MLR model excluding obesity is favored because of the principle of parsimony. 

```{r}
mlr_model_1 = lm(sleep ~ arthritis + bphigh + cancer + casthma + chd + copd + depression + diabetes + highchol + kidney + obesity, data = ny_data)
# Use backward elimination based on AIC to find a mlr model 
mlr_model_2 = step(mlr_model_1, direction = "backward")
summary(mlr_model_2) %>% broom::tidy() %>% knitr::kable(digits = 3)
anova(mlr_model_2, mlr_model_1)
```

## Examine assumptions for the chosen MLR model

```{r}
par(mfrow = c(2, 2))
plot(mlr_model_2)
```

The diagnostic plots indicates that all of the assumptions of MLR below are met.\
1.Residuals are normally distributed\
2.Variance of residuals is constant across the range of variables\
3.Residuals are independent of one another

## Comparing elastic net regression model with the chosen MLR model

```{r}
# Pseudo adjusted R squared of elastic model
elastic_model %>% broom::tidy() %>% select(dev.ratio) %>% unique()
# Adjusted R squared of MLR model
summary(mlr_model_2) %>% broom::glance() %>% select(adj.r.squared)
```

The dev.ratio and adjusted r.squared of `0.892` above shows that the elastic net regression model and MLR model give equally good fit to the training data from NY state.  

```{r, echo=FALSE}
# Use the data from the rest of states as test data
nation_df = data_df %>% 
  filter(state_abbr != 'NY') %>% 
  select(state_abbr, ends_with('prev')) %>% 
  rename_with(~str_remove(., '_crude_prev')) 
nation_df_2 = nation_df %>% select(-c(state_abbr, sleep)) %>% data.matrix()
 
predict_mlr = nation_df %>% add_predictions(mlr_model_2, var = "mlr_pred") %>% select(state_abbr, sleep, mlr_pred)
predict_elastic = predict(elastic_model, nation_df_2) %>% as_tibble() %>% select(elas_pred = s0)
result_mse = cbind(predict_mlr, predict_elastic) %>% 
  mutate(
    sse_mlr = (sleep - mlr_pred)^2,
    sse_elas = (sleep - elas_pred)^2
  ) %>% 
  group_by(state_abbr) %>% 
  dplyr::summarise(
    n = n(),
    mse_mlr = sum(sse_mlr)/n,
    mse_elas = sum(sse_elas)/n
  ) %>% na.omit()
result_mse %>% 
  select(starts_with('mse')) %>% 
  pivot_longer(
    everything(), 
    names_to = "model",
    values_to = "mse",
    names_prefix = "mse_"
  ) %>% 
  ggplot(aes(x = model, y = mse, fill = model)) + 
  geom_violin() +
  geom_jitter(shape=16, position=position_jitter(0.2)) +
  scale_fill_brewer(palette="RdBu") + theme_minimal() +
   labs(
      x = "Model",
      y = "Test MSE",
      caption = "Data from the CDC"
    ) 
result_mse %>% 
  select(starts_with('mse')) %>% 
  summary() %>% 
  knitr::kable()
```

We then use the data from rest of the country to test the performance of the two models in predicting the prevalence of inadequate sleep in other `50` states. The violion plot above shows that the test mean squared error of the elastic net regression and the multiple linear regression model. The statistics show that there is no significant difference between these two models in terms of predictive performance. Therefore, we conclude that both of models could be used for regression analysis while the elastic net model is more preferred for prediction since it has smaller test MSE (minimal difference though) and retain more information of the potential risk factors. 

On the other hand, the multiple linear regression model is favored for its simplicity of interpretation. While all of the physiological risk factors except for `OBESITY` have significant correlation with the prevalence of insufficient adult sleep, `ARTHRITIS`, `BPHIGH`, `CASTHMA`, `COPD`, `DIABETES` shows a positive correlation while `CANCER`, `CHD`, `DEPRESSION`, `HIGHCHOL`, `KIDNEY` suggest a negative correlation. It is worth noticing that an 1% increase in the prevalence of most disease factors leads to a less than 1% change in the prevalence of insufficient sleep except for `CANCER` and `KIDNEY`, which lead to a drop of 1.7% and 2.7% in prevalence of insufficient sleep respectively. A reasonable explanation is that the adults suffered from cancer and kidney diseases are more likely to have challenged body functions, which force them to spend more time on sleep/lay in bed for recovery and thus lower the prevalence of short sleep. 

# Statistical Analysis
## Multiple Linear Regression (MLR) 
### Data
```{r}
df_mod = slp_df %>% 
  select(gender, age, race, education_level, income_poverty_ratio, trouble_slp, sleepy_freq, ave_slp_hr)
```

```{r}
set.seed(1)
rt = 0.8
sub = sample(1:nrow(df_mod), round(nrow(df_mod)*rt))
data_train = df_mod[sub,]
data_test = df_mod[-sub,]
dim(data_train)
dim(data_test)
```

### Modeling
```{r}
lmraw = lm(ave_slp_hr ~ ., data = data_train)
summary(lmraw)
```

The model driven by multiple linear regression is $sleep\_hour=8.09-0.31I(gender = male)-0.31I(race = Non-Hispanic Black)-0.22I(education_level = college graduate or above)-0.28I(education_level = college or AA degree)-0.05income_poverty_ratio-0.10trouble_slpyes+0.32I(sleepy_freq = 2-4 per month)+0.37I(sleepy_freq = freqnever)+0.33I(sleepy_freq = freqonce a month)$. We find it strongly relating to gender, trouble_types, income_poverty_ratio, and some levels in race, education_level and sleep frequency.

### Diagnostic plots
```{r}
par(mfrow = c(2,2))
plot(lmraw, cex.lab = 1)
```

Overall, the diagnostic plots indicates that all of the assumptions of MLR below are met.\
1.Residuals are normally distributed\
2.Variance of residuals is constant across the range of variables\
3.Residuals are independent of one another

### MLR under stepwise
```{r}
step1 = step(lmraw, direction = 'both', trace=0)
summary(step1)
```

The model after stepwise method is $sleep\_hour=8.10-0.31I(gender = male)-0.28I(race = Non-Hispanic Black)+ 0.32I(sleepy_freq = 2-4 per month)+0.37I(sleepy_freq = freqnever)+0.33I(sleepy_freq = freqonce a month)-0.05income_poverty_ratio-0.10trouble_slpyes-0.22I(education_level = college graduate or above)-0.28I(education_level = college or AA degree)$ We find it is nearly the same as previous multiple linear regression model.

### Testing
```{r}
raw_pred = predict(lmraw, newdata = data_test)

dlm =  data.frame(
    Error = c("MSE", "MAE", "RMSE"), 
    Value = c(mean((data_test$ave_slp_hr - raw_pred)^2),
              caret::MAE(data_test$ave_slp_hr, raw_pred), 
              caret::RMSE(data_test$ave_slp_hr, raw_pred)))

knitr::kable(dlm)
```

The table above shows the statistical measurement factors about the linear model.

## Logistic
```{r}
data_logtrain = data_train %>% 
  mutate(sufficient_slp = ifelse((ave_slp_hr >= 7), 1, 0)) %>% 
  select(-ave_slp_hr)
glmraw = glm(sufficient_slp ~ ., data = data_logtrain, family = binomial)
summary(glmraw)
```

The first logistic model shows the significant predictors are gender, race, education_level, trouble_types, and sleep_freq.

```{r}
step2 = step(glmraw, direction = 'both', trace=0)
summary(step2)
```

The model after stepwise method shows the significant predictors are still gender, race, education_level, trouble_types, and sleep_freq, but the estimated coefficients change.

```{r}
anova(object = glmraw,test = "Chisq")
```

We did model significance test, ANOVA, and the significant predictors for logistic regression model are the same as stepwise results.

### Testing
```{r}
data_logtest = data_test %>% 
  mutate(sufficient_slp = ifelse((ave_slp_hr >= 7), 'sufficient', 'insufficient')) %>% 
  select(-ave_slp_hr)
prob2 = round(predict(object = glmraw, newdata = data_logtest,type = "response"))
pred2 = ifelse(prob2 == 1, 'sufficient', 'insufficient')
pred2 = factor(pred2, levels = c('insufficient', 'sufficient'), order=TRUE)
tablog = table(data_logtest$sufficient_slp, pred2, dnn=c("true","pre"))
tablog
```

From the table, we find:
1. the logistic model works well on sufficient sleep. $\frac{1166}{1166+5} = 99.57\%$
2. the logistic model works not well on sufficient sleep. $\frac{6}{6+356} =  0.02\%$
3. the total accuracy of the model prediction is $\frac{1166+6}{1166+5+6+356} = 76.45\%$

### XGBoost (Optimizing MLR)
#### Data
```{r}
clafeats = c('gender', 'race', 'education_level', 'trouble_slp', 'sleepy_freq')
dums = dummyVars(~ gender + race + education_level + trouble_slp + sleepy_freq, data = df_mod)
slp_oh = as.data.frame(predict(dums, newdata = df_mod))
slp_df_new =cbind(df_mod[,-c(which(colnames(df_mod) %in% clafeats))],slp_oh)
```

```{r}
set.seed(1)
rtxg = 0.8
subxg = sample(1:nrow(slp_df_new), round(nrow(slp_df_new)*rt))
data_trainxg = slp_df_new[sub,]
data_testxg = slp_df_new[-sub,]
```

```{r}
data_trainx = data_trainxg[,-3]
data_trainy = data.frame(data_trainxg[,3])
names(data_trainy) = c('label')
data_testx = data_testxg[,-3]
data_testy = data.frame(data_testxg[,3])
names(data_testy) = c('label')
```

```{r}
dtrain = xgb.DMatrix(data = as.matrix(data_trainx), label = data_trainy$label)
dtest = xgb.DMatrix(data = as.matrix(data_testx), label = data_testy$label)
```

#### Training
```{r}
xgb1 = xgboost(data=dtrain, booster = "gblinear", max_depth=5, eta = 0.1, nround = 300, objective = "reg:squarederror")
```

We choose the gblinear booster and reg:squarederror objective for training XGBoost model and comparing with MLR later.

#### Testing
```{r}
xgbpred1 = predict(xgb1, dtest)

dxg =  data.frame(
    Error = c("MSE", "MAE", "RMSE"), 
    Value = c(mean((data_testy$label - xgbpred1)^2),
              caret::MAE(data_testy$label, xgbpred1), 
              caret::RMSE(data_testy$label, xgbpred1)))

knitr::kable(dxg)
```

We find the table above for XGBoost shows the similar statistical measurement results with the linear model. We claim XGBoost model using gblinear booster here does not simply work better than MLR model.

```{r}
xgb_imp1 = xgb.importance(feature_names = xgb1$feature_names, model = xgb1)
xgb.ggplot.importance(xgb_imp1, top_n =10, n_clusters = 2)
```

The feature importance plot of XGBoost model indicates some levels of education_level and race factors are much important. Meanwhile, the result differs from MLR models as race:Mexican American and race:Non-Hispanic white are also important.

### XGBoost (Optimizing Logistic)
#### Data
```{r}
slp_df_new2 = slp_df_new %>% 
  mutate(sufficient_slp = ifelse((ave_slp_hr >= 7), 1, 0)) %>% 
  select(-ave_slp_hr)
```

```{r}
set.seed(123)
rtxg = 0.8
subxg = sample(1:nrow(slp_df_new2), round(nrow(slp_df_new2)*rt))
data_trainxg = slp_df_new2[sub,]
data_testxg = slp_df_new2[-sub,]
dim(data_trainxg)
dim(data_testxg)
```

```{r}
data_trainx = data_trainxg[,-25]
data_trainy = data.frame(data_trainxg[,25])
names(data_trainy) = c('label')
data_testx = data_testxg[,-25]
data_testy = data.frame(data_testxg[,25])
names(data_testy) = c('label')
```

```{r}
dtrain = xgb.DMatrix(data = as.matrix(data_trainx), label = data_trainy$label)
dtest = xgb.DMatrix(data = as.matrix(data_testx), label = data_testy$label)
```

#### Training
```{r}
xgb2 = xgboost(data = dtrain, booster = "gbtree", max.depth = 10, nround = 2000, objective = "binary:logistic")
print(xgb2)
```

We choose the gbtree booster and binary:logistic objective for training XGBoost model and comparing with Logistic regression model later.

#### Testing
```{r}
xgbpred2 = round(predict(xgb2, dtest))
tablog = table(data_testy$label, xgbpred2, dnn=c("true","pre"))
tablog
```

From the table, we find:
1. the logistic model works well on sufficient sleep. $\frac{970}{970+201} = 82.84\%$
2. the logistic model works not well on sufficient sleep. $\frac{75}{75+287} =  20.72\%$
3. the total accuracy of the model prediction is $\frac{75+970}{75+287+970+201} = 68.17\%$

Compared with logistic regression model, we find although the correctly prediction ability of XGBoost under gbtree booster and binary:logistic objective for sufficient sleep and the total are lower, the model actually is much reasonable as it could predict larger proportions of insufficient sleep cases with no extreme gap between the prediction ability between two groups.

```{r}
xgb_imp2 = xgb.importance(feature_names = xgb2$feature_names, model = xgb2)
xgb.ggplot.importance(xgb_imp2, top_n =10, n_clusters = 2)
```

The feature importance plot of XGBoost model indicates two extremely important factors, income_poverty_ratio and age. Which differs from linear models so that we want to explore the decision tree structure.

#### Tree plot
```{r}
xgb.plot.tree(model = xgb2, trees=1999, plot_width = 900)
```

According to the XGBoost tree plot above, we can clearly discover how the final tree model decide the values on each leaf which can directly lead to probability estimators based on the logistic function.

# Shiny APP
We made a questionnaire using Sleep Quality Scale and make different suggestions about sleeping in our shiny app. Users can answer the questions, and then they can get a score for their sleeping quality. The score can be calculated by adding all answers in 1-4 together. The higher the score is, the poorer the quality of their sleeping is. In the end, there will be some suggestions to improve usersâ€™ sleeping quality, which is really useful, because the suggestions are based on many well-conducted scientific research. 
