<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Sleep and Social Factors</title>

<script src="site_libs/header-attrs-2.18/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/journal.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.10.1/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<link href="site_libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="Places_data.html">Sleep &amp; Diseases</a>
</li>
<li>
  <a href="Data.html">Sleep &amp; Social factors</a>
</li>
<li>
  <a href="app.html">Survey about sleep</a>
</li>
<li>
  <a href="mailto:&lt;wz2631@cumc.columbia.edu&gt;">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/jchaicc/final.github.io.git">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="References.html">
    <span class="fa fa-book fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Sleep and Social Factors</h1>

</div>


<div id="data-sources" class="section level1">
<h1>Data Sources</h1>
<ul>
<li><p>The first data set that was used to explore the association
between sleeping and social factors was downloaded from the <a
href="https://www.cdc.gov/nchs/nhanes/index.htm">NHANES</a>
database.</p></li>
<li><p>The second data set was from <a
href="https://www.cdc.gov/sleep/data-and-statistics/adults.html">CDC</a>.</p></li>
</ul>
<div id="data-processing-and-cleaning" class="section level2">
<h2>Data Processing and Cleaning</h2>
<ul>
<li>For data from the NHANES database: The predictors and the response
variable were from different data base in NHANES. First, the the column
name for these response variables was changed to appropriate names. The
original data included numbers to represent difference levels for each
predictor, specific names was also replaced for each number. Then, the
response variable related to sleeping was from another data base. For
this data set, similar steps was used with dealing with the predictors
data set. All column names was converted to appropriate names, and
character names also replaced the numbers as well. An extra column was
added to the data set:<code>ave_slp_hr</code>, which is the average
sleeping hours per day, by calculating the mean of the weekday and
weekend sleeping hours. For the third step, all empty cells was removed.
Finally, the two cleaned data sets was joined through the patients’
unique sequence number.</li>
</ul>
<pre class="r"><code>demo=foreign::read.xport(&quot;nhanes/P_DEMO.XPT&quot;) %&gt;%
  janitor::clean_names() %&gt;%
  select(seqn,riagendr,ridageyr,ridreth3,dmdeduc2,indfmpir) %&gt;%
  drop_na() %&gt;%
  rename(gender=riagendr,
         age=ridageyr,
         race=ridreth3,
         education_level=dmdeduc2,
         income_poverty_ratio=indfmpir) %&gt;%
  mutate(gender=case_when(gender==1 ~ &quot;male&quot;,
                          gender==2 ~ &quot;female&quot;)) %&gt;%
  mutate(race=case_when(race==1 ~ &quot;Mexican American&quot;,
                        race==2 ~ &quot;Other Hispanic&quot;,
                        race==3 ~ &quot;Non-Hispanic White&quot;,
                        race==4 ~ &quot;Non-Hispanic Black&quot;,
                        race==6 ~ &quot;Non-Hispanic Asian&quot;,
                        race==7 ~ &quot;Other Race&quot;)) %&gt;%
  mutate(education_level=case_when(education_level==1 ~ &quot;less than 9th grade&quot;,
                                   education_level==2 ~ &quot;9-11th grade&quot;,
                                   education_level==3 ~ &quot;high school graduate&quot;,
                                   education_level==4 ~ &quot;college or AA degree&quot;,
                                   education_level==5 ~ &quot;college graduate or above&quot;,
                                   education_level==7 ~ &quot;refused&quot;,
                                   education_level==9 ~ &quot;don&#39;t know&quot;))


sleep_df=foreign::read.xport(&quot;nhanes/P_SLQ.XPT&quot;) %&gt;%
  janitor::clean_names() %&gt;% 
  select(seqn,slq300,slq310,sld012,slq320,slq330,sld013,slq050,slq120) %&gt;%
  rename(weekday_slp_time=slq300,
         weekday_wake_time=slq310,
         weekday_slp_hr=sld012,
         weekend_slp_time=slq320,
         weekend_wake_time=slq330,
         weekend_slp_hr=sld013,
         trouble_slp=slq050,
         sleepy_freq=slq120
         )              %&gt;%
  filter(weekday_slp_time!=77777 &amp; weekday_slp_time!= 99999) %&gt;%
  filter(weekday_wake_time!=77777 &amp; weekday_wake_time!= 99999) %&gt;%
  filter(weekend_slp_time!=77777 &amp; weekend_slp_time != 99999) %&gt;%
  filter(weekend_wake_time!= 77777 &amp; weekend_wake_time != 99999) %&gt;%
  filter(trouble_slp!=7 &amp; trouble_slp!= 9) %&gt;%
  filter(sleepy_freq!= 7 &amp; sleepy_freq!=9) %&gt;%
  drop_na() %&gt;%
  mutate(trouble_slp=case_when(trouble_slp==1 ~ &quot;yes&quot;,
                               trouble_slp==2 ~ &quot;no&quot;)) %&gt;%
  mutate(sleepy_freq=case_when(sleepy_freq==0 ~ &quot;never&quot;,
                               sleepy_freq==1 ~ &quot;once a month&quot;,
                               sleepy_freq==2 ~ &quot;2-4 per month&quot;,
                               sleepy_freq==3 ~ &quot;5-15 per month&quot;,
                               sleepy_freq==4 ~ &quot;16-30 per month&quot;,
                               ))%&gt;% 
  mutate(ave_slp_hr = (5*weekday_slp_hr + 2*weekend_slp_hr)/7)</code></pre>
<pre class="r"><code>slp_df=inner_join(demo,sleep_df,by=&quot;seqn&quot;) %&gt;%
  na_if(&quot;&quot;) %&gt;%
  na.omit()
write_csv(slp_df, &quot;data/slp_df.csv&quot;)</code></pre>
</div>
<div id="data-description" class="section level2">
<h2>Data Description</h2>
<div id="social-factors-related-to-sleeping-status-data"
class="section level4">
<h4>Social factors related to Sleeping Status Data</h4>
<p>The resulting data file of <code>slp_df</code> contains a single
dataframe with 7665 rows of data on 15 variables, the list below is our
variables of interest:</p>
<ul>
<li><code>seqn</code>. The sequence number of the candidate.</li>
<li><code>gender</code>. Gender of the candidate.</li>
<li><code>age</code>. Age of the candidate.</li>
<li><code>race</code>. Race of the candidate.</li>
<li><code>hour</code>. Time(hour) violation occurred.</li>
<li><code>education_level</code>. The education level of the
candidate.</li>
<li><code>income_poverty_ratio</code>. A ratio to classify poverty and
not poverty. Ratio greater than 1 will be considered as not poverty,
ratio less than 1 will considered as poverty.</li>
<li><code>weekday_slp_time</code>. Sleep time on weekdays or
workdays.</li>
<li><code>weekday_wake_time</code>. Wake time on weekdays and
workdays.</li>
<li><code>weekday_slp_hr</code>. Average sleeping hours on weekdays or
workdays.</li>
<li><code>weekend_slp_time</code>. Sleep time on weekends.</li>
<li><code>weekday_wake_time</code>.Wake time on weekends.</li>
<li><code>weekend_slp_hr</code>. Average sleeping hours on
weekends.</li>
<li><code>trouble_slp</code>. A response on whether telling the doctor
having trouble sleeping.</li>
<li><code>sleepy_freq</code>. Average sleeply times during daytime per
month.</li>
<li><code>ave_slp_hr</code>. Average sleeping hours per day.</li>
</ul>
</div>
</div>
</div>
<div id="exploratory-analysis" class="section level1">
<h1>Exploratory Analysis</h1>
<div id="national-trends-in-short-sleep-duration"
class="section level2">
<h2>National trends in short sleep duration</h2>
<p>Short sleep duration is based on age group recommended hours of sleep
per day and defined as less than 7 hours for adults. This plot shows
age-adjusted prevalence of adults who reported short sleep duration from
2013 to 2020. Overall, short sleep duration prevalence was higher among
males than females across years.</p>
<pre class="r"><code>year_df=read_csv(&quot;data/AdultTrends.csv&quot;) %&gt;%
  janitor::clean_names() %&gt;%
  pivot_longer(cols=c(&quot;female&quot;, &quot;male&quot;),
               names_to = &quot;sex&quot;,
               values_to = &quot;age_adjust_prev&quot;) %&gt;%
  ggplot(aes(x=year,y=age_adjust_prev,col=sex))+geom_line()+geom_point()+labs(
    title = &quot;National trends in short sleep duration&quot;,
    x = &quot;Year&quot;,
    y = &quot;Age Adjusted Prevalence %&quot;
    )

year_df+transition_reveal(year)</code></pre>
<p><img
src="Data_files/figure-html/unnamed-chunk-3-1.gif" /><!-- --></p>
</div>
<div id="education-level" class="section level2">
<h2>Education level</h2>
<p>We first want to get the distribution of sleeping hours less than 7
hours across the five different education levels. We will construct a
bar chart tabulating the average sleeping hours per week in each of the
five education levels. Gender consideration was also added into to the
bar chart in order to see a difference between female and male in each
category.</p>
<pre class="r"><code> edu_plot=slp_df %&gt;%
  filter(ave_slp_hr&lt;7)%&gt;%
  group_by(education_level,gender) %&gt;%
  summarize(ave_sleep=mean((weekday_slp_hr*5+weekend_slp_hr*2)/7)) %&gt;% 
  ungroup() %&gt;%
  mutate(education_level=fct_reorder(education_level,ave_sleep)) %&gt;%
  ggplot(aes(x=education_level,y=ave_sleep,fill=gender))+ geom_bar(width=0.5,stat=&quot;identity&quot;)+
  viridis::scale_fill_viridis(
    name = &quot;gender&quot;,
    discrete = TRUE
  ) + geom_text(aes(label = round(ave_sleep, 2)),position = position_stack(vjust=0.9), color = &quot;white&quot;, size = 4)+
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust=1))+labs(
    title = &quot;Distribution of sleeping hours across education level&quot;,
    x = &quot;Education Level&quot;,
    y = &quot;Average Sleeping hours per day&quot;
    )
edu_plot</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
<p>From the above plot, high school graduates has the least sleeping
hours, while college graduates or above has the highest. Female and male
does not have a significant difference in both groups.</p>
</div>
<div id="race" class="section level2">
<h2>Race</h2>
<p>A heat map was made to visualize sleeping hours less than 7 hours
among different races. The below plot indicates that there is a large
number of people who has a sleeping hour of 6 hours per day among the
Non-Hispanic black group. There are also many Non-Hispanic Whites who
has an average of 6 hours per day.</p>
<pre class="r"><code>race_plot=slp_df %&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  mutate(sleep_ave=(weekday_slp_hr*5+weekend_slp_hr*2)/7) %&gt;%
  group_by(race,sleep_ave) %&gt;%
  summarise(obs=n()) %&gt;%
  plot_ly(
    x = ~sleep_ave, y = ~race, z = ~obs, type = &quot;heatmap&quot;, colors = &quot;BuPu&quot;
  ) %&gt;%
  colorbar(title = &quot;Number of People&quot;, x = 1, y = 0.5) 
layout(race_plot, xaxis = list(title = &quot;Average Sleeping Hours Per Day&quot;), yaxis = list(title = &quot;Race&quot;))</code></pre>
<div id="htmlwidget-cd179f9f0acf86e7a0fa" style="width:768px;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-cd179f9f0acf86e7a0fa">{"x":{"visdat":{"463031435bf2":["function () ","plotlyVisDat"]},"cur_data":"463031435bf2","attrs":{"463031435bf2":{"x":{},"y":{},"z":{},"colors":"BuPu","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"heatmap"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"title":"Average Sleeping Hours Per Day"},"yaxis":{"domain":[0,1],"automargin":true,"title":"Race","type":"category","categoryorder":"array","categoryarray":["Mexican American","Non-Hispanic Asian","Non-Hispanic Black","Non-Hispanic White","Other Hispanic","Other Race"]},"scene":{"zaxis":{"title":"obs"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"Number of People","ticklen":2,"len":0.5,"lenmode":"fraction","y":0.5,"yanchor":"top","x":1},"colorscale":[["0","rgba(247,252,253,1)"],["0.0416666666666667","rgba(239,247,250,1)"],["0.0833333333333333","rgba(232,241,247,1)"],["0.125","rgba(224,236,244,1)"],["0.166666666666667","rgba(213,228,239,1)"],["0.208333333333333","rgba(202,219,235,1)"],["0.25","rgba(191,211,230,1)"],["0.291666666666667","rgba(180,203,226,1)"],["0.333333333333333","rgba(169,196,222,1)"],["0.375","rgba(158,188,218,1)"],["0.416666666666667","rgba(152,175,211,1)"],["0.458333333333333","rgba(146,163,205,1)"],["0.5","rgba(140,150,198,1)"],["0.541666666666667","rgba(141,136,191,1)"],["0.583333333333333","rgba(141,122,184,1)"],["0.625","rgba(140,107,177,1)"],["0.666666666666667","rgba(139,94,170,1)"],["0.708333333333333","rgba(138,80,164,1)"],["0.75","rgba(136,65,157,1)"],["0.791666666666667","rgba(134,52,146,1)"],["0.833333333333333","rgba(132,36,135,1)"],["0.875","rgba(129,15,124,1)"],["0.916666666666667","rgba(111,8,107,1)"],["0.958333333333333","rgba(94,3,91,1)"],["1","rgba(77,0,75,1)"]],"showscale":true,"x":[3.5,3.57142857142857,3.71428571428571,4,4.28571428571429,4.42857142857143,4.5,4.71428571428571,5,5.14285714285714,5.28571428571429,5.35714285714286,5.42857142857143,5.5,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3.14285714285714,3.5,4,4.07142857142857,4.14285714285714,4.28571428571429,4.42857142857143,4.85714285714286,4.92857142857143,5,5.07142857142857,5.14285714285714,5.21428571428571,5.28571428571429,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3,3.28571428571429,3.5,3.57142857142857,3.85714285714286,3.92857142857143,4,4.07142857142857,4.14285714285714,4.21428571428571,4.28571428571429,4.35714285714286,4.42857142857143,4.5,4.57142857142857,4.64285714285714,4.71428571428571,4.78571428571429,4.85714285714286,4.92857142857143,5,5.07142857142857,5.14285714285714,5.21428571428571,5.28571428571429,5.35714285714286,5.42857142857143,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3,3.28571428571429,3.5,3.57142857142857,3.71428571428571,3.78571428571429,3.92857142857143,4,4.14285714285714,4.21428571428571,4.28571428571429,4.35714285714286,4.42857142857143,4.5,4.57142857142857,4.64285714285714,4.71428571428571,4.78571428571429,4.85714285714286,5,5.07142857142857,5.14285714285714,5.21428571428571,5.28571428571429,5.35714285714286,5.42857142857143,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3,3.5,4,4.14285714285714,4.28571428571429,4.5,4.57142857142857,4.71428571428571,4.92857142857143,5,5.07142857142857,5.14285714285714,5.28571428571429,5.35714285714286,5.42857142857143,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3,3.5,3.78571428571429,3.92857142857143,4,4.28571428571429,4.42857142857143,4.57142857142857,4.85714285714286,5,5.14285714285714,5.28571428571429,5.42857142857143,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.85714285714286,6.92857142857143],"y":["Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race"],"z":[1,1,1,2,4,1,3,1,3,1,1,1,3,1,3,4,1,5,3,14,7,4,11,3,5,14,19,5,11,9,8,12,1,1,5,1,1,3,2,1,1,13,2,1,1,3,5,3,2,2,2,6,23,3,5,2,7,2,5,14,16,8,7,6,7,9,6,3,2,2,4,1,19,2,5,1,15,2,5,10,6,1,13,3,4,5,34,5,12,2,24,2,11,11,20,6,21,10,13,6,69,8,13,10,35,7,24,31,31,19,36,17,30,24,7,2,4,1,3,1,1,17,1,2,3,1,1,11,2,2,1,4,4,52,2,2,4,7,2,5,18,10,3,7,4,10,8,65,10,6,6,31,13,19,59,33,28,23,17,30,26,2,1,6,2,1,3,3,1,3,11,1,1,2,2,2,3,3,2,1,1,4,1,24,1,7,5,10,6,5,18,8,4,8,6,11,5,1,1,1,1,3,2,1,2,1,7,2,1,1,3,2,1,4,2,3,3,11,3,1,2,9,2,6,8,3,3,6,9,6],"type":"heatmap","xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div id="race-gender-gap-by-education-level" class="section level3">
<h3>Race Gender Gap by Education Level</h3>
<p>The below plot demonstrates the gender gap in the patients for
different races. Male outnumber female for sleeping less than 7 hours
per day for all race, except Non-Hispanic Black and Non-Hispanic Asian.
The bubble represents the degree of the gap, along inlcuding their
education level.</p>
<pre class="r"><code>gender_plot=slp_df %&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  group_by(race,education_level) %&gt;%
  summarize(total_f=sum(gender==&quot;female&quot;),
            total_m=sum(gender==&quot;male&quot;),
            gap=total_m-total_f) %&gt;%
  mutate(text_lable=str_c(&quot;Race=&quot;,race,&quot;\nEducation level: &quot;, education_level)) %&gt;%
  plot_ly(x=~total_m,y=~total_f,text=~text_lable,color=~race,size=~gap,type=&quot;scatter&quot;,mode=&quot;markers&quot;,
          colors=&quot;viridis&quot;,sizes = c(50, 700), marker = list(opacity = 0.7))

layout(gender_plot, title = &quot;Race Gender Gap by Education Level&quot;, xaxis = list(title = &quot;Number of Male Sleeping less than 7 hrs&quot;), yaxis = list(title = &quot;Number of Female Sleeping less than 7 hrs&quot;))</code></pre>
<div id="htmlwidget-e13ba7d3553f67d92e78" style="width:768px;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-e13ba7d3553f67d92e78">{"x":{"visdat":{"463058a54255":["function () ","plotlyVisDat"]},"cur_data":"463058a54255","attrs":{"463058a54255":{"x":{},"y":{},"text":{},"mode":"markers","marker":{"opacity":0.7},"color":{},"size":{},"colors":"viridis","alpha_stroke":1,"sizes":[50,700],"spans":[1,20],"type":"scatter"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"Race Gender Gap by Education Level","xaxis":{"domain":[0,1],"automargin":true,"title":"Number of Male Sleeping less than 7 hrs"},"yaxis":{"domain":[0,1],"automargin":true,"title":"Number of Female Sleeping less than 7 hrs"},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[16,13,26,21,19],"y":[11,4,22,17,13],"text":["Race=Mexican American<br />Education level: 9-11th grade","Race=Mexican American<br />Education level: college graduate or above","Race=Mexican American<br />Education level: college or AA degree","Race=Mexican American<br />Education level: high school graduate","Race=Mexican American<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(68,1,84,1)","size":[241.176470588235,292.156862745098,228.43137254902,228.43137254902,253.921568627451],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(68,1,84,1)"}},"type":"scatter","name":"Mexican American","textfont":{"color":"rgba(68,1,84,1)","size":[241.176470588235,292.156862745098,228.43137254902,228.43137254902,253.921568627451]},"error_y":{"color":"rgba(68,1,84,1)","width":[]},"error_x":{"color":"rgba(68,1,84,1)","width":[]},"line":{"color":"rgba(68,1,84,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[6,42,20,6,2],"y":[10,37,21,15,11],"text":["Race=Non-Hispanic Asian<br />Education level: 9-11th grade","Race=Non-Hispanic Asian<br />Education level: college graduate or above","Race=Non-Hispanic Asian<br />Education level: college or AA degree","Race=Non-Hispanic Asian<br />Education level: high school graduate","Race=Non-Hispanic Asian<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(65,68,135,1)","size":[126.470588235294,241.176470588235,164.705882352941,62.7450980392157,62.7450980392157],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(65,68,135,1)"}},"type":"scatter","name":"Non-Hispanic Asian","textfont":{"color":"rgba(65,68,135,1)","size":[126.470588235294,241.176470588235,164.705882352941,62.7450980392157,62.7450980392157]},"error_y":{"color":"rgba(65,68,135,1)","width":[]},"error_x":{"color":"rgba(65,68,135,1)","width":[]},"line":{"color":"rgba(65,68,135,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[34,65,131,101,11],"y":[29,75,133,60,1],"text":["Race=Non-Hispanic Black<br />Education level: 9-11th grade","Race=Non-Hispanic Black<br />Education level: college graduate or above","Race=Non-Hispanic Black<br />Education level: college or AA degree","Race=Non-Hispanic Black<br />Education level: high school graduate","Race=Non-Hispanic Black<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(42,120,142,1)","size":[241.176470588235,50,151.960784313725,700,304.901960784314],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(42,120,142,1)"}},"type":"scatter","name":"Non-Hispanic Black","textfont":{"color":"rgba(42,120,142,1)","size":[241.176470588235,50,151.960784313725,700,304.901960784314]},"error_y":{"color":"rgba(42,120,142,1)","width":[]},"error_x":{"color":"rgba(42,120,142,1)","width":[]},"line":{"color":"rgba(42,120,142,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[32,65,126,89,11],"y":[36,41,102,65,1],"text":["Race=Non-Hispanic White<br />Education level: 9-11th grade","Race=Non-Hispanic White<br />Education level: college graduate or above","Race=Non-Hispanic White<br />Education level: college or AA degree","Race=Non-Hispanic White<br />Education level: high school graduate","Race=Non-Hispanic White<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(34,168,132,1)","size":[126.470588235294,483.333333333333,483.333333333333,483.333333333333,304.901960784314],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(34,168,132,1)"}},"type":"scatter","name":"Non-Hispanic White","textfont":{"color":"rgba(34,168,132,1)","size":[126.470588235294,483.333333333333,483.333333333333,483.333333333333,304.901960784314]},"error_y":{"color":"rgba(34,168,132,1)","width":[]},"error_x":{"color":"rgba(34,168,132,1)","width":[]},"line":{"color":"rgba(34,168,132,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[13,18,27,19,16],"y":[7,16,23,14,21],"text":["Race=Other Hispanic<br />Education level: 9-11th grade","Race=Other Hispanic<br />Education level: college graduate or above","Race=Other Hispanic<br />Education level: college or AA degree","Race=Other Hispanic<br />Education level: high school graduate","Race=Other Hispanic<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(122,209,81,1)","size":[253.921568627451,202.941176470588,228.43137254902,241.176470588235,113.725490196078],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(122,209,81,1)"}},"type":"scatter","name":"Other Hispanic","textfont":{"color":"rgba(122,209,81,1)","size":[253.921568627451,202.941176470588,228.43137254902,241.176470588235,113.725490196078]},"error_y":{"color":"rgba(122,209,81,1)","width":[]},"error_x":{"color":"rgba(122,209,81,1)","width":[]},"line":{"color":"rgba(122,209,81,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[9,9,25,18,2],"y":[2,11,23,12,0],"text":["Race=Other Race<br />Education level: 9-11th grade","Race=Other Race<br />Education level: college graduate or above","Race=Other Race<br />Education level: college or AA degree","Race=Other Race<br />Education level: high school graduate","Race=Other Race<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(253,231,37,1)","size":[266.666666666667,151.960784313725,202.941176470588,253.921568627451,202.941176470588],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(253,231,37,1)"}},"type":"scatter","name":"Other Race","textfont":{"color":"rgba(253,231,37,1)","size":[266.666666666667,151.960784313725,202.941176470588,253.921568627451,202.941176470588]},"error_y":{"color":"rgba(253,231,37,1)","width":[]},"error_x":{"color":"rgba(253,231,37,1)","width":[]},"line":{"color":"rgba(253,231,37,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="poverty-status" class="section level2">
<h2>Poverty Status</h2>
<p>We can observe that people who are in poverty tend to have less
sleeping hours than those who are not in poverty.</p>
<pre class="r"><code>income_df=slp_df %&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  mutate(ip_stat=case_when(income_poverty_ratio &gt; 1 ~ &quot;not in poverty&quot;,
                           income_poverty_ratio &lt; 1~ &quot;in poverty&quot;,
                           income_poverty_ratio == 1~ &quot;in poverty&quot;)) %&gt;%
  ggplot(aes(x=weekday_slp_hr,y=ip_stat,fill=ip_stat))+
  geom_density_ridges(
    aes(point_color = ip_stat, point_shape = ip_stat,point_fill=ip_stat),
    alpha = .3, point_alpha = 0.7)+
   scale_x_continuous(
    breaks = c(2, 4, 6), 
    labels = c(&quot;2hrs&quot;, &quot;4hrs&quot;, &quot;6hrs&quot;),
    limits = c(2, 6)
    )+labs(
    x = &quot; Average Sleeping Hours&quot;
    )
  
box_plot=
  slp_df %&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  mutate(ip_stat=case_when(income_poverty_ratio &gt; 1 ~ &quot;not in poverty&quot;,
                           income_poverty_ratio &lt; 1~ &quot;in poverty&quot;,
                           income_poverty_ratio == 1~ &quot;in poverty&quot;)) %&gt;%
  mutate(sleep_ave=(weekday_slp_hr*5+weekend_slp_hr*2)/7) %&gt;%
  ggplot(aes(x=ip_stat,y=sleep_ave))+geom_boxplot(aes(fill = ip_stat), alpha = 0.3)+
  geom_hline(aes(yintercept=median(sleep_ave),
            color=&quot;red&quot;, linetype=&quot;dashed&quot;))+
  geom_text(aes(0, median(ave_slp_hr), label = &quot;sleep hours median&quot;), vjust = -0.5, hjust = 0, color = &quot;red&quot;)+labs(
    x = &quot; Poverty Status&quot;,
    y = &quot;Average Sleeping Hours&quot;
    )

comb=income_df+box_plot
comb+plot_annotation(
  title = &quot;Sleeping Hours By Poverty Status&quot;
) </code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-7-1.png" width="960" /></p>
</div>
<div id="age" class="section level2">
<h2>Age</h2>
<p>A bar plot was made to see the distribution of average sleeping hours
less than 7 hours in different age groups. People age between 71 and 80
has the least sleeping hours with about 5.68 hours per day.</p>
<pre class="r"><code>age_group= slp_df%&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  mutate(age_gp=case_when(age&gt;=20 &amp; age&lt;=30 ~ &quot;20-30&quot;,
                          age&gt;=31 &amp;age &lt;=40 ~ &quot;31-40&quot;,
                          age&gt;=41 &amp;age&lt;=50 ~ &quot;41-50&quot;,
                          age&gt;=51 &amp;age&lt;=60 ~ &quot;51-60&quot;,
                          age&gt;=61 &amp;age&lt;=70 ~ &quot;61-70&quot;,
                          age&gt;=71 &amp; age &lt;=80 ~ &quot;71-80&quot;)) %&gt;%
  group_by(age_gp) %&gt;%
  summarise(ave_slp=mean((weekday_slp_hr*5+weekend_slp_hr*2)/7))%&gt;%
  ungroup() %&gt;%
  mutate(age_gp=fct_reorder(age_gp,ave_slp)) %&gt;%
  ggplot(aes(x=age_gp,y=ave_slp,fill=age_gp))+ geom_bar(stat=&quot;identity&quot;)+ scale_fill_viridis_d()+
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust=1))+
  geom_text(aes(label = round(ave_slp, 2)),position = position_stack(vjust=0.9), color = &quot;white&quot;, size = 4)+labs(x=&quot;Age Group&quot;,y=&quot;Average Sleeping Hours Per Day&quot;,title=&quot;Distribution of Sleeping Hours across Age Group&quot;)
age_group</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-8-1.png" width="768" /></p>
</div>
</div>
<div id="statistical-analysis" class="section level1">
<h1>Statistical Analysis</h1>
<div id="models" class="section level2">
<h2>Models</h2>
<div id="data" class="section level3">
<h3>Data</h3>
<pre class="r"><code>df_mod = slp_df %&gt;% 
  select(gender, age, race, education_level, income_poverty_ratio, trouble_slp, sleepy_freq, ave_slp_hr)</code></pre>
<pre class="r"><code>set.seed(1)
rt = 0.8
sub = sample(1:nrow(df_mod), round(nrow(df_mod)*rt))
data_train = df_mod[sub,]
data_test = df_mod[-sub,]
dim(data_train)</code></pre>
<pre><code>## [1] 6132    8</code></pre>
<pre class="r"><code>dim(data_test)</code></pre>
<pre><code>## [1] 1533    8</code></pre>
</div>
<div id="multiple-linear-regression-mlr" class="section level3">
<h3>Multiple Linear Regression (MLR)</h3>
<pre class="r"><code>lmraw = lm(ave_slp_hr ~ ., data = data_train)
summary(lmraw)</code></pre>
<pre><code>## 
## Call:
## lm(formula = ave_slp_hr ~ ., data = data_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1483 -0.8543  0.0182  0.8587  5.9047 
## 
## Coefficients:
##                                            Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                               8.0876424  0.1144355  70.674  &lt; 2e-16
## gendermale                               -0.3136910  0.0376044  -8.342  &lt; 2e-16
## age                                       0.0003161  0.0011022   0.287 0.774282
## raceNon-Hispanic Asian                   -0.0977512  0.0826846  -1.182 0.237165
## raceNon-Hispanic Black                   -0.2831461  0.0705015  -4.016 5.99e-05
## raceNon-Hispanic White                   -0.0056274  0.0687206  -0.082 0.934738
## raceOther Hispanic                       -0.1073814  0.0830328  -1.293 0.195977
## raceOther Race                           -0.0969024  0.1031590  -0.939 0.347588
## education_levelcollege graduate or above -0.2173878  0.0745577  -2.916 0.003562
## education_levelcollege or AA degree      -0.2798869  0.0671686  -4.167 3.13e-05
## education_leveldon&#39;t know                 0.0117396  1.0337248   0.011 0.990939
## education_levelhigh school graduate      -0.1417923  0.0688075  -2.061 0.039373
## education_levelless than 9th grade       -0.0510489  0.0950936  -0.537 0.591406
## education_levelrefused                   -1.3479607  1.4601834  -0.923 0.355968
## income_poverty_ratio                     -0.0457828  0.0133222  -3.437 0.000593
## trouble_slpyes                           -0.0973259  0.0429400  -2.267 0.023452
## sleepy_freq2-4 per month                  0.3154540  0.0738469   4.272 1.97e-05
## sleepy_freq5-15 per month                 0.1221374  0.0795163   1.536 0.124589
## sleepy_freqnever                          0.3721973  0.0816463   4.559 5.25e-06
## sleepy_freqonce a month                   0.3315710  0.0774745   4.280 1.90e-05
##                                             
## (Intercept)                              ***
## gendermale                               ***
## age                                         
## raceNon-Hispanic Asian                      
## raceNon-Hispanic Black                   ***
## raceNon-Hispanic White                      
## raceOther Hispanic                          
## raceOther Race                              
## education_levelcollege graduate or above ** 
## education_levelcollege or AA degree      ***
## education_leveldon&#39;t know                   
## education_levelhigh school graduate      *  
## education_levelless than 9th grade          
## education_levelrefused                      
## income_poverty_ratio                     ***
## trouble_slpyes                           *  
## sleepy_freq2-4 per month                 ***
## sleepy_freq5-15 per month                   
## sleepy_freqnever                         ***
## sleepy_freqonce a month                  ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.457 on 6112 degrees of freedom
## Multiple R-squared:  0.02996,    Adjusted R-squared:  0.02694 
## F-statistic: 9.935 on 19 and 6112 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The model driven by multiple linear regression is <span
class="math inline">\(sleep\_hour=8.03-0.30I(gender = male)-0.31I(race =
Non-Hispanic Black)-0.18I(education_level = college graduate or
above)-0.22I(education_level = college or AA
degree)-0.05income_poverty_ratio-0.13trouble_slpyes+0.34I(sleepy_freq =
2-4 per month)+0.18I(sleepy_freq = 5-15 per month)+0.39I(sleepy_freq =
freqnever)+0.35I(sleepy_freq = freqonce a month)\)</span>. We find it
strongly relating to</p>
<pre class="r"><code>par(mfrow = c(2,2))
plot(lmraw, cex.lab = 1)</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-12-1.png" width="768" /></p>
<pre class="r"><code>step1 = step(lmraw, direction = &#39;both&#39;, trace=0)
summary(step1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = ave_slp_hr ~ gender + race + education_level + income_poverty_ratio + 
##     trouble_slp + sleepy_freq, data = data_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1527 -0.8536  0.0156  0.8573  5.9100 
## 
## Coefficients:
##                                           Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                               8.101124   0.104327  77.651  &lt; 2e-16
## gendermale                               -0.313685   0.037602  -8.342  &lt; 2e-16
## raceNon-Hispanic Asian                   -0.096871   0.082621  -1.172 0.241054
## raceNon-Hispanic Black                   -0.281045   0.070115  -4.008 6.19e-05
## raceNon-Hispanic White                   -0.002086   0.067597  -0.031 0.975382
## raceOther Hispanic                       -0.106063   0.082899  -1.279 0.200798
## raceOther Race                           -0.095610   0.103053  -0.928 0.353559
## education_levelcollege graduate or above -0.218982   0.074344  -2.946 0.003236
## education_levelcollege or AA degree      -0.281696   0.066867  -4.213 2.56e-05
## education_leveldon&#39;t know                 0.010356   1.033636   0.010 0.992007
## education_levelhigh school graduate      -0.142589   0.068746  -2.074 0.038108
## education_levelless than 9th grade       -0.048651   0.094718  -0.514 0.607522
## education_levelrefused                   -1.348516   1.460072  -0.924 0.355734
## income_poverty_ratio                     -0.045427   0.013263  -3.425 0.000619
## trouble_slpyes                           -0.095939   0.042664  -2.249 0.024565
## sleepy_freq2-4 per month                  0.315329   0.073840   4.270 1.98e-05
## sleepy_freq5-15 per month                 0.121501   0.079479   1.529 0.126389
## sleepy_freqnever                          0.373336   0.081544   4.578 4.78e-06
## sleepy_freqonce a month                   0.332232   0.077434   4.290 1.81e-05
##                                             
## (Intercept)                              ***
## gendermale                               ***
## raceNon-Hispanic Asian                      
## raceNon-Hispanic Black                   ***
## raceNon-Hispanic White                      
## raceOther Hispanic                          
## raceOther Race                              
## education_levelcollege graduate or above ** 
## education_levelcollege or AA degree      ***
## education_leveldon&#39;t know                   
## education_levelhigh school graduate      *  
## education_levelless than 9th grade          
## education_levelrefused                      
## income_poverty_ratio                     ***
## trouble_slpyes                           *  
## sleepy_freq2-4 per month                 ***
## sleepy_freq5-15 per month                   
## sleepy_freqnever                         ***
## sleepy_freqonce a month                  ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.457 on 6113 degrees of freedom
## Multiple R-squared:  0.02995,    Adjusted R-squared:  0.02709 
## F-statistic: 10.48 on 18 and 6113 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The model after stepwise both direction is <span
class="math inline">\(sleep\_hour=8.04-0.30I(gender = male)-0.30I(race =
Non-Hispanic Black)+ 0.34I(sleepy_freq = 2-4 per
month)+0.18I(sleepy_freq = 5-15 per month)+0.39I(sleepy_freq =
freqnever)+0.35I(sleepy_freq = freqonce a
month)-0.05income_poverty_ratio-0.13trouble_slpyes-0.18I(education_level
= college graduate or above)-0.22I(education_level = college or AA
degree)\)</span></p>
<div id="testing-cross-validation" class="section level4">
<h4>Testing : Cross-Validation</h4>
<pre class="r"><code># set.seed(1)
# train = trainControl(method = &quot;cv&quot;, number = 100)
# model_caret = train(ave_slp_hr ~ ., data = data_train, method = &#39;lm&#39;, na.action = na.pass)
# model_caret$finalModel
# model_caret</code></pre>
<pre class="r"><code>raw_pred = predict(lmraw, newdata = data_test)

dlm =  data.frame(
    Error = c(&quot;MSE&quot;, &quot;MAE&quot;, &quot;RMSE&quot;), 
    Value = c(mean((data_test$ave_slp_hr - raw_pred)^2),
              caret::MAE(data_test$ave_slp_hr, raw_pred), 
              caret::RMSE(data_test$ave_slp_hr, raw_pred)))

knitr::kable(dlm)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Error</th>
<th align="right">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">MSE</td>
<td align="right">2.111956</td>
</tr>
<tr class="even">
<td align="left">MAE</td>
<td align="right">1.089663</td>
</tr>
<tr class="odd">
<td align="left">RMSE</td>
<td align="right">1.453257</td>
</tr>
</tbody>
</table>
<pre class="r"><code># raw_pred = predict(lmraw, newdata = data_test)</code></pre>
<pre class="r"><code># res_raw = residuals(lmraw)
# segments(obs, pred, obs, pred_res)</code></pre>
</div>
</div>
<div id="logistic" class="section level3">
<h3>logistic</h3>
<pre class="r"><code>data_logtrain = data_train %&gt;% 
  mutate(sufficient_slp = ifelse((ave_slp_hr &gt;= 7), 1, 0)) %&gt;% 
  select(-ave_slp_hr)
glmraw = glm(sufficient_slp ~ ., data = data_logtrain, family = binomial)
summary(glmraw)</code></pre>
<pre><code>## 
## Call:
## glm(formula = sufficient_slp ~ ., family = binomial, data = data_logtrain)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0485   0.5121   0.6424   0.7626   1.3531  
## 
## Coefficients:
##                                            Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)                               1.146e+00  1.852e-01   6.187 6.14e-10
## gendermale                               -3.906e-01  6.194e-02  -6.307 2.85e-10
## age                                      -7.509e-04  1.810e-03  -0.415  0.67817
## raceNon-Hispanic Asian                   -1.281e-01  1.446e-01  -0.886  0.37549
## raceNon-Hispanic Black                   -7.118e-01  1.189e-01  -5.987 2.14e-09
## raceNon-Hispanic White                    8.169e-03  1.196e-01   0.068  0.94556
## raceOther Hispanic                       -2.026e-01  1.421e-01  -1.426  0.15380
## raceOther Race                           -4.518e-01  1.660e-01  -2.722  0.00650
## education_levelcollege graduate or above  1.215e-01  1.237e-01   0.982  0.32635
## education_levelcollege or AA degree      -1.499e-01  1.094e-01  -1.370  0.17077
## education_leveldon&#39;t know                 1.136e+01  2.287e+02   0.050  0.96039
## education_levelhigh school graduate       1.404e-02  1.132e-01   0.124  0.90135
## education_levelless than 9th grade       -7.949e-04  1.606e-01  -0.005  0.99605
## education_levelrefused                    1.073e+01  3.247e+02   0.033  0.97364
## income_poverty_ratio                     -2.549e-02  2.181e-02  -1.169  0.24256
## trouble_slpyes                           -1.337e-01  6.901e-02  -1.937  0.05278
## sleepy_freq2-4 per month                  7.378e-01  1.110e-01   6.645 3.03e-11
## sleepy_freq5-15 per month                 3.253e-01  1.176e-01   2.767  0.00565
## sleepy_freqnever                          8.705e-01  1.271e-01   6.848 7.48e-12
## sleepy_freqonce a month                   8.279e-01  1.185e-01   6.984 2.87e-12
##                                             
## (Intercept)                              ***
## gendermale                               ***
## age                                         
## raceNon-Hispanic Asian                      
## raceNon-Hispanic Black                   ***
## raceNon-Hispanic White                      
## raceOther Hispanic                          
## raceOther Race                           ** 
## education_levelcollege graduate or above    
## education_levelcollege or AA degree         
## education_leveldon&#39;t know                   
## education_levelhigh school graduate         
## education_levelless than 9th grade          
## education_levelrefused                      
## income_poverty_ratio                        
## trouble_slpyes                           .  
## sleepy_freq2-4 per month                 ***
## sleepy_freq5-15 per month                ** 
## sleepy_freqnever                         ***
## sleepy_freqonce a month                  ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 6738.4  on 6131  degrees of freedom
## Residual deviance: 6509.2  on 6112  degrees of freedom
## AIC: 6549.2
## 
## Number of Fisher Scoring iterations: 11</code></pre>
<p>The first logistic model shows the significant predictors are gender,
race, education_level, trouble_types, and sleep_freq.</p>
<pre class="r"><code>step2 = step(glmraw, direction = &#39;both&#39;, trace=0)
summary(step2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = sufficient_slp ~ gender + race + trouble_slp + 
##     sleepy_freq, family = binomial, data = data_logtrain)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0226   0.5288   0.6476   0.7722   1.2735  
## 
## Coefficients:
##                           Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                1.02780    0.14214   7.231 4.80e-13 ***
## gendermale                -0.38836    0.06156  -6.308 2.82e-10 ***
## raceNon-Hispanic Asian    -0.09637    0.13922  -0.692  0.48881    
## raceNon-Hispanic Black    -0.71862    0.11440  -6.281 3.35e-10 ***
## raceNon-Hispanic White    -0.01019    0.11377  -0.090  0.92865    
## raceOther Hispanic        -0.20316    0.14107  -1.440  0.14984    
## raceOther Race            -0.45807    0.16313  -2.808  0.00499 ** 
## trouble_slpyes            -0.14384    0.06851  -2.100  0.03576 *  
## sleepy_freq2-4 per month   0.73843    0.11044   6.686 2.29e-11 ***
## sleepy_freq5-15 per month  0.32642    0.11713   2.787  0.00532 ** 
## sleepy_freqnever           0.87908    0.12666   6.941 3.90e-12 ***
## sleepy_freqonce a month    0.82618    0.11788   7.009 2.41e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 6738.4  on 6131  degrees of freedom
## Residual deviance: 6522.3  on 6120  degrees of freedom
## AIC: 6546.3
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The model after stepwise method shows the significant predictors are
still gender, race, education_level, trouble_types, and sleep_freq, but
the estimated coefficients change.</p>
<pre class="r"><code>anova(object = glmraw,test = &quot;Chisq&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: sufficient_slp
## 
## Terms added sequentially (first to last)
## 
## 
##                      Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
## NULL                                  6131     6738.4              
## gender                1   28.957      6130     6709.4 7.402e-08 ***
## age                   1    0.106      6129     6709.3   0.74514    
## race                  5   92.495      6124     6616.8 &lt; 2.2e-16 ***
## education_level       6   15.238      6118     6601.6   0.01848 *  
## income_poverty_ratio  1    0.367      6117     6601.2   0.54443    
## trouble_slp           1   15.230      6116     6586.0 9.517e-05 ***
## sleepy_freq           4   76.823      6112     6509.2 8.199e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We did model significance test, ANOVA, and determine the significant
predictors for logistic regression model are the same from stepwise
results.</p>
<div id="testing" class="section level4">
<h4>testing</h4>
<pre class="r"><code>data_logtest = data_test %&gt;% 
  mutate(sufficient_slp = ifelse((ave_slp_hr &gt;= 7), &#39;sufficient&#39;, &#39;insufficient&#39;)) %&gt;% 
  select(-ave_slp_hr)
prob2 = round(predict(object = glmraw, newdata = data_logtest,type = &quot;response&quot;))
pred2 = ifelse(prob2 == 1, &#39;sufficient&#39;, &#39;insufficient&#39;)
pred2 = factor(pred2, levels = c(&#39;insufficient&#39;, &#39;sufficient&#39;), order=TRUE)
tablog = table(data_logtest$sufficient_slp, pred2, dnn=c(&quot;true&quot;,&quot;pre&quot;))
tablog</code></pre>
<pre><code>##               pre
## true           insufficient sufficient
##   insufficient            6        356
##   sufficient              5       1166</code></pre>
<p>From the table, we find: 1. the logistic model works well on
sufficient sleep. <span class="math inline">\(\frac{1166}{1166+5} =
99.57\%\)</span> 2. the logistic model works not well on sufficient
sleep. <span class="math inline">\(\frac{6}{6+356} = 0.02\%\)</span> 3.
the total accuracy of the model prediction is <span
class="math inline">\(\frac{1166+6}{1166+5+6+356} = 76.45\%\)</span></p>
</div>
<div id="roc-auc-curve" class="section level4">
<h4>ROC-AUC curve</h4>
<pre class="r"><code>roc_curve = roc(data_logtest$sufficient_slp, prob2)
names(roc_curve)</code></pre>
<pre><code>##  [1] &quot;percent&quot;            &quot;sensitivities&quot;      &quot;specificities&quot;     
##  [4] &quot;thresholds&quot;         &quot;direction&quot;          &quot;cases&quot;             
##  [7] &quot;controls&quot;           &quot;fun.sesp&quot;           &quot;auc&quot;               
## [10] &quot;call&quot;               &quot;original.predictor&quot; &quot;original.response&quot; 
## [13] &quot;predictor&quot;          &quot;response&quot;           &quot;levels&quot;</code></pre>
<pre class="r"><code>x = 1-roc_curve$specificities
y = roc_curve$sensitivities

ggplot(data = NULL, aes(x = x, y = y)) + 
  geom_line(colour = &#39;red&#39;) + 
  geom_abline(intercept = 0, slope = 1) +
  annotate(&#39;text&#39;, x = 0.5, y = 0.5, label =paste(&#39;AUC=&#39;,round(roc_curve$auc,2))) +
  labs(x = &#39;1-specificities&#39;,y = &#39;sensitivities&#39;, title = &#39;ROC Curve&#39;)</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-22-1.png" width="768" /></p>
</div>
</div>
<div id="xgboost-compare-with-mlr" class="section level3">
<h3>xgboost (compare with mlr)</h3>
<pre class="r"><code>clafeats = c(&#39;gender&#39;, &#39;race&#39;, &#39;education_level&#39;, &#39;trouble_slp&#39;, &#39;sleepy_freq&#39;)
dums = dummyVars(~ gender + race + education_level + trouble_slp + sleepy_freq, data = df_mod)
slp_oh = as.data.frame(predict(dums, newdata = df_mod))
slp_df_new =cbind(df_mod[,-c(which(colnames(df_mod) %in% clafeats))],slp_oh)</code></pre>
<pre class="r"><code>set.seed(1)
rtxg = 0.8
subxg = sample(1:nrow(slp_df_new), round(nrow(slp_df_new)*rt))
data_trainxg = slp_df_new[sub,]
data_testxg = slp_df_new[-sub,]</code></pre>
<pre class="r"><code>data_trainx = data_trainxg[,-3]
data_trainy = data.frame(data_trainxg[,3])
names(data_trainy) = c(&#39;label&#39;)
data_testx = data_testxg[,-3]
data_testy = data.frame(data_testxg[,3])
names(data_testy) = c(&#39;label&#39;)</code></pre>
<pre class="r"><code>dtrain = xgb.DMatrix(data = as.matrix(data_trainx), label = data_trainy$label)
dtest = xgb.DMatrix(data = as.matrix(data_testx), label = data_testy$label)</code></pre>
<div id="model-training" class="section level4">
<h4>model training</h4>
<pre class="r"><code>xgb1 = xgboost(data = dtrain, max.depth = 10, nrounds = 300)</code></pre>
<pre><code>## [1]  train-rmse:5.308995 
## [2]  train-rmse:3.858269 
## [3]  train-rmse:2.886284 
## [4]  train-rmse:2.245182 
## [5]  train-rmse:1.829713 
## [6]  train-rmse:1.563952 
## [7]  train-rmse:1.392055 
## [8]  train-rmse:1.288338 
## [9]  train-rmse:1.196116 
## [10] train-rmse:1.145216 
## [11] train-rmse:1.099318 
## [12] train-rmse:1.067830 
## [13] train-rmse:1.049116 
## [14] train-rmse:1.017877 
## [15] train-rmse:0.991405 
## [16] train-rmse:0.978619 
## [17] train-rmse:0.946701 
## [18] train-rmse:0.929606 
## [19] train-rmse:0.917407 
## [20] train-rmse:0.895506 
## [21] train-rmse:0.880546 
## [22] train-rmse:0.846006 
## [23] train-rmse:0.817986 
## [24] train-rmse:0.789218 
## [25] train-rmse:0.773734 
## [26] train-rmse:0.768133 
## [27] train-rmse:0.751800 
## [28] train-rmse:0.733366 
## [29] train-rmse:0.714330 
## [30] train-rmse:0.700871 
## [31] train-rmse:0.699048 
## [32] train-rmse:0.687810 
## [33] train-rmse:0.674260 
## [34] train-rmse:0.663500 
## [35] train-rmse:0.649702 
## [36] train-rmse:0.632670 
## [37] train-rmse:0.624316 
## [38] train-rmse:0.618072 
## [39] train-rmse:0.598711 
## [40] train-rmse:0.580068 
## [41] train-rmse:0.571179 
## [42] train-rmse:0.565043 
## [43] train-rmse:0.555764 
## [44] train-rmse:0.552828 
## [45] train-rmse:0.547112 
## [46] train-rmse:0.526388 
## [47] train-rmse:0.508920 
## [48] train-rmse:0.505224 
## [49] train-rmse:0.496811 
## [50] train-rmse:0.485498 
## [51] train-rmse:0.479899 
## [52] train-rmse:0.471725 
## [53] train-rmse:0.466391 
## [54] train-rmse:0.461427 
## [55] train-rmse:0.445579 
## [56] train-rmse:0.434203 
## [57] train-rmse:0.421850 
## [58] train-rmse:0.416001 
## [59] train-rmse:0.414282 
## [60] train-rmse:0.408379 
## [61] train-rmse:0.405502 
## [62] train-rmse:0.400294 
## [63] train-rmse:0.390322 
## [64] train-rmse:0.387858 
## [65] train-rmse:0.379002 
## [66] train-rmse:0.374546 
## [67] train-rmse:0.367244 
## [68] train-rmse:0.363043 
## [69] train-rmse:0.359578 
## [70] train-rmse:0.356745 
## [71] train-rmse:0.353339 
## [72] train-rmse:0.341020 
## [73] train-rmse:0.337169 
## [74] train-rmse:0.333815 
## [75] train-rmse:0.330953 
## [76] train-rmse:0.326217 
## [77] train-rmse:0.325415 
## [78] train-rmse:0.320496 
## [79] train-rmse:0.312095 
## [80] train-rmse:0.307786 
## [81] train-rmse:0.303101 
## [82] train-rmse:0.298793 
## [83] train-rmse:0.295484 
## [84] train-rmse:0.294416 
## [85] train-rmse:0.287551 
## [86] train-rmse:0.285232 
## [87] train-rmse:0.281262 
## [88] train-rmse:0.279899 
## [89] train-rmse:0.278293 
## [90] train-rmse:0.275461 
## [91] train-rmse:0.273785 
## [92] train-rmse:0.269553 
## [93] train-rmse:0.268639 
## [94] train-rmse:0.266809 
## [95] train-rmse:0.262565 
## [96] train-rmse:0.261277 
## [97] train-rmse:0.259774 
## [98] train-rmse:0.258482 
## [99] train-rmse:0.257647 
## [100]    train-rmse:0.251650 
## [101]    train-rmse:0.249392 
## [102]    train-rmse:0.245773 
## [103]    train-rmse:0.242933 
## [104]    train-rmse:0.241774 
## [105]    train-rmse:0.239926 
## [106]    train-rmse:0.238836 
## [107]    train-rmse:0.238597 
## [108]    train-rmse:0.238210 
## [109]    train-rmse:0.236929 
## [110]    train-rmse:0.236801 
## [111]    train-rmse:0.229160 
## [112]    train-rmse:0.226891 
## [113]    train-rmse:0.225383 
## [114]    train-rmse:0.223347 
## [115]    train-rmse:0.222462 
## [116]    train-rmse:0.219371 
## [117]    train-rmse:0.216784 
## [118]    train-rmse:0.213923 
## [119]    train-rmse:0.211518 
## [120]    train-rmse:0.208543 
## [121]    train-rmse:0.207334 
## [122]    train-rmse:0.206497 
## [123]    train-rmse:0.205929 
## [124]    train-rmse:0.201403 
## [125]    train-rmse:0.199694 
## [126]    train-rmse:0.197094 
## [127]    train-rmse:0.194529 
## [128]    train-rmse:0.192421 
## [129]    train-rmse:0.189329 
## [130]    train-rmse:0.187777 
## [131]    train-rmse:0.185517 
## [132]    train-rmse:0.183585 
## [133]    train-rmse:0.182636 
## [134]    train-rmse:0.182394 
## [135]    train-rmse:0.182214 
## [136]    train-rmse:0.180579 
## [137]    train-rmse:0.179656 
## [138]    train-rmse:0.178946 
## [139]    train-rmse:0.178125 
## [140]    train-rmse:0.177232 
## [141]    train-rmse:0.176552 
## [142]    train-rmse:0.175672 
## [143]    train-rmse:0.173688 
## [144]    train-rmse:0.173256 
## [145]    train-rmse:0.172489 
## [146]    train-rmse:0.172160 
## [147]    train-rmse:0.171569 
## [148]    train-rmse:0.170235 
## [149]    train-rmse:0.169845 
## [150]    train-rmse:0.168789 
## [151]    train-rmse:0.168375 
## [152]    train-rmse:0.167607 
## [153]    train-rmse:0.167422 
## [154]    train-rmse:0.166957 
## [155]    train-rmse:0.166045 
## [156]    train-rmse:0.165517 
## [157]    train-rmse:0.165054 
## [158]    train-rmse:0.164035 
## [159]    train-rmse:0.163609 
## [160]    train-rmse:0.163569 
## [161]    train-rmse:0.163437 
## [162]    train-rmse:0.162774 
## [163]    train-rmse:0.161641 
## [164]    train-rmse:0.160421 
## [165]    train-rmse:0.160076 
## [166]    train-rmse:0.159900 
## [167]    train-rmse:0.159597 
## [168]    train-rmse:0.159435 
## [169]    train-rmse:0.158923 
## [170]    train-rmse:0.158658 
## [171]    train-rmse:0.158063 
## [172]    train-rmse:0.157670 
## [173]    train-rmse:0.156804 
## [174]    train-rmse:0.156680 
## [175]    train-rmse:0.156062 
## [176]    train-rmse:0.155900 
## [177]    train-rmse:0.155539 
## [178]    train-rmse:0.155157 
## [179]    train-rmse:0.154894 
## [180]    train-rmse:0.154571 
## [181]    train-rmse:0.154229 
## [182]    train-rmse:0.153831 
## [183]    train-rmse:0.153294 
## [184]    train-rmse:0.153118 
## [185]    train-rmse:0.153010 
## [186]    train-rmse:0.152747 
## [187]    train-rmse:0.152608 
## [188]    train-rmse:0.152443 
## [189]    train-rmse:0.152268 
## [190]    train-rmse:0.152187 
## [191]    train-rmse:0.152113 
## [192]    train-rmse:0.152080 
## [193]    train-rmse:0.151849 
## [194]    train-rmse:0.151824 
## [195]    train-rmse:0.151518 
## [196]    train-rmse:0.151408 
## [197]    train-rmse:0.151108 
## [198]    train-rmse:0.150927 
## [199]    train-rmse:0.150765 
## [200]    train-rmse:0.150601 
## [201]    train-rmse:0.150519 
## [202]    train-rmse:0.150249 
## [203]    train-rmse:0.150207 
## [204]    train-rmse:0.150043 
## [205]    train-rmse:0.149787 
## [206]    train-rmse:0.149570 
## [207]    train-rmse:0.149367 
## [208]    train-rmse:0.149296 
## [209]    train-rmse:0.149103 
## [210]    train-rmse:0.148973 
## [211]    train-rmse:0.148853 
## [212]    train-rmse:0.148664 
## [213]    train-rmse:0.148572 
## [214]    train-rmse:0.148524 
## [215]    train-rmse:0.148452 
## [216]    train-rmse:0.148355 
## [217]    train-rmse:0.148305 
## [218]    train-rmse:0.148156 
## [219]    train-rmse:0.148009 
## [220]    train-rmse:0.147962 
## [221]    train-rmse:0.147828 
## [222]    train-rmse:0.147725 
## [223]    train-rmse:0.147691 
## [224]    train-rmse:0.147659 
## [225]    train-rmse:0.147581 
## [226]    train-rmse:0.147494 
## [227]    train-rmse:0.147435 
## [228]    train-rmse:0.147333 
## [229]    train-rmse:0.147206 
## [230]    train-rmse:0.147068 
## [231]    train-rmse:0.146967 
## [232]    train-rmse:0.146852 
## [233]    train-rmse:0.146848 
## [234]    train-rmse:0.146840 
## [235]    train-rmse:0.146839 
## [236]    train-rmse:0.146833 
## [237]    train-rmse:0.146803 
## [238]    train-rmse:0.146725 
## [239]    train-rmse:0.146660 
## [240]    train-rmse:0.146599 
## [241]    train-rmse:0.146559 
## [242]    train-rmse:0.146514 
## [243]    train-rmse:0.146492 
## [244]    train-rmse:0.146460 
## [245]    train-rmse:0.146443 
## [246]    train-rmse:0.146407 
## [247]    train-rmse:0.146379 
## [248]    train-rmse:0.146329 
## [249]    train-rmse:0.146284 
## [250]    train-rmse:0.146229 
## [251]    train-rmse:0.146209 
## [252]    train-rmse:0.146173 
## [253]    train-rmse:0.146136 
## [254]    train-rmse:0.146077 
## [255]    train-rmse:0.146009 
## [256]    train-rmse:0.145993 
## [257]    train-rmse:0.145955 
## [258]    train-rmse:0.145944 
## [259]    train-rmse:0.145920 
## [260]    train-rmse:0.145894 
## [261]    train-rmse:0.145882 
## [262]    train-rmse:0.145872 
## [263]    train-rmse:0.145872 
## [264]    train-rmse:0.145855 
## [265]    train-rmse:0.145841 
## [266]    train-rmse:0.145820 
## [267]    train-rmse:0.145811 
## [268]    train-rmse:0.145798 
## [269]    train-rmse:0.145783 
## [270]    train-rmse:0.145769 
## [271]    train-rmse:0.145750 
## [272]    train-rmse:0.145716 
## [273]    train-rmse:0.145705 
## [274]    train-rmse:0.145696 
## [275]    train-rmse:0.145696 
## [276]    train-rmse:0.145676 
## [277]    train-rmse:0.145674 
## [278]    train-rmse:0.145667 
## [279]    train-rmse:0.145663 
## [280]    train-rmse:0.145661 
## [281]    train-rmse:0.145644 
## [282]    train-rmse:0.145640 
## [283]    train-rmse:0.145638 
## [284]    train-rmse:0.145635 
## [285]    train-rmse:0.145627 
## [286]    train-rmse:0.145622 
## [287]    train-rmse:0.145620 
## [288]    train-rmse:0.145617 
## [289]    train-rmse:0.145608 
## [290]    train-rmse:0.145605 
## [291]    train-rmse:0.145598 
## [292]    train-rmse:0.145589 
## [293]    train-rmse:0.145586 
## [294]    train-rmse:0.145564 
## [295]    train-rmse:0.145554 
## [296]    train-rmse:0.145543 
## [297]    train-rmse:0.145528 
## [298]    train-rmse:0.145504 
## [299]    train-rmse:0.145494 
## [300]    train-rmse:0.145478</code></pre>
<pre class="r"><code>print(xgb1)</code></pre>
<pre><code>## ##### xgb.Booster
## raw: 6.4 Mb 
## call:
##   xgb.train(params = params, data = dtrain, nrounds = nrounds, 
##     watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, 
##     early_stopping_rounds = early_stopping_rounds, maximize = maximize, 
##     save_period = save_period, save_name = save_name, xgb_model = xgb_model, 
##     callbacks = callbacks, max.depth = 10)
## params (as set within xgb.train):
##   max_depth = &quot;10&quot;, validate_parameters = &quot;1&quot;
## xgb.attributes:
##   niter
## callbacks:
##   cb.print.evaluation(period = print_every_n)
##   cb.evaluation.log()
## # of features: 24 
## niter: 300
## nfeatures : 24 
## evaluation_log:
##     iter train_rmse
##        1  5.3089952
##        2  3.8582693
## ---                
##      299  0.1454936
##      300  0.1454778</code></pre>
</div>
<div id="model-prediction" class="section level4">
<h4>model prediction</h4>
<pre class="r"><code>xgbpred1 = predict(xgb1, dtest)

dxg =  data.frame(
    Error = c(&quot;MSE&quot;, &quot;MAE&quot;, &quot;RMSE&quot;), 
    Value = c(mean((data_testy$label - xgbpred1)^2),
              caret::MAE(data_testy$label, xgbpred1), 
              caret::RMSE(data_testy$label, xgbpred1)))

knitr::kable(dxg)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Error</th>
<th align="right">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">MSE</td>
<td align="right">2.843923</td>
</tr>
<tr class="even">
<td align="left">MAE</td>
<td align="right">1.289155</td>
</tr>
<tr class="odd">
<td align="left">RMSE</td>
<td align="right">1.686394</td>
</tr>
</tbody>
</table>
<pre class="r"><code>x = 1:length(data_testy$label)
plot(x, data_testy$label, col = &quot;red&quot;, type = &quot;l&quot;)
lines(x, xgbpred1, col = &quot;blue&quot;, type = &quot;l&quot;)</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-29-1.png" width="768" /></p>
<pre class="r"><code>names = dimnames(as.matrix(data_testx))[[2]]
importance_matrix = xgb.importance(names, model = xgb1)
xgb.plot.importance(importance_matrix[1:10,])</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-30-1.png" width="768" /></p>
</div>
</div>
<div id="xgboost-compare-with-logit" class="section level3">
<h3>xgboost (compare with logit)</h3>
<pre class="r"><code>slp_df_new2 = slp_df_new %&gt;% 
  mutate(sufficient_slp = ifelse((ave_slp_hr &gt;= 7), 1, 0)) %&gt;% 
  select(-ave_slp_hr)</code></pre>
<pre class="r"><code>set.seed(123)
rtxg = 0.8
subxg = sample(1:nrow(slp_df_new2), round(nrow(slp_df_new2)*rt))
data_trainxg = slp_df_new2[sub,]
data_testxg = slp_df_new2[-sub,]
dim(data_trainxg)</code></pre>
<pre><code>## [1] 6132   25</code></pre>
<pre class="r"><code>dim(data_testxg)</code></pre>
<pre><code>## [1] 1533   25</code></pre>
<pre class="r"><code>data_trainx = data_trainxg[,-25]
data_trainy = data.frame(data_trainxg[,25])
names(data_trainy) = c(&#39;label&#39;)
data_testx = data_testxg[,-25]
data_testy = data.frame(data_testxg[,25])
names(data_testy) = c(&#39;label&#39;)</code></pre>
<pre class="r"><code>dtrain = xgb.DMatrix(data = as.matrix(data_trainx), label = data_trainy$label)
dtest = xgb.DMatrix(data = as.matrix(data_testx), label = data_testy$label)</code></pre>
<div id="model-training-1" class="section level4">
<h4>model training</h4>
<pre class="r"><code>xgb2 = xgboost(data = dtrain, max.depth = 10, nround = 1000, objective = &quot;binary:logistic&quot;)</code></pre>
<pre><code>## [1]  train-logloss:0.595103 
## [2]  train-logloss:0.533724 
## [3]  train-logloss:0.493842 
## [4]  train-logloss:0.460196 
## [5]  train-logloss:0.435977 
## [6]  train-logloss:0.416258 
## [7]  train-logloss:0.399244 
## [8]  train-logloss:0.385926 
## [9]  train-logloss:0.374759 
## [10] train-logloss:0.366774 
## [11] train-logloss:0.358857 
## [12] train-logloss:0.350756 
## [13] train-logloss:0.342906 
## [14] train-logloss:0.336685 
## [15] train-logloss:0.330592 
## [16] train-logloss:0.323608 
## [17] train-logloss:0.317016 
## [18] train-logloss:0.310800 
## [19] train-logloss:0.303157 
## [20] train-logloss:0.294666 
## [21] train-logloss:0.292984 
## [22] train-logloss:0.288325 
## [23] train-logloss:0.282724 
## [24] train-logloss:0.279133 
## [25] train-logloss:0.277500 
## [26] train-logloss:0.273079 
## [27] train-logloss:0.269688 
## [28] train-logloss:0.264441 
## [29] train-logloss:0.260404 
## [30] train-logloss:0.253897 
## [31] train-logloss:0.249394 
## [32] train-logloss:0.242178 
## [33] train-logloss:0.238214 
## [34] train-logloss:0.234103 
## [35] train-logloss:0.229644 
## [36] train-logloss:0.227190 
## [37] train-logloss:0.224585 
## [38] train-logloss:0.222979 
## [39] train-logloss:0.220366 
## [40] train-logloss:0.216074 
## [41] train-logloss:0.213704 
## [42] train-logloss:0.211118 
## [43] train-logloss:0.209022 
## [44] train-logloss:0.207148 
## [45] train-logloss:0.205041 
## [46] train-logloss:0.203200 
## [47] train-logloss:0.202752 
## [48] train-logloss:0.202074 
## [49] train-logloss:0.198164 
## [50] train-logloss:0.195927 
## [51] train-logloss:0.193988 
## [52] train-logloss:0.192572 
## [53] train-logloss:0.190629 
## [54] train-logloss:0.187560 
## [55] train-logloss:0.182883 
## [56] train-logloss:0.178936 
## [57] train-logloss:0.177336 
## [58] train-logloss:0.175200 
## [59] train-logloss:0.173411 
## [60] train-logloss:0.172121 
## [61] train-logloss:0.171201 
## [62] train-logloss:0.170574 
## [63] train-logloss:0.169082 
## [64] train-logloss:0.167451 
## [65] train-logloss:0.166425 
## [66] train-logloss:0.164137 
## [67] train-logloss:0.161914 
## [68] train-logloss:0.161142 
## [69] train-logloss:0.159611 
## [70] train-logloss:0.157487 
## [71] train-logloss:0.155825 
## [72] train-logloss:0.155288 
## [73] train-logloss:0.152466 
## [74] train-logloss:0.150597 
## [75] train-logloss:0.148940 
## [76] train-logloss:0.148298 
## [77] train-logloss:0.146376 
## [78] train-logloss:0.145403 
## [79] train-logloss:0.144831 
## [80] train-logloss:0.143580 
## [81] train-logloss:0.142904 
## [82] train-logloss:0.141494 
## [83] train-logloss:0.140831 
## [84] train-logloss:0.140252 
## [85] train-logloss:0.139912 
## [86] train-logloss:0.139315 
## [87] train-logloss:0.139072 
## [88] train-logloss:0.138024 
## [89] train-logloss:0.137538 
## [90] train-logloss:0.135859 
## [91] train-logloss:0.133533 
## [92] train-logloss:0.132895 
## [93] train-logloss:0.131592 
## [94] train-logloss:0.131180 
## [95] train-logloss:0.130041 
## [96] train-logloss:0.128861 
## [97] train-logloss:0.127563 
## [98] train-logloss:0.126176 
## [99] train-logloss:0.125991 
## [100]    train-logloss:0.124417 
## [101]    train-logloss:0.123124 
## [102]    train-logloss:0.122835 
## [103]    train-logloss:0.122412 
## [104]    train-logloss:0.121938 
## [105]    train-logloss:0.120869 
## [106]    train-logloss:0.120297 
## [107]    train-logloss:0.120037 
## [108]    train-logloss:0.119792 
## [109]    train-logloss:0.119515 
## [110]    train-logloss:0.119245 
## [111]    train-logloss:0.118409 
## [112]    train-logloss:0.118183 
## [113]    train-logloss:0.117582 
## [114]    train-logloss:0.115911 
## [115]    train-logloss:0.114459 
## [116]    train-logloss:0.113657 
## [117]    train-logloss:0.112834 
## [118]    train-logloss:0.112129 
## [119]    train-logloss:0.111786 
## [120]    train-logloss:0.111168 
## [121]    train-logloss:0.110780 
## [122]    train-logloss:0.110356 
## [123]    train-logloss:0.110153 
## [124]    train-logloss:0.109294 
## [125]    train-logloss:0.108302 
## [126]    train-logloss:0.107762 
## [127]    train-logloss:0.107163 
## [128]    train-logloss:0.105914 
## [129]    train-logloss:0.105150 
## [130]    train-logloss:0.104294 
## [131]    train-logloss:0.103625 
## [132]    train-logloss:0.102905 
## [133]    train-logloss:0.102122 
## [134]    train-logloss:0.101202 
## [135]    train-logloss:0.100827 
## [136]    train-logloss:0.100500 
## [137]    train-logloss:0.100193 
## [138]    train-logloss:0.100056 
## [139]    train-logloss:0.099874 
## [140]    train-logloss:0.099663 
## [141]    train-logloss:0.099178 
## [142]    train-logloss:0.098432 
## [143]    train-logloss:0.097834 
## [144]    train-logloss:0.097484 
## [145]    train-logloss:0.097184 
## [146]    train-logloss:0.096417 
## [147]    train-logloss:0.096057 
## [148]    train-logloss:0.095251 
## [149]    train-logloss:0.094579 
## [150]    train-logloss:0.093663 
## [151]    train-logloss:0.093000 
## [152]    train-logloss:0.092812 
## [153]    train-logloss:0.092529 
## [154]    train-logloss:0.092084 
## [155]    train-logloss:0.091048 
## [156]    train-logloss:0.089923 
## [157]    train-logloss:0.089581 
## [158]    train-logloss:0.089055 
## [159]    train-logloss:0.088853 
## [160]    train-logloss:0.088687 
## [161]    train-logloss:0.088273 
## [162]    train-logloss:0.087994 
## [163]    train-logloss:0.087561 
## [164]    train-logloss:0.086696 
## [165]    train-logloss:0.086590 
## [166]    train-logloss:0.086111 
## [167]    train-logloss:0.085334 
## [168]    train-logloss:0.084968 
## [169]    train-logloss:0.084609 
## [170]    train-logloss:0.084414 
## [171]    train-logloss:0.083755 
## [172]    train-logloss:0.083297 
## [173]    train-logloss:0.082799 
## [174]    train-logloss:0.082588 
## [175]    train-logloss:0.082282 
## [176]    train-logloss:0.082054 
## [177]    train-logloss:0.081886 
## [178]    train-logloss:0.081667 
## [179]    train-logloss:0.081503 
## [180]    train-logloss:0.081355 
## [181]    train-logloss:0.081249 
## [182]    train-logloss:0.080836 
## [183]    train-logloss:0.080593 
## [184]    train-logloss:0.080537 
## [185]    train-logloss:0.080096 
## [186]    train-logloss:0.079892 
## [187]    train-logloss:0.079539 
## [188]    train-logloss:0.079110 
## [189]    train-logloss:0.078746 
## [190]    train-logloss:0.078072 
## [191]    train-logloss:0.077705 
## [192]    train-logloss:0.076572 
## [193]    train-logloss:0.076094 
## [194]    train-logloss:0.075842 
## [195]    train-logloss:0.075361 
## [196]    train-logloss:0.075083 
## [197]    train-logloss:0.074735 
## [198]    train-logloss:0.074221 
## [199]    train-logloss:0.074035 
## [200]    train-logloss:0.073868 
## [201]    train-logloss:0.073606 
## [202]    train-logloss:0.073394 
## [203]    train-logloss:0.072568 
## [204]    train-logloss:0.072354 
## [205]    train-logloss:0.072181 
## [206]    train-logloss:0.071658 
## [207]    train-logloss:0.071027 
## [208]    train-logloss:0.070471 
## [209]    train-logloss:0.070288 
## [210]    train-logloss:0.069879 
## [211]    train-logloss:0.069266 
## [212]    train-logloss:0.069129 
## [213]    train-logloss:0.068916 
## [214]    train-logloss:0.068822 
## [215]    train-logloss:0.068745 
## [216]    train-logloss:0.068477 
## [217]    train-logloss:0.068115 
## [218]    train-logloss:0.067813 
## [219]    train-logloss:0.067749 
## [220]    train-logloss:0.067153 
## [221]    train-logloss:0.066844 
## [222]    train-logloss:0.066629 
## [223]    train-logloss:0.066491 
## [224]    train-logloss:0.065874 
## [225]    train-logloss:0.065567 
## [226]    train-logloss:0.065369 
## [227]    train-logloss:0.065060 
## [228]    train-logloss:0.064792 
## [229]    train-logloss:0.064406 
## [230]    train-logloss:0.064006 
## [231]    train-logloss:0.063743 
## [232]    train-logloss:0.063603 
## [233]    train-logloss:0.063065 
## [234]    train-logloss:0.062818 
## [235]    train-logloss:0.062459 
## [236]    train-logloss:0.062154 
## [237]    train-logloss:0.061956 
## [238]    train-logloss:0.061480 
## [239]    train-logloss:0.061286 
## [240]    train-logloss:0.061110 
## [241]    train-logloss:0.060894 
## [242]    train-logloss:0.060744 
## [243]    train-logloss:0.060630 
## [244]    train-logloss:0.060134 
## [245]    train-logloss:0.059707 
## [246]    train-logloss:0.059521 
## [247]    train-logloss:0.059351 
## [248]    train-logloss:0.059254 
## [249]    train-logloss:0.058993 
## [250]    train-logloss:0.058915 
## [251]    train-logloss:0.058702 
## [252]    train-logloss:0.058557 
## [253]    train-logloss:0.058386 
## [254]    train-logloss:0.058219 
## [255]    train-logloss:0.058054 
## [256]    train-logloss:0.057932 
## [257]    train-logloss:0.057740 
## [258]    train-logloss:0.057538 
## [259]    train-logloss:0.057360 
## [260]    train-logloss:0.057223 
## [261]    train-logloss:0.057196 
## [262]    train-logloss:0.057062 
## [263]    train-logloss:0.057014 
## [264]    train-logloss:0.056909 
## [265]    train-logloss:0.056778 
## [266]    train-logloss:0.056699 
## [267]    train-logloss:0.056615 
## [268]    train-logloss:0.056492 
## [269]    train-logloss:0.056423 
## [270]    train-logloss:0.056162 
## [271]    train-logloss:0.056067 
## [272]    train-logloss:0.055881 
## [273]    train-logloss:0.055560 
## [274]    train-logloss:0.055460 
## [275]    train-logloss:0.055402 
## [276]    train-logloss:0.055267 
## [277]    train-logloss:0.054943 
## [278]    train-logloss:0.054652 
## [279]    train-logloss:0.054507 
## [280]    train-logloss:0.054310 
## [281]    train-logloss:0.054027 
## [282]    train-logloss:0.053847 
## [283]    train-logloss:0.053691 
## [284]    train-logloss:0.053483 
## [285]    train-logloss:0.053347 
## [286]    train-logloss:0.053234 
## [287]    train-logloss:0.053098 
## [288]    train-logloss:0.052970 
## [289]    train-logloss:0.052881 
## [290]    train-logloss:0.052818 
## [291]    train-logloss:0.052626 
## [292]    train-logloss:0.052544 
## [293]    train-logloss:0.052478 
## [294]    train-logloss:0.052374 
## [295]    train-logloss:0.052243 
## [296]    train-logloss:0.052166 
## [297]    train-logloss:0.052081 
## [298]    train-logloss:0.052063 
## [299]    train-logloss:0.051991 
## [300]    train-logloss:0.051871 
## [301]    train-logloss:0.051814 
## [302]    train-logloss:0.051673 
## [303]    train-logloss:0.051540 
## [304]    train-logloss:0.051455 
## [305]    train-logloss:0.051351 
## [306]    train-logloss:0.051267 
## [307]    train-logloss:0.051244 
## [308]    train-logloss:0.051143 
## [309]    train-logloss:0.050904 
## [310]    train-logloss:0.050686 
## [311]    train-logloss:0.050579 
## [312]    train-logloss:0.050485 
## [313]    train-logloss:0.050344 
## [314]    train-logloss:0.050161 
## [315]    train-logloss:0.049914 
## [316]    train-logloss:0.049612 
## [317]    train-logloss:0.049445 
## [318]    train-logloss:0.049264 
## [319]    train-logloss:0.049131 
## [320]    train-logloss:0.048938 
## [321]    train-logloss:0.048823 
## [322]    train-logloss:0.048574 
## [323]    train-logloss:0.048340 
## [324]    train-logloss:0.048130 
## [325]    train-logloss:0.047894 
## [326]    train-logloss:0.047806 
## [327]    train-logloss:0.047727 
## [328]    train-logloss:0.047611 
## [329]    train-logloss:0.047453 
## [330]    train-logloss:0.047283 
## [331]    train-logloss:0.047259 
## [332]    train-logloss:0.047125 
## [333]    train-logloss:0.047040 
## [334]    train-logloss:0.046925 
## [335]    train-logloss:0.046852 
## [336]    train-logloss:0.046720 
## [337]    train-logloss:0.046612 
## [338]    train-logloss:0.046460 
## [339]    train-logloss:0.046408 
## [340]    train-logloss:0.046332 
## [341]    train-logloss:0.046208 
## [342]    train-logloss:0.046105 
## [343]    train-logloss:0.045982 
## [344]    train-logloss:0.045946 
## [345]    train-logloss:0.045877 
## [346]    train-logloss:0.045629 
## [347]    train-logloss:0.045551 
## [348]    train-logloss:0.045331 
## [349]    train-logloss:0.045238 
## [350]    train-logloss:0.045183 
## [351]    train-logloss:0.045148 
## [352]    train-logloss:0.045111 
## [353]    train-logloss:0.045046 
## [354]    train-logloss:0.045005 
## [355]    train-logloss:0.044930 
## [356]    train-logloss:0.044888 
## [357]    train-logloss:0.044639 
## [358]    train-logloss:0.044540 
## [359]    train-logloss:0.044407 
## [360]    train-logloss:0.044277 
## [361]    train-logloss:0.043993 
## [362]    train-logloss:0.043909 
## [363]    train-logloss:0.043860 
## [364]    train-logloss:0.043793 
## [365]    train-logloss:0.043698 
## [366]    train-logloss:0.043630 
## [367]    train-logloss:0.043600 
## [368]    train-logloss:0.043474 
## [369]    train-logloss:0.043390 
## [370]    train-logloss:0.043261 
## [371]    train-logloss:0.043155 
## [372]    train-logloss:0.043083 
## [373]    train-logloss:0.043026 
## [374]    train-logloss:0.042899 
## [375]    train-logloss:0.042735 
## [376]    train-logloss:0.042488 
## [377]    train-logloss:0.042388 
## [378]    train-logloss:0.042210 
## [379]    train-logloss:0.042117 
## [380]    train-logloss:0.041946 
## [381]    train-logloss:0.041889 
## [382]    train-logloss:0.041822 
## [383]    train-logloss:0.041774 
## [384]    train-logloss:0.041733 
## [385]    train-logloss:0.041712 
## [386]    train-logloss:0.041671 
## [387]    train-logloss:0.041573 
## [388]    train-logloss:0.041472 
## [389]    train-logloss:0.041348 
## [390]    train-logloss:0.041292 
## [391]    train-logloss:0.041230 
## [392]    train-logloss:0.041170 
## [393]    train-logloss:0.041076 
## [394]    train-logloss:0.041046 
## [395]    train-logloss:0.040984 
## [396]    train-logloss:0.040875 
## [397]    train-logloss:0.040758 
## [398]    train-logloss:0.040638 
## [399]    train-logloss:0.040554 
## [400]    train-logloss:0.040536 
## [401]    train-logloss:0.040492 
## [402]    train-logloss:0.040448 
## [403]    train-logloss:0.040411 
## [404]    train-logloss:0.040370 
## [405]    train-logloss:0.040347 
## [406]    train-logloss:0.040314 
## [407]    train-logloss:0.040279 
## [408]    train-logloss:0.040198 
## [409]    train-logloss:0.040156 
## [410]    train-logloss:0.040130 
## [411]    train-logloss:0.040114 
## [412]    train-logloss:0.040106 
## [413]    train-logloss:0.040048 
## [414]    train-logloss:0.039954 
## [415]    train-logloss:0.039935 
## [416]    train-logloss:0.039922 
## [417]    train-logloss:0.039763 
## [418]    train-logloss:0.039623 
## [419]    train-logloss:0.039562 
## [420]    train-logloss:0.039419 
## [421]    train-logloss:0.039353 
## [422]    train-logloss:0.039312 
## [423]    train-logloss:0.039239 
## [424]    train-logloss:0.039119 
## [425]    train-logloss:0.039015 
## [426]    train-logloss:0.038954 
## [427]    train-logloss:0.038883 
## [428]    train-logloss:0.038834 
## [429]    train-logloss:0.038786 
## [430]    train-logloss:0.038759 
## [431]    train-logloss:0.038727 
## [432]    train-logloss:0.038670 
## [433]    train-logloss:0.038603 
## [434]    train-logloss:0.038593 
## [435]    train-logloss:0.038513 
## [436]    train-logloss:0.038481 
## [437]    train-logloss:0.038447 
## [438]    train-logloss:0.038403 
## [439]    train-logloss:0.038390 
## [440]    train-logloss:0.038362 
## [441]    train-logloss:0.038306 
## [442]    train-logloss:0.038285 
## [443]    train-logloss:0.038254 
## [444]    train-logloss:0.038190 
## [445]    train-logloss:0.038031 
## [446]    train-logloss:0.037915 
## [447]    train-logloss:0.037852 
## [448]    train-logloss:0.037811 
## [449]    train-logloss:0.037718 
## [450]    train-logloss:0.037649 
## [451]    train-logloss:0.037558 
## [452]    train-logloss:0.037526 
## [453]    train-logloss:0.037488 
## [454]    train-logloss:0.037388 
## [455]    train-logloss:0.037275 
## [456]    train-logloss:0.037197 
## [457]    train-logloss:0.037141 
## [458]    train-logloss:0.036968 
## [459]    train-logloss:0.036926 
## [460]    train-logloss:0.036830 
## [461]    train-logloss:0.036719 
## [462]    train-logloss:0.036608 
## [463]    train-logloss:0.036555 
## [464]    train-logloss:0.036459 
## [465]    train-logloss:0.036365 
## [466]    train-logloss:0.036278 
## [467]    train-logloss:0.036196 
## [468]    train-logloss:0.036142 
## [469]    train-logloss:0.035949 
## [470]    train-logloss:0.035890 
## [471]    train-logloss:0.035809 
## [472]    train-logloss:0.035751 
## [473]    train-logloss:0.035687 
## [474]    train-logloss:0.035626 
## [475]    train-logloss:0.035569 
## [476]    train-logloss:0.035541 
## [477]    train-logloss:0.035462 
## [478]    train-logloss:0.035418 
## [479]    train-logloss:0.035368 
## [480]    train-logloss:0.035330 
## [481]    train-logloss:0.035271 
## [482]    train-logloss:0.035198 
## [483]    train-logloss:0.035087 
## [484]    train-logloss:0.035029 
## [485]    train-logloss:0.034990 
## [486]    train-logloss:0.034959 
## [487]    train-logloss:0.034925 
## [488]    train-logloss:0.034892 
## [489]    train-logloss:0.034863 
## [490]    train-logloss:0.034769 
## [491]    train-logloss:0.034736 
## [492]    train-logloss:0.034719 
## [493]    train-logloss:0.034686 
## [494]    train-logloss:0.034584 
## [495]    train-logloss:0.034527 
## [496]    train-logloss:0.034489 
## [497]    train-logloss:0.034436 
## [498]    train-logloss:0.034413 
## [499]    train-logloss:0.034352 
## [500]    train-logloss:0.034297 
## [501]    train-logloss:0.034280 
## [502]    train-logloss:0.034238 
## [503]    train-logloss:0.034217 
## [504]    train-logloss:0.034119 
## [505]    train-logloss:0.034089 
## [506]    train-logloss:0.034012 
## [507]    train-logloss:0.033891 
## [508]    train-logloss:0.033871 
## [509]    train-logloss:0.033718 
## [510]    train-logloss:0.033644 
## [511]    train-logloss:0.033532 
## [512]    train-logloss:0.033504 
## [513]    train-logloss:0.033486 
## [514]    train-logloss:0.033463 
## [515]    train-logloss:0.033370 
## [516]    train-logloss:0.033362 
## [517]    train-logloss:0.033277 
## [518]    train-logloss:0.033243 
## [519]    train-logloss:0.033199 
## [520]    train-logloss:0.033102 
## [521]    train-logloss:0.033000 
## [522]    train-logloss:0.032900 
## [523]    train-logloss:0.032839 
## [524]    train-logloss:0.032799 
## [525]    train-logloss:0.032772 
## [526]    train-logloss:0.032713 
## [527]    train-logloss:0.032665 
## [528]    train-logloss:0.032608 
## [529]    train-logloss:0.032564 
## [530]    train-logloss:0.032551 
## [531]    train-logloss:0.032510 
## [532]    train-logloss:0.032464 
## [533]    train-logloss:0.032381 
## [534]    train-logloss:0.032335 
## [535]    train-logloss:0.032269 
## [536]    train-logloss:0.032203 
## [537]    train-logloss:0.032134 
## [538]    train-logloss:0.032117 
## [539]    train-logloss:0.032096 
## [540]    train-logloss:0.032086 
## [541]    train-logloss:0.032067 
## [542]    train-logloss:0.032052 
## [543]    train-logloss:0.032030 
## [544]    train-logloss:0.031988 
## [545]    train-logloss:0.031959 
## [546]    train-logloss:0.031930 
## [547]    train-logloss:0.031910 
## [548]    train-logloss:0.031865 
## [549]    train-logloss:0.031845 
## [550]    train-logloss:0.031735 
## [551]    train-logloss:0.031679 
## [552]    train-logloss:0.031617 
## [553]    train-logloss:0.031579 
## [554]    train-logloss:0.031561 
## [555]    train-logloss:0.031507 
## [556]    train-logloss:0.031480 
## [557]    train-logloss:0.031461 
## [558]    train-logloss:0.031391 
## [559]    train-logloss:0.031284 
## [560]    train-logloss:0.031243 
## [561]    train-logloss:0.031197 
## [562]    train-logloss:0.031098 
## [563]    train-logloss:0.031043 
## [564]    train-logloss:0.031025 
## [565]    train-logloss:0.030984 
## [566]    train-logloss:0.030945 
## [567]    train-logloss:0.030907 
## [568]    train-logloss:0.030895 
## [569]    train-logloss:0.030836 
## [570]    train-logloss:0.030783 
## [571]    train-logloss:0.030735 
## [572]    train-logloss:0.030683 
## [573]    train-logloss:0.030641 
## [574]    train-logloss:0.030603 
## [575]    train-logloss:0.030556 
## [576]    train-logloss:0.030530 
## [577]    train-logloss:0.030515 
## [578]    train-logloss:0.030453 
## [579]    train-logloss:0.030373 
## [580]    train-logloss:0.030349 
## [581]    train-logloss:0.030315 
## [582]    train-logloss:0.030283 
## [583]    train-logloss:0.030258 
## [584]    train-logloss:0.030197 
## [585]    train-logloss:0.030164 
## [586]    train-logloss:0.030124 
## [587]    train-logloss:0.030092 
## [588]    train-logloss:0.030023 
## [589]    train-logloss:0.029981 
## [590]    train-logloss:0.029958 
## [591]    train-logloss:0.029936 
## [592]    train-logloss:0.029865 
## [593]    train-logloss:0.029820 
## [594]    train-logloss:0.029766 
## [595]    train-logloss:0.029701 
## [596]    train-logloss:0.029683 
## [597]    train-logloss:0.029657 
## [598]    train-logloss:0.029629 
## [599]    train-logloss:0.029604 
## [600]    train-logloss:0.029573 
## [601]    train-logloss:0.029538 
## [602]    train-logloss:0.029528 
## [603]    train-logloss:0.029522 
## [604]    train-logloss:0.029520 
## [605]    train-logloss:0.029494 
## [606]    train-logloss:0.029427 
## [607]    train-logloss:0.029405 
## [608]    train-logloss:0.029363 
## [609]    train-logloss:0.029316 
## [610]    train-logloss:0.029281 
## [611]    train-logloss:0.029237 
## [612]    train-logloss:0.029205 
## [613]    train-logloss:0.029191 
## [614]    train-logloss:0.029174 
## [615]    train-logloss:0.029172 
## [616]    train-logloss:0.029150 
## [617]    train-logloss:0.029137 
## [618]    train-logloss:0.029130 
## [619]    train-logloss:0.029122 
## [620]    train-logloss:0.029098 
## [621]    train-logloss:0.029058 
## [622]    train-logloss:0.029024 
## [623]    train-logloss:0.029015 
## [624]    train-logloss:0.028986 
## [625]    train-logloss:0.028962 
## [626]    train-logloss:0.028956 
## [627]    train-logloss:0.028938 
## [628]    train-logloss:0.028928 
## [629]    train-logloss:0.028911 
## [630]    train-logloss:0.028855 
## [631]    train-logloss:0.028824 
## [632]    train-logloss:0.028801 
## [633]    train-logloss:0.028767 
## [634]    train-logloss:0.028743 
## [635]    train-logloss:0.028727 
## [636]    train-logloss:0.028675 
## [637]    train-logloss:0.028656 
## [638]    train-logloss:0.028630 
## [639]    train-logloss:0.028606 
## [640]    train-logloss:0.028580 
## [641]    train-logloss:0.028565 
## [642]    train-logloss:0.028502 
## [643]    train-logloss:0.028486 
## [644]    train-logloss:0.028463 
## [645]    train-logloss:0.028444 
## [646]    train-logloss:0.028411 
## [647]    train-logloss:0.028388 
## [648]    train-logloss:0.028377 
## [649]    train-logloss:0.028361 
## [650]    train-logloss:0.028336 
## [651]    train-logloss:0.028330 
## [652]    train-logloss:0.028304 
## [653]    train-logloss:0.028288 
## [654]    train-logloss:0.028265 
## [655]    train-logloss:0.028231 
## [656]    train-logloss:0.028221 
## [657]    train-logloss:0.028207 
## [658]    train-logloss:0.028193 
## [659]    train-logloss:0.028168 
## [660]    train-logloss:0.028151 
## [661]    train-logloss:0.028087 
## [662]    train-logloss:0.028058 
## [663]    train-logloss:0.028051 
## [664]    train-logloss:0.028040 
## [665]    train-logloss:0.028037 
## [666]    train-logloss:0.027998 
## [667]    train-logloss:0.027989 
## [668]    train-logloss:0.027962 
## [669]    train-logloss:0.027955 
## [670]    train-logloss:0.027929 
## [671]    train-logloss:0.027925 
## [672]    train-logloss:0.027903 
## [673]    train-logloss:0.027879 
## [674]    train-logloss:0.027868 
## [675]    train-logloss:0.027859 
## [676]    train-logloss:0.027848 
## [677]    train-logloss:0.027812 
## [678]    train-logloss:0.027793 
## [679]    train-logloss:0.027775 
## [680]    train-logloss:0.027728 
## [681]    train-logloss:0.027712 
## [682]    train-logloss:0.027669 
## [683]    train-logloss:0.027649 
## [684]    train-logloss:0.027623 
## [685]    train-logloss:0.027592 
## [686]    train-logloss:0.027553 
## [687]    train-logloss:0.027517 
## [688]    train-logloss:0.027472 
## [689]    train-logloss:0.027450 
## [690]    train-logloss:0.027414 
## [691]    train-logloss:0.027402 
## [692]    train-logloss:0.027375 
## [693]    train-logloss:0.027352 
## [694]    train-logloss:0.027340 
## [695]    train-logloss:0.027314 
## [696]    train-logloss:0.027308 
## [697]    train-logloss:0.027290 
## [698]    train-logloss:0.027250 
## [699]    train-logloss:0.027223 
## [700]    train-logloss:0.027194 
## [701]    train-logloss:0.027177 
## [702]    train-logloss:0.027167 
## [703]    train-logloss:0.027130 
## [704]    train-logloss:0.027110 
## [705]    train-logloss:0.027091 
## [706]    train-logloss:0.027070 
## [707]    train-logloss:0.027055 
## [708]    train-logloss:0.027039 
## [709]    train-logloss:0.027011 
## [710]    train-logloss:0.026990 
## [711]    train-logloss:0.026947 
## [712]    train-logloss:0.026921 
## [713]    train-logloss:0.026900 
## [714]    train-logloss:0.026863 
## [715]    train-logloss:0.026823 
## [716]    train-logloss:0.026807 
## [717]    train-logloss:0.026799 
## [718]    train-logloss:0.026775 
## [719]    train-logloss:0.026747 
## [720]    train-logloss:0.026670 
## [721]    train-logloss:0.026630 
## [722]    train-logloss:0.026567 
## [723]    train-logloss:0.026519 
## [724]    train-logloss:0.026499 
## [725]    train-logloss:0.026439 
## [726]    train-logloss:0.026418 
## [727]    train-logloss:0.026400 
## [728]    train-logloss:0.026354 
## [729]    train-logloss:0.026319 
## [730]    train-logloss:0.026290 
## [731]    train-logloss:0.026244 
## [732]    train-logloss:0.026209 
## [733]    train-logloss:0.026165 
## [734]    train-logloss:0.026147 
## [735]    train-logloss:0.026130 
## [736]    train-logloss:0.026100 
## [737]    train-logloss:0.026074 
## [738]    train-logloss:0.026045 
## [739]    train-logloss:0.026005 
## [740]    train-logloss:0.025951 
## [741]    train-logloss:0.025918 
## [742]    train-logloss:0.025903 
## [743]    train-logloss:0.025879 
## [744]    train-logloss:0.025858 
## [745]    train-logloss:0.025825 
## [746]    train-logloss:0.025808 
## [747]    train-logloss:0.025771 
## [748]    train-logloss:0.025753 
## [749]    train-logloss:0.025741 
## [750]    train-logloss:0.025733 
## [751]    train-logloss:0.025711 
## [752]    train-logloss:0.025677 
## [753]    train-logloss:0.025639 
## [754]    train-logloss:0.025622 
## [755]    train-logloss:0.025590 
## [756]    train-logloss:0.025583 
## [757]    train-logloss:0.025562 
## [758]    train-logloss:0.025515 
## [759]    train-logloss:0.025488 
## [760]    train-logloss:0.025460 
## [761]    train-logloss:0.025445 
## [762]    train-logloss:0.025426 
## [763]    train-logloss:0.025405 
## [764]    train-logloss:0.025390 
## [765]    train-logloss:0.025367 
## [766]    train-logloss:0.025360 
## [767]    train-logloss:0.025351 
## [768]    train-logloss:0.025321 
## [769]    train-logloss:0.025299 
## [770]    train-logloss:0.025282 
## [771]    train-logloss:0.025276 
## [772]    train-logloss:0.025257 
## [773]    train-logloss:0.025248 
## [774]    train-logloss:0.025242 
## [775]    train-logloss:0.025221 
## [776]    train-logloss:0.025187 
## [777]    train-logloss:0.025177 
## [778]    train-logloss:0.025142 
## [779]    train-logloss:0.025105 
## [780]    train-logloss:0.025084 
## [781]    train-logloss:0.025068 
## [782]    train-logloss:0.025041 
## [783]    train-logloss:0.025016 
## [784]    train-logloss:0.024974 
## [785]    train-logloss:0.024958 
## [786]    train-logloss:0.024942 
## [787]    train-logloss:0.024911 
## [788]    train-logloss:0.024897 
## [789]    train-logloss:0.024876 
## [790]    train-logloss:0.024870 
## [791]    train-logloss:0.024866 
## [792]    train-logloss:0.024847 
## [793]    train-logloss:0.024838 
## [794]    train-logloss:0.024808 
## [795]    train-logloss:0.024769 
## [796]    train-logloss:0.024764 
## [797]    train-logloss:0.024761 
## [798]    train-logloss:0.024735 
## [799]    train-logloss:0.024727 
## [800]    train-logloss:0.024717 
## [801]    train-logloss:0.024706 
## [802]    train-logloss:0.024693 
## [803]    train-logloss:0.024676 
## [804]    train-logloss:0.024662 
## [805]    train-logloss:0.024646 
## [806]    train-logloss:0.024639 
## [807]    train-logloss:0.024625 
## [808]    train-logloss:0.024614 
## [809]    train-logloss:0.024604 
## [810]    train-logloss:0.024592 
## [811]    train-logloss:0.024575 
## [812]    train-logloss:0.024562 
## [813]    train-logloss:0.024541 
## [814]    train-logloss:0.024527 
## [815]    train-logloss:0.024517 
## [816]    train-logloss:0.024500 
## [817]    train-logloss:0.024490 
## [818]    train-logloss:0.024471 
## [819]    train-logloss:0.024456 
## [820]    train-logloss:0.024447 
## [821]    train-logloss:0.024424 
## [822]    train-logloss:0.024412 
## [823]    train-logloss:0.024405 
## [824]    train-logloss:0.024389 
## [825]    train-logloss:0.024379 
## [826]    train-logloss:0.024328 
## [827]    train-logloss:0.024320 
## [828]    train-logloss:0.024302 
## [829]    train-logloss:0.024276 
## [830]    train-logloss:0.024269 
## [831]    train-logloss:0.024258 
## [832]    train-logloss:0.024244 
## [833]    train-logloss:0.024225 
## [834]    train-logloss:0.024219 
## [835]    train-logloss:0.024193 
## [836]    train-logloss:0.024182 
## [837]    train-logloss:0.024172 
## [838]    train-logloss:0.024160 
## [839]    train-logloss:0.024142 
## [840]    train-logloss:0.024115 
## [841]    train-logloss:0.024086 
## [842]    train-logloss:0.024062 
## [843]    train-logloss:0.024048 
## [844]    train-logloss:0.024031 
## [845]    train-logloss:0.024012 
## [846]    train-logloss:0.024007 
## [847]    train-logloss:0.023997 
## [848]    train-logloss:0.023987 
## [849]    train-logloss:0.023959 
## [850]    train-logloss:0.023951 
## [851]    train-logloss:0.023945 
## [852]    train-logloss:0.023930 
## [853]    train-logloss:0.023922 
## [854]    train-logloss:0.023917 
## [855]    train-logloss:0.023908 
## [856]    train-logloss:0.023893 
## [857]    train-logloss:0.023888 
## [858]    train-logloss:0.023880 
## [859]    train-logloss:0.023862 
## [860]    train-logloss:0.023847 
## [861]    train-logloss:0.023842 
## [862]    train-logloss:0.023832 
## [863]    train-logloss:0.023817 
## [864]    train-logloss:0.023811 
## [865]    train-logloss:0.023787 
## [866]    train-logloss:0.023762 
## [867]    train-logloss:0.023748 
## [868]    train-logloss:0.023717 
## [869]    train-logloss:0.023705 
## [870]    train-logloss:0.023699 
## [871]    train-logloss:0.023661 
## [872]    train-logloss:0.023652 
## [873]    train-logloss:0.023644 
## [874]    train-logloss:0.023630 
## [875]    train-logloss:0.023620 
## [876]    train-logloss:0.023610 
## [877]    train-logloss:0.023603 
## [878]    train-logloss:0.023599 
## [879]    train-logloss:0.023574 
## [880]    train-logloss:0.023561 
## [881]    train-logloss:0.023543 
## [882]    train-logloss:0.023527 
## [883]    train-logloss:0.023487 
## [884]    train-logloss:0.023466 
## [885]    train-logloss:0.023448 
## [886]    train-logloss:0.023434 
## [887]    train-logloss:0.023412 
## [888]    train-logloss:0.023371 
## [889]    train-logloss:0.023338 
## [890]    train-logloss:0.023323 
## [891]    train-logloss:0.023272 
## [892]    train-logloss:0.023249 
## [893]    train-logloss:0.023220 
## [894]    train-logloss:0.023194 
## [895]    train-logloss:0.023177 
## [896]    train-logloss:0.023167 
## [897]    train-logloss:0.023145 
## [898]    train-logloss:0.023124 
## [899]    train-logloss:0.023101 
## [900]    train-logloss:0.023080 
## [901]    train-logloss:0.023057 
## [902]    train-logloss:0.023036 
## [903]    train-logloss:0.023019 
## [904]    train-logloss:0.023008 
## [905]    train-logloss:0.022997 
## [906]    train-logloss:0.022992 
## [907]    train-logloss:0.022970 
## [908]    train-logloss:0.022960 
## [909]    train-logloss:0.022937 
## [910]    train-logloss:0.022928 
## [911]    train-logloss:0.022908 
## [912]    train-logloss:0.022897 
## [913]    train-logloss:0.022878 
## [914]    train-logloss:0.022852 
## [915]    train-logloss:0.022838 
## [916]    train-logloss:0.022834 
## [917]    train-logloss:0.022828 
## [918]    train-logloss:0.022798 
## [919]    train-logloss:0.022792 
## [920]    train-logloss:0.022785 
## [921]    train-logloss:0.022758 
## [922]    train-logloss:0.022747 
## [923]    train-logloss:0.022726 
## [924]    train-logloss:0.022711 
## [925]    train-logloss:0.022701 
## [926]    train-logloss:0.022695 
## [927]    train-logloss:0.022689 
## [928]    train-logloss:0.022678 
## [929]    train-logloss:0.022659 
## [930]    train-logloss:0.022648 
## [931]    train-logloss:0.022632 
## [932]    train-logloss:0.022617 
## [933]    train-logloss:0.022610 
## [934]    train-logloss:0.022603 
## [935]    train-logloss:0.022600 
## [936]    train-logloss:0.022595 
## [937]    train-logloss:0.022579 
## [938]    train-logloss:0.022558 
## [939]    train-logloss:0.022543 
## [940]    train-logloss:0.022536 
## [941]    train-logloss:0.022531 
## [942]    train-logloss:0.022526 
## [943]    train-logloss:0.022519 
## [944]    train-logloss:0.022501 
## [945]    train-logloss:0.022466 
## [946]    train-logloss:0.022439 
## [947]    train-logloss:0.022417 
## [948]    train-logloss:0.022401 
## [949]    train-logloss:0.022389 
## [950]    train-logloss:0.022376 
## [951]    train-logloss:0.022365 
## [952]    train-logloss:0.022357 
## [953]    train-logloss:0.022335 
## [954]    train-logloss:0.022316 
## [955]    train-logloss:0.022309 
## [956]    train-logloss:0.022306 
## [957]    train-logloss:0.022294 
## [958]    train-logloss:0.022276 
## [959]    train-logloss:0.022270 
## [960]    train-logloss:0.022262 
## [961]    train-logloss:0.022252 
## [962]    train-logloss:0.022249 
## [963]    train-logloss:0.022235 
## [964]    train-logloss:0.022233 
## [965]    train-logloss:0.022222 
## [966]    train-logloss:0.022217 
## [967]    train-logloss:0.022199 
## [968]    train-logloss:0.022190 
## [969]    train-logloss:0.022172 
## [970]    train-logloss:0.022161 
## [971]    train-logloss:0.022156 
## [972]    train-logloss:0.022151 
## [973]    train-logloss:0.022133 
## [974]    train-logloss:0.022121 
## [975]    train-logloss:0.022090 
## [976]    train-logloss:0.022076 
## [977]    train-logloss:0.022046 
## [978]    train-logloss:0.022030 
## [979]    train-logloss:0.022013 
## [980]    train-logloss:0.022001 
## [981]    train-logloss:0.021982 
## [982]    train-logloss:0.021964 
## [983]    train-logloss:0.021951 
## [984]    train-logloss:0.021937 
## [985]    train-logloss:0.021919 
## [986]    train-logloss:0.021903 
## [987]    train-logloss:0.021889 
## [988]    train-logloss:0.021880 
## [989]    train-logloss:0.021868 
## [990]    train-logloss:0.021865 
## [991]    train-logloss:0.021845 
## [992]    train-logloss:0.021836 
## [993]    train-logloss:0.021822 
## [994]    train-logloss:0.021812 
## [995]    train-logloss:0.021792 
## [996]    train-logloss:0.021779 
## [997]    train-logloss:0.021770 
## [998]    train-logloss:0.021766 
## [999]    train-logloss:0.021754 
## [1000]   train-logloss:0.021747</code></pre>
<pre class="r"><code>print(xgb2)</code></pre>
<pre><code>## ##### xgb.Booster
## raw: 4.5 Mb 
## call:
##   xgb.train(params = params, data = dtrain, nrounds = nrounds, 
##     watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, 
##     early_stopping_rounds = early_stopping_rounds, maximize = maximize, 
##     save_period = save_period, save_name = save_name, xgb_model = xgb_model, 
##     callbacks = callbacks, max.depth = 10, objective = &quot;binary:logistic&quot;)
## params (as set within xgb.train):
##   max_depth = &quot;10&quot;, objective = &quot;binary:logistic&quot;, validate_parameters = &quot;TRUE&quot;
## xgb.attributes:
##   niter
## callbacks:
##   cb.print.evaluation(period = print_every_n)
##   cb.evaluation.log()
## # of features: 24 
## niter: 1000
## nfeatures : 24 
## evaluation_log:
##     iter train_logloss
##        1    0.59510259
##        2    0.53372440
## ---                   
##      999    0.02175435
##     1000    0.02174671</code></pre>
</div>
<div id="model-prediction-1" class="section level4">
<h4>model prediction</h4>
</div>
<div id="testing-1" class="section level4">
<h4>testing</h4>
<pre class="r"><code>xgbpred2 = round(predict(xgb2, dtest))
tablog = table(data_testy$label, xgbpred2, dnn=c(&quot;true&quot;,&quot;pre&quot;))
tablog</code></pre>
<pre><code>##     pre
## true   0   1
##    0  70 292
##    1 195 976</code></pre>
<p>From the table, we find: 1. the logistic model works well on
sufficient sleep. <span class="math inline">\(\frac{976}{976+195} =
83.35\%\)</span> 2. the logistic model works not well on sufficient
sleep. <span class="math inline">\(\frac{70}{70+292} = 19.34\%\)</span>
3. the total accuracy of the model prediction is <span
class="math inline">\(\frac{70+976}{70+192+976+195} =
68.23\%\)</span></p>
</div>
<div id="roc-auc-curve-1" class="section level4">
<h4>ROC-AUC curve</h4>
<pre class="r"><code># roc_curve = roc(data_testy$label, xgpred)
# names(roc_curve)
# x = 1-roc_curve$specificities
# y = roc_curve$sensitivities
# 
# ggplot(data = NULL, aes(x = x, y = y)) + 
#   geom_line(colour = &#39;red&#39;) + 
#   geom_abline(intercept = 0, slope = 1) +
#   annotate(&#39;text&#39;, x = 0.5, y = 0.5, label =paste(&#39;AUC=&#39;,round(roc_curve$auc,2))) +
#   labs(x = &#39;1-specificities&#39;,y = &#39;sensitivities&#39;, title = &#39;ROC Curve&#39;)</code></pre>
<pre class="r"><code>names = dimnames(as.matrix(data_testx))[[2]]
importance_matrix = xgb.importance(names, model = xgb2)
xgb.plot.importance(importance_matrix[1:10,])</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-38-1.png" width="768" /></p>
</div>
<div id="tree-plot" class="section level4">
<h4>Tree-plot</h4>
<pre class="r"><code># devtools::load_all()</code></pre>
<pre class="r"><code># xgb.dump(xgb2, with_stats = T)</code></pre>
<pre class="r"><code># xgpt = xgb.plot.tree(model = xgb2, trees=0:1, render=FALSE)
# export_graph(xgpt, here::here(&quot;images/tree.png&quot;))</code></pre>
</div>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
