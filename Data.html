<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Sleep and Social Factors</title>

<script src="site_libs/header-attrs-2.18/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/journal.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="site_libs/plotly-binding-4.10.1/plotly.js"></script>
<script src="site_libs/typedarray-0.1/typedarray.min.js"></script>
<link href="site_libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="site_libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="site_libs/viz-1.8.2/viz.js"></script>
<link href="site_libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="site_libs/grViz-binding-1.0.9/grViz.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Home</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="Places_data.html">Sleep &amp; Diseases</a>
</li>
<li>
  <a href="Data.html">Sleep &amp; Social factors</a>
</li>
<li>
  <a href="https://rheaaa.shinyapps.io/myshiny/?_ga=2.230249179.631808033.1670647588-175632102.1670647588">Survey about sleep</a>
</li>
<li>
  <a href="report.html">Final Report</a>
</li>
<li>
  <a href="mailto:&lt;sleep8105@gmail.com&gt;">
    <span class="fa fa-envelope fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://github.com/jchaicc/final.github.io.git">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="References.html">
    <span class="fa fa-book fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">

<div class="btn-group pull-right float-right">
<button type="button" class="btn btn-default btn-xs btn-secondary btn-sm dropdown-toggle" data-toggle="dropdown" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu dropdown-menu-right" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Sleep and Social Factors</h1>

</div>


<div id="data-sources" class="section level1">
<h1>Data Sources</h1>
<ul>
<li><p>The first data set that was used to explore the association
between sleeping and social factors was downloaded from the <a
href="https://www.cdc.gov/nchs/nhanes/index.htm">NHANES</a>
database.</p></li>
<li><p>The second data set was from <a
href="https://www.cdc.gov/sleep/data-and-statistics/adults.html">CDC</a>.</p></li>
</ul>
<div id="data-processing-and-cleaning" class="section level2">
<h2>Data Processing and Cleaning</h2>
<ul>
<li>For data from the NHANES database: The predictors and the response
variable were from different data base in NHANES. First, the the column
name for these response variables was changed to appropriate names. The
original data included numbers to represent difference levels for each
predictor, specific names was also replaced for each number. Then, the
response variable related to sleeping was from another data base. For
this data set, similar steps was used with dealing with the predictors
data set. All column names was converted to appropriate names, and
character names also replaced the numbers as well. An extra column was
added to the data set:<code>ave_slp_hr</code>, which is the average
sleeping hours per day, by calculating the mean of the weekday and
weekend sleeping hours. For the third step, all empty cells was removed.
Finally, the two cleaned data sets was joined through the patientsâ€™
unique sequence number.</li>
</ul>
<pre class="r"><code>demo=foreign::read.xport(&quot;nhanes/P_DEMO.XPT&quot;) %&gt;%
  janitor::clean_names() %&gt;%
  select(seqn,riagendr,ridageyr,ridreth3,dmdeduc2,indfmpir) %&gt;%
  drop_na() %&gt;%
  rename(gender=riagendr,
         age=ridageyr,
         race=ridreth3,
         education_level=dmdeduc2,
         income_poverty_ratio=indfmpir) %&gt;%
  mutate(gender=case_when(gender==1 ~ &quot;male&quot;,
                          gender==2 ~ &quot;female&quot;)) %&gt;%
  mutate(race=case_when(race==1 ~ &quot;Mexican American&quot;,
                        race==2 ~ &quot;Other Hispanic&quot;,
                        race==3 ~ &quot;Non-Hispanic White&quot;,
                        race==4 ~ &quot;Non-Hispanic Black&quot;,
                        race==6 ~ &quot;Non-Hispanic Asian&quot;,
                        race==7 ~ &quot;Other Race&quot;)) %&gt;%
  mutate(education_level=case_when(education_level==1 ~ &quot;less than 9th grade&quot;,
                                   education_level==2 ~ &quot;9-11th grade&quot;,
                                   education_level==3 ~ &quot;high school graduate&quot;,
                                   education_level==4 ~ &quot;college or AA degree&quot;,
                                   education_level==5 ~ &quot;college graduate or above&quot;,
                                   education_level==7 ~ &quot;refused&quot;,
                                   education_level==9 ~ &quot;don&#39;t know&quot;))


sleep_df=foreign::read.xport(&quot;nhanes/P_SLQ.XPT&quot;) %&gt;%
  janitor::clean_names() %&gt;% 
  select(seqn,slq300,slq310,sld012,slq320,slq330,sld013,slq050,slq120) %&gt;%
  rename(weekday_slp_time=slq300,
         weekday_wake_time=slq310,
         weekday_slp_hr=sld012,
         weekend_slp_time=slq320,
         weekend_wake_time=slq330,
         weekend_slp_hr=sld013,
         trouble_slp=slq050,
         sleepy_freq=slq120
         )              %&gt;%
  filter(weekday_slp_time!=77777 &amp; weekday_slp_time!= 99999) %&gt;%
  filter(weekday_wake_time!=77777 &amp; weekday_wake_time!= 99999) %&gt;%
  filter(weekend_slp_time!=77777 &amp; weekend_slp_time != 99999) %&gt;%
  filter(weekend_wake_time!= 77777 &amp; weekend_wake_time != 99999) %&gt;%
  filter(trouble_slp!=7 &amp; trouble_slp!= 9) %&gt;%
  filter(sleepy_freq!= 7 &amp; sleepy_freq!=9) %&gt;%
  drop_na() %&gt;%
  mutate(trouble_slp=case_when(trouble_slp==1 ~ &quot;yes&quot;,
                               trouble_slp==2 ~ &quot;no&quot;)) %&gt;%
  mutate(sleepy_freq=case_when(sleepy_freq==0 ~ &quot;never&quot;,
                               sleepy_freq==1 ~ &quot;once a month&quot;,
                               sleepy_freq==2 ~ &quot;2-4 per month&quot;,
                               sleepy_freq==3 ~ &quot;5-15 per month&quot;,
                               sleepy_freq==4 ~ &quot;16-30 per month&quot;,
                               ))%&gt;% 
  mutate(ave_slp_hr = (5*weekday_slp_hr + 2*weekend_slp_hr)/7)</code></pre>
<pre class="r"><code>slp_df=inner_join(demo,sleep_df,by=&quot;seqn&quot;) %&gt;%
  na_if(&quot;&quot;) %&gt;%
  na.omit()
write_csv(slp_df, &quot;data/slp_df.csv&quot;)</code></pre>
</div>
<div id="data-description" class="section level2">
<h2>Data Description</h2>
<div id="social-factors-related-to-sleeping-status-data"
class="section level4">
<h4>Social factors related to Sleeping Status Data</h4>
<p>The resulting data file of <code>slp_df</code> contains a single
dataframe with 7665 rows of data on 15 variables, the list below is our
variables of interest:</p>
<ul>
<li><code>seqn</code>. The sequence number of the candidate.</li>
<li><code>gender</code>. Gender of the candidate.</li>
<li><code>age</code>. Age of the candidate.</li>
<li><code>race</code>. Race of the candidate.</li>
<li><code>hour</code>. Time(hour) violation occurred.</li>
<li><code>education_level</code>. The education level of the
candidate.</li>
<li><code>income_poverty_ratio</code>. A ratio to classify poverty and
not poverty. Ratio greater than 1 will be considered as not poverty,
ratio less than 1 will considered as poverty.</li>
<li><code>weekday_slp_time</code>. Sleep time on weekdays or
workdays.</li>
<li><code>weekday_wake_time</code>. Wake time on weekdays and
workdays.</li>
<li><code>weekday_slp_hr</code>. Average sleeping hours on weekdays or
workdays.</li>
<li><code>weekend_slp_time</code>. Sleep time on weekends.</li>
<li><code>weekday_wake_time</code>.Wake time on weekends.</li>
<li><code>weekend_slp_hr</code>. Average sleeping hours on
weekends.</li>
<li><code>trouble_slp</code>. A response on whether telling the doctor
having trouble sleeping.</li>
<li><code>sleepy_freq</code>. Average sleeply times during daytime per
month.</li>
<li><code>ave_slp_hr</code>. Average sleeping hours per day.</li>
</ul>
</div>
</div>
</div>
<div id="exploratory-analysis" class="section level1">
<h1>Exploratory Analysis</h1>
<div id="national-trends-in-short-sleep-duration"
class="section level2">
<h2>National trends in short sleep duration</h2>
<p>Short sleep duration is based on age group recommended hours of sleep
per day and defined as less than 7 hours for adults. This plot shows
age-adjusted prevalence of adults who reported short sleep duration from
2013 to 2020. Overall, short sleep duration prevalence was higher among
males than females across years.</p>
<pre class="r"><code>year_df=read_csv(&quot;data/AdultTrends.csv&quot;) %&gt;%
  janitor::clean_names() %&gt;%
  pivot_longer(cols=c(&quot;female&quot;, &quot;male&quot;),
               names_to = &quot;sex&quot;,
               values_to = &quot;age_adjust_prev&quot;) %&gt;%
  ggplot(aes(x=year,y=age_adjust_prev,col=sex))+geom_line()+geom_point()+labs(
    title = &quot;National trends in short sleep duration&quot;,
    x = &quot;Year&quot;,
    y = &quot;Age Adjusted Prevalence %&quot;
    )

year_df+transition_reveal(year)</code></pre>
<p><img
src="Data_files/figure-html/unnamed-chunk-3-1.gif" /><!-- --></p>
</div>
<div id="education-level" class="section level2">
<h2>Education level</h2>
<p>We first want to get the distribution of sleeping hours less than 7
hours across the five different education levels. We will construct a
bar chart tabulating the average sleeping hours per week in each of the
five education levels. Gender consideration was also added into to the
bar chart in order to see a difference between female and male in each
category.</p>
<pre class="r"><code> edu_plot=slp_df %&gt;%
  filter(ave_slp_hr&lt;7)%&gt;%
  group_by(education_level,gender) %&gt;%
  summarize(ave_sleep=mean((weekday_slp_hr*5+weekend_slp_hr*2)/7)) %&gt;% 
  ungroup() %&gt;%
  ggplot(aes(x=education_level,y=ave_sleep,fill=gender))+ geom_bar(width=0.5,stat=&quot;identity&quot;)+
  viridis::scale_fill_viridis(
    name = &quot;gender&quot;,
    discrete = TRUE
  ) + geom_text(aes(label = round(ave_sleep, 2)),position = position_stack(vjust=0.9), color = &quot;white&quot;, size = 4)+
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust=1))+labs(
    title = &quot;Distribution of sleeping hours across education level&quot;,
    x = &quot;Education Level&quot;,
    y = &quot;Average Sleeping hours per day&quot;
    )
edu_plot</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-4-1.png" width="768" /></p>
<p>From the above plot, high school graduates has the least sleeping
hours, while college graduates or above has the highest. Female and male
does not have a significant difference in both groups.</p>
</div>
<div id="race" class="section level2">
<h2>Race</h2>
<p>A heat map was made to visualize sleeping hours less than 7 hours
among different races. The below plot indicates that there is a large
number of people who has a sleeping hour of 6 hours per day among the
Non-Hispanic black group. There are also many Non-Hispanic Whites who
has an average of 6 hours per day.</p>
<pre class="r"><code>race_plot=slp_df %&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  mutate(sleep_ave=(weekday_slp_hr*5+weekend_slp_hr*2)/7) %&gt;%
  group_by(race,sleep_ave) %&gt;%
  summarise(obs=n()) %&gt;%
  plot_ly(
    x = ~sleep_ave, y = ~race, z = ~obs, type = &quot;heatmap&quot;, colors = &quot;BuPu&quot;
  ) %&gt;%
  colorbar(title = &quot;Number of People&quot;, x = 1, y = 0.5) 
layout(race_plot, xaxis = list(title = &quot;Average Sleeping Hours Per Day&quot;), yaxis = list(title = &quot;Race&quot;))</code></pre>
<div id="htmlwidget-990231640db3a7e9c2ac" style="width:768px;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-990231640db3a7e9c2ac">{"x":{"visdat":{"48c86a407dfa":["function () ","plotlyVisDat"]},"cur_data":"48c86a407dfa","attrs":{"48c86a407dfa":{"x":{},"y":{},"z":{},"colors":"BuPu","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"heatmap"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"xaxis":{"domain":[0,1],"automargin":true,"title":"Average Sleeping Hours Per Day"},"yaxis":{"domain":[0,1],"automargin":true,"title":"Race","type":"category","categoryorder":"array","categoryarray":["Mexican American","Non-Hispanic Asian","Non-Hispanic Black","Non-Hispanic White","Other Hispanic","Other Race"]},"scene":{"zaxis":{"title":"obs"}},"hovermode":"closest","showlegend":false,"legend":{"yanchor":"top","y":0.5}},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"colorbar":{"title":"Number of People","ticklen":2,"len":0.5,"lenmode":"fraction","y":0.5,"yanchor":"top","x":1},"colorscale":[["0","rgba(247,252,253,1)"],["0.0416666666666667","rgba(239,247,250,1)"],["0.0833333333333333","rgba(232,241,247,1)"],["0.125","rgba(224,236,244,1)"],["0.166666666666667","rgba(213,228,239,1)"],["0.208333333333333","rgba(202,219,235,1)"],["0.25","rgba(191,211,230,1)"],["0.291666666666667","rgba(180,203,226,1)"],["0.333333333333333","rgba(169,196,222,1)"],["0.375","rgba(158,188,218,1)"],["0.416666666666667","rgba(152,175,211,1)"],["0.458333333333333","rgba(146,163,205,1)"],["0.5","rgba(140,150,198,1)"],["0.541666666666667","rgba(141,136,191,1)"],["0.583333333333333","rgba(141,122,184,1)"],["0.625","rgba(140,107,177,1)"],["0.666666666666667","rgba(139,94,170,1)"],["0.708333333333333","rgba(138,80,164,1)"],["0.75","rgba(136,65,157,1)"],["0.791666666666667","rgba(134,52,146,1)"],["0.833333333333333","rgba(132,36,135,1)"],["0.875","rgba(129,15,124,1)"],["0.916666666666667","rgba(111,8,107,1)"],["0.958333333333333","rgba(94,3,91,1)"],["1","rgba(77,0,75,1)"]],"showscale":true,"x":[3.5,3.57142857142857,3.71428571428571,4,4.28571428571429,4.42857142857143,4.5,4.71428571428571,5,5.14285714285714,5.28571428571429,5.35714285714286,5.42857142857143,5.5,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3.14285714285714,3.5,4,4.07142857142857,4.14285714285714,4.28571428571429,4.42857142857143,4.85714285714286,4.92857142857143,5,5.07142857142857,5.14285714285714,5.21428571428571,5.28571428571429,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3,3.28571428571429,3.5,3.57142857142857,3.85714285714286,3.92857142857143,4,4.07142857142857,4.14285714285714,4.21428571428571,4.28571428571429,4.35714285714286,4.42857142857143,4.5,4.57142857142857,4.64285714285714,4.71428571428571,4.78571428571429,4.85714285714286,4.92857142857143,5,5.07142857142857,5.14285714285714,5.21428571428571,5.28571428571429,5.35714285714286,5.42857142857143,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3,3.28571428571429,3.5,3.57142857142857,3.71428571428571,3.78571428571429,3.92857142857143,4,4.14285714285714,4.21428571428571,4.28571428571429,4.35714285714286,4.42857142857143,4.5,4.57142857142857,4.64285714285714,4.71428571428571,4.78571428571429,4.85714285714286,5,5.07142857142857,5.14285714285714,5.21428571428571,5.28571428571429,5.35714285714286,5.42857142857143,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3,3.5,4,4.14285714285714,4.28571428571429,4.5,4.57142857142857,4.71428571428571,4.92857142857143,5,5.07142857142857,5.14285714285714,5.28571428571429,5.35714285714286,5.42857142857143,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.78571428571429,6.85714285714286,6.92857142857143,3,3.5,3.78571428571429,3.92857142857143,4,4.28571428571429,4.42857142857143,4.57142857142857,4.85714285714286,5,5.14285714285714,5.28571428571429,5.42857142857143,5.5,5.57142857142857,5.64285714285714,5.71428571428571,5.78571428571429,5.85714285714286,5.92857142857143,6,6.07142857142857,6.14285714285714,6.21428571428571,6.28571428571429,6.35714285714286,6.42857142857143,6.5,6.57142857142857,6.64285714285714,6.71428571428571,6.85714285714286,6.92857142857143],"y":["Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Mexican American","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Asian","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic Black","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Non-Hispanic White","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Hispanic","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race","Other Race"],"z":[1,1,1,2,4,1,3,1,3,1,1,1,3,1,3,4,1,5,3,14,7,4,11,3,5,14,19,5,11,9,8,12,1,1,5,1,1,3,2,1,1,13,2,1,1,3,5,3,2,2,2,6,23,3,5,2,7,2,5,14,16,8,7,6,7,9,6,3,2,2,4,1,19,2,5,1,15,2,5,10,6,1,13,3,4,5,34,5,12,2,24,2,11,11,20,6,21,10,13,6,69,8,13,10,35,7,24,31,31,19,36,17,30,24,7,2,4,1,3,1,1,17,1,2,3,1,1,11,2,2,1,4,4,52,2,2,4,7,2,5,18,10,3,7,4,10,8,65,10,6,6,31,13,19,59,33,28,23,17,30,26,2,1,6,2,1,3,3,1,3,11,1,1,2,2,2,3,3,2,1,1,4,1,24,1,7,5,10,6,5,18,8,4,8,6,11,5,1,1,1,1,3,2,1,2,1,7,2,1,1,3,2,1,4,2,3,3,11,3,1,2,9,2,6,8,3,3,6,9,6],"type":"heatmap","xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<div id="race-gender-gap-by-education-level" class="section level3">
<h3>Race Gender Gap by Education Level</h3>
<p>The below plot demonstrates the gender gap in the patients for
different races. Male outnumber female for sleeping less than 7 hours
per day for all race, except Non-Hispanic Black and Non-Hispanic Asian.
The bubble represents the degree of the gap, along inlcuding their
education level.</p>
<pre class="r"><code>gender_plot=slp_df %&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  group_by(race,education_level) %&gt;%
  summarize(total_f=sum(gender==&quot;female&quot;),
            total_m=sum(gender==&quot;male&quot;),
            gap=total_m-total_f) %&gt;%
  mutate(text_lable=str_c(&quot;Race=&quot;,race,&quot;\nEducation level: &quot;, education_level)) %&gt;%
  plot_ly(x=~total_m,y=~total_f,text=~text_lable,color=~race,size=~gap,type=&quot;scatter&quot;,mode=&quot;markers&quot;,
          colors=&quot;viridis&quot;,sizes = c(50, 700), marker = list(opacity = 0.7))

layout(gender_plot, title = &quot;Race Gender Gap by Education Level&quot;, xaxis = list(title = &quot;Number of Male Sleeping less than 7 hrs&quot;), yaxis = list(title = &quot;Number of Female Sleeping less than 7 hrs&quot;))</code></pre>
<div id="htmlwidget-e24fc87ea502949641fd" style="width:768px;height:576px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-e24fc87ea502949641fd">{"x":{"visdat":{"48c85614170f":["function () ","plotlyVisDat"]},"cur_data":"48c85614170f","attrs":{"48c85614170f":{"x":{},"y":{},"text":{},"mode":"markers","marker":{"opacity":0.7},"color":{},"size":{},"colors":"viridis","alpha_stroke":1,"sizes":[50,700],"spans":[1,20],"type":"scatter"}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"title":"Race Gender Gap by Education Level","xaxis":{"domain":[0,1],"automargin":true,"title":"Number of Male Sleeping less than 7 hrs"},"yaxis":{"domain":[0,1],"automargin":true,"title":"Number of Female Sleeping less than 7 hrs"},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[16,13,26,21,19],"y":[11,4,22,17,13],"text":["Race=Mexican American<br />Education level: 9-11th grade","Race=Mexican American<br />Education level: college graduate or above","Race=Mexican American<br />Education level: college or AA degree","Race=Mexican American<br />Education level: high school graduate","Race=Mexican American<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(68,1,84,1)","size":[241.176470588235,292.156862745098,228.43137254902,228.43137254902,253.921568627451],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(68,1,84,1)"}},"type":"scatter","name":"Mexican American","textfont":{"color":"rgba(68,1,84,1)","size":[241.176470588235,292.156862745098,228.43137254902,228.43137254902,253.921568627451]},"error_y":{"color":"rgba(68,1,84,1)","width":[]},"error_x":{"color":"rgba(68,1,84,1)","width":[]},"line":{"color":"rgba(68,1,84,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[6,42,20,6,2],"y":[10,37,21,15,11],"text":["Race=Non-Hispanic Asian<br />Education level: 9-11th grade","Race=Non-Hispanic Asian<br />Education level: college graduate or above","Race=Non-Hispanic Asian<br />Education level: college or AA degree","Race=Non-Hispanic Asian<br />Education level: high school graduate","Race=Non-Hispanic Asian<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(65,68,135,1)","size":[126.470588235294,241.176470588235,164.705882352941,62.7450980392157,62.7450980392157],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(65,68,135,1)"}},"type":"scatter","name":"Non-Hispanic Asian","textfont":{"color":"rgba(65,68,135,1)","size":[126.470588235294,241.176470588235,164.705882352941,62.7450980392157,62.7450980392157]},"error_y":{"color":"rgba(65,68,135,1)","width":[]},"error_x":{"color":"rgba(65,68,135,1)","width":[]},"line":{"color":"rgba(65,68,135,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[34,65,131,101,11],"y":[29,75,133,60,1],"text":["Race=Non-Hispanic Black<br />Education level: 9-11th grade","Race=Non-Hispanic Black<br />Education level: college graduate or above","Race=Non-Hispanic Black<br />Education level: college or AA degree","Race=Non-Hispanic Black<br />Education level: high school graduate","Race=Non-Hispanic Black<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(42,120,142,1)","size":[241.176470588235,50,151.960784313725,700,304.901960784314],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(42,120,142,1)"}},"type":"scatter","name":"Non-Hispanic Black","textfont":{"color":"rgba(42,120,142,1)","size":[241.176470588235,50,151.960784313725,700,304.901960784314]},"error_y":{"color":"rgba(42,120,142,1)","width":[]},"error_x":{"color":"rgba(42,120,142,1)","width":[]},"line":{"color":"rgba(42,120,142,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[32,65,126,89,11],"y":[36,41,102,65,1],"text":["Race=Non-Hispanic White<br />Education level: 9-11th grade","Race=Non-Hispanic White<br />Education level: college graduate or above","Race=Non-Hispanic White<br />Education level: college or AA degree","Race=Non-Hispanic White<br />Education level: high school graduate","Race=Non-Hispanic White<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(34,168,132,1)","size":[126.470588235294,483.333333333333,483.333333333333,483.333333333333,304.901960784314],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(34,168,132,1)"}},"type":"scatter","name":"Non-Hispanic White","textfont":{"color":"rgba(34,168,132,1)","size":[126.470588235294,483.333333333333,483.333333333333,483.333333333333,304.901960784314]},"error_y":{"color":"rgba(34,168,132,1)","width":[]},"error_x":{"color":"rgba(34,168,132,1)","width":[]},"line":{"color":"rgba(34,168,132,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[13,18,27,19,16],"y":[7,16,23,14,21],"text":["Race=Other Hispanic<br />Education level: 9-11th grade","Race=Other Hispanic<br />Education level: college graduate or above","Race=Other Hispanic<br />Education level: college or AA degree","Race=Other Hispanic<br />Education level: high school graduate","Race=Other Hispanic<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(122,209,81,1)","size":[253.921568627451,202.941176470588,228.43137254902,241.176470588235,113.725490196078],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(122,209,81,1)"}},"type":"scatter","name":"Other Hispanic","textfont":{"color":"rgba(122,209,81,1)","size":[253.921568627451,202.941176470588,228.43137254902,241.176470588235,113.725490196078]},"error_y":{"color":"rgba(122,209,81,1)","width":[]},"error_x":{"color":"rgba(122,209,81,1)","width":[]},"line":{"color":"rgba(122,209,81,1)"},"xaxis":"x","yaxis":"y","frame":null},{"x":[9,9,25,18,2],"y":[2,11,23,12,0],"text":["Race=Other Race<br />Education level: 9-11th grade","Race=Other Race<br />Education level: college graduate or above","Race=Other Race<br />Education level: college or AA degree","Race=Other Race<br />Education level: high school graduate","Race=Other Race<br />Education level: less than 9th grade"],"mode":"markers","marker":{"color":"rgba(253,231,37,1)","size":[266.666666666667,151.960784313725,202.941176470588,253.921568627451,202.941176470588],"sizemode":"area","opacity":0.7,"line":{"color":"rgba(253,231,37,1)"}},"type":"scatter","name":"Other Race","textfont":{"color":"rgba(253,231,37,1)","size":[266.666666666667,151.960784313725,202.941176470588,253.921568627451,202.941176470588]},"error_y":{"color":"rgba(253,231,37,1)","width":[]},"error_x":{"color":"rgba(253,231,37,1)","width":[]},"line":{"color":"rgba(253,231,37,1)"},"xaxis":"x","yaxis":"y","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
</div>
</div>
<div id="poverty-status" class="section level2">
<h2>Poverty Status</h2>
<p>We can observe that people who are in poverty tend to have less
sleeping hours than those who are not in poverty.</p>
<pre class="r"><code>income_df=slp_df %&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  mutate(ip_stat=case_when(income_poverty_ratio &gt; 1 ~ &quot;not in poverty&quot;,
                           income_poverty_ratio &lt; 1~ &quot;in poverty&quot;,
                           income_poverty_ratio == 1~ &quot;in poverty&quot;)) %&gt;%
  ggplot(aes(x=weekday_slp_hr,y=ip_stat,fill=ip_stat))+
  geom_density_ridges(
    aes(point_color = ip_stat, point_shape = ip_stat,point_fill=ip_stat),
    alpha = .3, point_alpha = 0.7)+
   scale_x_continuous(
    breaks = c(2, 4, 6), 
    labels = c(&quot;2hrs&quot;, &quot;4hrs&quot;, &quot;6hrs&quot;),
    limits = c(2, 6)
    )+labs(
    x = &quot; Average Sleeping Hours&quot;
    )
  
box_plot=
  slp_df %&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  mutate(ip_stat=case_when(income_poverty_ratio &gt; 1 ~ &quot;not in poverty&quot;,
                           income_poverty_ratio &lt; 1~ &quot;in poverty&quot;,
                           income_poverty_ratio == 1~ &quot;in poverty&quot;)) %&gt;%
  mutate(sleep_ave=(weekday_slp_hr*5+weekend_slp_hr*2)/7) %&gt;%
  ggplot(aes(x=ip_stat,y=sleep_ave))+geom_boxplot(aes(fill = ip_stat), alpha = 0.3)+
  geom_hline(aes(yintercept=median(sleep_ave),
            color=&quot;red&quot;, linetype=&quot;dashed&quot;))+
  geom_text(aes(0, median(ave_slp_hr), label = &quot;sleep hours median&quot;), vjust = -0.5, hjust = 0, color = &quot;red&quot;)+labs(
    x = &quot; Poverty Status&quot;,
    y = &quot;Average Sleeping Hours&quot;
    )

comb=income_df+box_plot
comb+plot_annotation(
  title = &quot;Sleeping Hours By Poverty Status&quot;
) </code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-7-1.png" width="960" /></p>
</div>
<div id="age" class="section level2">
<h2>Age</h2>
<p>A bar plot was made to see the distribution of average sleeping hours
less than 7 hours in different age groups. People age between 71 and 80
has the least sleeping hours with about 5.68 hours per day.</p>
<pre class="r"><code>age_group= slp_df%&gt;%
  filter(ave_slp_hr&lt;7) %&gt;%
  mutate(age_gp=case_when(age&gt;=20 &amp; age&lt;=30 ~ &quot;20-30&quot;,
                          age&gt;=31 &amp;age &lt;=40 ~ &quot;31-40&quot;,
                          age&gt;=41 &amp;age&lt;=50 ~ &quot;41-50&quot;,
                          age&gt;=51 &amp;age&lt;=60 ~ &quot;51-60&quot;,
                          age&gt;=61 &amp;age&lt;=70 ~ &quot;61-70&quot;,
                          age&gt;=71 &amp; age &lt;=80 ~ &quot;71-80&quot;)) %&gt;%
  group_by(age_gp) %&gt;%
  summarise(ave_slp=mean((weekday_slp_hr*5+weekend_slp_hr*2)/7))%&gt;%
  ungroup() %&gt;%
  mutate(age_gp=fct_reorder(age_gp,ave_slp)) %&gt;%
  ggplot(aes(x=age_gp,y=ave_slp,fill=age_gp))+ geom_bar(stat=&quot;identity&quot;)+ scale_fill_viridis_d()+
  theme(axis.text.x = element_text(angle = -90, vjust = 0.5, hjust=1))+
  geom_text(aes(label = round(ave_slp, 2)),position = position_stack(vjust=0.9), color = &quot;white&quot;, size = 4)+labs(x=&quot;Age Group&quot;,y=&quot;Average Sleeping Hours Per Day&quot;,title=&quot;Distribution of Sleeping Hours across Age Group&quot;)
age_group</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-8-1.png" width="768" /></p>
</div>
</div>
<div id="statistical-analysis" class="section level1">
<h1>Statistical Analysis</h1>
<div id="multiple-linear-regression-mlr" class="section level2">
<h2>Multiple Linear Regression (MLR)</h2>
<div id="data" class="section level3">
<h3>Data</h3>
<pre class="r"><code>df_mod = slp_df %&gt;% 
  select(gender, age, race, education_level, income_poverty_ratio, trouble_slp, sleepy_freq, ave_slp_hr)</code></pre>
<pre class="r"><code>set.seed(1)
rt = 0.8
sub = sample(1:nrow(df_mod), round(nrow(df_mod)*rt))
data_train = df_mod[sub,]
data_test = df_mod[-sub,]
dim(data_train)</code></pre>
<pre><code>## [1] 6132    8</code></pre>
<pre class="r"><code>dim(data_test)</code></pre>
<pre><code>## [1] 1533    8</code></pre>
</div>
<div id="modeling" class="section level3">
<h3>Modeling</h3>
<pre class="r"><code>lmraw = lm(ave_slp_hr ~ ., data = data_train)
summary(lmraw)</code></pre>
<pre><code>## 
## Call:
## lm(formula = ave_slp_hr ~ ., data = data_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1483 -0.8543  0.0182  0.8587  5.9047 
## 
## Coefficients:
##                                            Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                               8.0876424  0.1144355  70.674  &lt; 2e-16
## gendermale                               -0.3136910  0.0376044  -8.342  &lt; 2e-16
## age                                       0.0003161  0.0011022   0.287 0.774282
## raceNon-Hispanic Asian                   -0.0977512  0.0826846  -1.182 0.237165
## raceNon-Hispanic Black                   -0.2831461  0.0705015  -4.016 5.99e-05
## raceNon-Hispanic White                   -0.0056274  0.0687206  -0.082 0.934738
## raceOther Hispanic                       -0.1073814  0.0830328  -1.293 0.195977
## raceOther Race                           -0.0969024  0.1031590  -0.939 0.347588
## education_levelcollege graduate or above -0.2173878  0.0745577  -2.916 0.003562
## education_levelcollege or AA degree      -0.2798869  0.0671686  -4.167 3.13e-05
## education_leveldon&#39;t know                 0.0117396  1.0337248   0.011 0.990939
## education_levelhigh school graduate      -0.1417923  0.0688075  -2.061 0.039373
## education_levelless than 9th grade       -0.0510489  0.0950936  -0.537 0.591406
## education_levelrefused                   -1.3479607  1.4601834  -0.923 0.355968
## income_poverty_ratio                     -0.0457828  0.0133222  -3.437 0.000593
## trouble_slpyes                           -0.0973259  0.0429400  -2.267 0.023452
## sleepy_freq2-4 per month                  0.3154540  0.0738469   4.272 1.97e-05
## sleepy_freq5-15 per month                 0.1221374  0.0795163   1.536 0.124589
## sleepy_freqnever                          0.3721973  0.0816463   4.559 5.25e-06
## sleepy_freqonce a month                   0.3315710  0.0774745   4.280 1.90e-05
##                                             
## (Intercept)                              ***
## gendermale                               ***
## age                                         
## raceNon-Hispanic Asian                      
## raceNon-Hispanic Black                   ***
## raceNon-Hispanic White                      
## raceOther Hispanic                          
## raceOther Race                              
## education_levelcollege graduate or above ** 
## education_levelcollege or AA degree      ***
## education_leveldon&#39;t know                   
## education_levelhigh school graduate      *  
## education_levelless than 9th grade          
## education_levelrefused                      
## income_poverty_ratio                     ***
## trouble_slpyes                           *  
## sleepy_freq2-4 per month                 ***
## sleepy_freq5-15 per month                   
## sleepy_freqnever                         ***
## sleepy_freqonce a month                  ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.457 on 6112 degrees of freedom
## Multiple R-squared:  0.02996,    Adjusted R-squared:  0.02694 
## F-statistic: 9.935 on 19 and 6112 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The model driven by multiple linear regression is <span
class="math inline">\(sleep\_hour=8.09-0.31I(gender = male)-0.31I(race =
Non-Hispanic Black)-0.22I(education_level = college graduate or
above)-0.28I(education_level = college or AA
degree)-0.05income_poverty_ratio-0.10trouble_slpyes+0.32I(sleepy_freq =
2-4 per month)+0.37I(sleepy_freq = freqnever)+0.33I(sleepy_freq =
freqonce a month)\)</span>. We find it strongly relating to gender,
trouble_types, income_poverty_ratio, and some levels in race,
education_level and sleep frequency.</p>
</div>
<div id="diagnostic-plots" class="section level3">
<h3>Diagnostic plots</h3>
<pre class="r"><code>par(mfrow = c(2,2))
plot(lmraw, cex.lab = 1)</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-12-1.png" width="768" /></p>
<p>Overall, the diagnostic plots indicates that all of the assumptions
of MLR below are met.<br />
1.Residuals are normally distributed<br />
2.Variance of residuals is constant across the range of variables<br />
3.Residuals are independent of one another</p>
</div>
<div id="mlr-under-stepwise" class="section level3">
<h3>MLR under stepwise</h3>
<pre class="r"><code>step1 = step(lmraw, direction = &#39;both&#39;, trace=0)
summary(step1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = ave_slp_hr ~ gender + race + education_level + income_poverty_ratio + 
##     trouble_slp + sleepy_freq, data = data_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.1527 -0.8536  0.0156  0.8573  5.9100 
## 
## Coefficients:
##                                           Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                               8.101124   0.104327  77.651  &lt; 2e-16
## gendermale                               -0.313685   0.037602  -8.342  &lt; 2e-16
## raceNon-Hispanic Asian                   -0.096871   0.082621  -1.172 0.241054
## raceNon-Hispanic Black                   -0.281045   0.070115  -4.008 6.19e-05
## raceNon-Hispanic White                   -0.002086   0.067597  -0.031 0.975382
## raceOther Hispanic                       -0.106063   0.082899  -1.279 0.200798
## raceOther Race                           -0.095610   0.103053  -0.928 0.353559
## education_levelcollege graduate or above -0.218982   0.074344  -2.946 0.003236
## education_levelcollege or AA degree      -0.281696   0.066867  -4.213 2.56e-05
## education_leveldon&#39;t know                 0.010356   1.033636   0.010 0.992007
## education_levelhigh school graduate      -0.142589   0.068746  -2.074 0.038108
## education_levelless than 9th grade       -0.048651   0.094718  -0.514 0.607522
## education_levelrefused                   -1.348516   1.460072  -0.924 0.355734
## income_poverty_ratio                     -0.045427   0.013263  -3.425 0.000619
## trouble_slpyes                           -0.095939   0.042664  -2.249 0.024565
## sleepy_freq2-4 per month                  0.315329   0.073840   4.270 1.98e-05
## sleepy_freq5-15 per month                 0.121501   0.079479   1.529 0.126389
## sleepy_freqnever                          0.373336   0.081544   4.578 4.78e-06
## sleepy_freqonce a month                   0.332232   0.077434   4.290 1.81e-05
##                                             
## (Intercept)                              ***
## gendermale                               ***
## raceNon-Hispanic Asian                      
## raceNon-Hispanic Black                   ***
## raceNon-Hispanic White                      
## raceOther Hispanic                          
## raceOther Race                              
## education_levelcollege graduate or above ** 
## education_levelcollege or AA degree      ***
## education_leveldon&#39;t know                   
## education_levelhigh school graduate      *  
## education_levelless than 9th grade          
## education_levelrefused                      
## income_poverty_ratio                     ***
## trouble_slpyes                           *  
## sleepy_freq2-4 per month                 ***
## sleepy_freq5-15 per month                   
## sleepy_freqnever                         ***
## sleepy_freqonce a month                  ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.457 on 6113 degrees of freedom
## Multiple R-squared:  0.02995,    Adjusted R-squared:  0.02709 
## F-statistic: 10.48 on 18 and 6113 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The model after stepwise method is <span
class="math inline">\(sleep\_hour=8.10-0.31I(gender = male)-0.28I(race =
Non-Hispanic Black)+ 0.32I(sleepy_freq = 2-4 per
month)+0.37I(sleepy_freq = freqnever)+0.33I(sleepy_freq = freqonce a
month)-0.05income_poverty_ratio-0.10trouble_slpyes-0.22I(education_level
= college graduate or above)-0.28I(education_level = college or AA
degree)\)</span> We find it is nearly the same as previous multiple
linear regression model.</p>
</div>
<div id="testing" class="section level3">
<h3>Testing</h3>
<pre class="r"><code>raw_pred = predict(lmraw, newdata = data_test)

dlm =  data.frame(
    Error = c(&quot;MSE&quot;, &quot;MAE&quot;, &quot;RMSE&quot;), 
    Value = c(mean((data_test$ave_slp_hr - raw_pred)^2),
              caret::MAE(data_test$ave_slp_hr, raw_pred), 
              caret::RMSE(data_test$ave_slp_hr, raw_pred)))

knitr::kable(dlm)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Error</th>
<th align="right">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">MSE</td>
<td align="right">2.111956</td>
</tr>
<tr class="even">
<td align="left">MAE</td>
<td align="right">1.089663</td>
</tr>
<tr class="odd">
<td align="left">RMSE</td>
<td align="right">1.453257</td>
</tr>
</tbody>
</table>
<p>The table above shows the statistical measurement factors about the
linear model.</p>
</div>
</div>
<div id="logistic" class="section level2">
<h2>Logistic</h2>
<pre class="r"><code>data_logtrain = data_train %&gt;% 
  mutate(sufficient_slp = ifelse((ave_slp_hr &gt;= 7), 1, 0)) %&gt;% 
  select(-ave_slp_hr)
glmraw = glm(sufficient_slp ~ ., data = data_logtrain, family = binomial)
summary(glmraw)</code></pre>
<pre><code>## 
## Call:
## glm(formula = sufficient_slp ~ ., family = binomial, data = data_logtrain)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0485   0.5121   0.6424   0.7626   1.3531  
## 
## Coefficients:
##                                            Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept)                               1.146e+00  1.852e-01   6.187 6.14e-10
## gendermale                               -3.906e-01  6.194e-02  -6.307 2.85e-10
## age                                      -7.509e-04  1.810e-03  -0.415  0.67817
## raceNon-Hispanic Asian                   -1.281e-01  1.446e-01  -0.886  0.37549
## raceNon-Hispanic Black                   -7.118e-01  1.189e-01  -5.987 2.14e-09
## raceNon-Hispanic White                    8.169e-03  1.196e-01   0.068  0.94556
## raceOther Hispanic                       -2.026e-01  1.421e-01  -1.426  0.15380
## raceOther Race                           -4.518e-01  1.660e-01  -2.722  0.00650
## education_levelcollege graduate or above  1.215e-01  1.237e-01   0.982  0.32635
## education_levelcollege or AA degree      -1.499e-01  1.094e-01  -1.370  0.17077
## education_leveldon&#39;t know                 1.136e+01  2.287e+02   0.050  0.96039
## education_levelhigh school graduate       1.404e-02  1.132e-01   0.124  0.90135
## education_levelless than 9th grade       -7.949e-04  1.606e-01  -0.005  0.99605
## education_levelrefused                    1.073e+01  3.247e+02   0.033  0.97364
## income_poverty_ratio                     -2.549e-02  2.181e-02  -1.169  0.24256
## trouble_slpyes                           -1.337e-01  6.901e-02  -1.937  0.05278
## sleepy_freq2-4 per month                  7.378e-01  1.110e-01   6.645 3.03e-11
## sleepy_freq5-15 per month                 3.253e-01  1.176e-01   2.767  0.00565
## sleepy_freqnever                          8.705e-01  1.271e-01   6.848 7.48e-12
## sleepy_freqonce a month                   8.279e-01  1.185e-01   6.984 2.87e-12
##                                             
## (Intercept)                              ***
## gendermale                               ***
## age                                         
## raceNon-Hispanic Asian                      
## raceNon-Hispanic Black                   ***
## raceNon-Hispanic White                      
## raceOther Hispanic                          
## raceOther Race                           ** 
## education_levelcollege graduate or above    
## education_levelcollege or AA degree         
## education_leveldon&#39;t know                   
## education_levelhigh school graduate         
## education_levelless than 9th grade          
## education_levelrefused                      
## income_poverty_ratio                        
## trouble_slpyes                           .  
## sleepy_freq2-4 per month                 ***
## sleepy_freq5-15 per month                ** 
## sleepy_freqnever                         ***
## sleepy_freqonce a month                  ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 6738.4  on 6131  degrees of freedom
## Residual deviance: 6509.2  on 6112  degrees of freedom
## AIC: 6549.2
## 
## Number of Fisher Scoring iterations: 11</code></pre>
<p>The first logistic model shows the significant predictors are gender,
race, education_level, trouble_types, and sleep_freq.</p>
<pre class="r"><code>step2 = step(glmraw, direction = &#39;both&#39;, trace=0)
summary(step2)</code></pre>
<pre><code>## 
## Call:
## glm(formula = sufficient_slp ~ gender + race + trouble_slp + 
##     sleepy_freq, family = binomial, data = data_logtrain)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.0226   0.5288   0.6476   0.7722   1.2735  
## 
## Coefficients:
##                           Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)                1.02780    0.14214   7.231 4.80e-13 ***
## gendermale                -0.38836    0.06156  -6.308 2.82e-10 ***
## raceNon-Hispanic Asian    -0.09637    0.13922  -0.692  0.48881    
## raceNon-Hispanic Black    -0.71862    0.11440  -6.281 3.35e-10 ***
## raceNon-Hispanic White    -0.01019    0.11377  -0.090  0.92865    
## raceOther Hispanic        -0.20316    0.14107  -1.440  0.14984    
## raceOther Race            -0.45807    0.16313  -2.808  0.00499 ** 
## trouble_slpyes            -0.14384    0.06851  -2.100  0.03576 *  
## sleepy_freq2-4 per month   0.73843    0.11044   6.686 2.29e-11 ***
## sleepy_freq5-15 per month  0.32642    0.11713   2.787  0.00532 ** 
## sleepy_freqnever           0.87908    0.12666   6.941 3.90e-12 ***
## sleepy_freqonce a month    0.82618    0.11788   7.009 2.41e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 6738.4  on 6131  degrees of freedom
## Residual deviance: 6522.3  on 6120  degrees of freedom
## AIC: 6546.3
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The model after stepwise method shows the significant predictors are
still gender, race, education_level, trouble_types, and sleep_freq, but
the estimated coefficients change.</p>
<pre class="r"><code>anova(object = glmraw,test = &quot;Chisq&quot;)</code></pre>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: sufficient_slp
## 
## Terms added sequentially (first to last)
## 
## 
##                      Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
## NULL                                  6131     6738.4              
## gender                1   28.957      6130     6709.4 7.402e-08 ***
## age                   1    0.106      6129     6709.3   0.74514    
## race                  5   92.495      6124     6616.8 &lt; 2.2e-16 ***
## education_level       6   15.238      6118     6601.6   0.01848 *  
## income_poverty_ratio  1    0.367      6117     6601.2   0.54443    
## trouble_slp           1   15.230      6116     6586.0 9.517e-05 ***
## sleepy_freq           4   76.823      6112     6509.2 8.199e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We did model significance test, ANOVA, and the significant predictors
for logistic regression model are the same as stepwise results.</p>
<div id="testing-1" class="section level3">
<h3>Testing</h3>
<pre class="r"><code>data_logtest = data_test %&gt;% 
  mutate(sufficient_slp = ifelse((ave_slp_hr &gt;= 7), &#39;sufficient&#39;, &#39;insufficient&#39;)) %&gt;% 
  select(-ave_slp_hr)
prob2 = round(predict(object = glmraw, newdata = data_logtest,type = &quot;response&quot;))
pred2 = ifelse(prob2 == 1, &#39;sufficient&#39;, &#39;insufficient&#39;)
pred2 = factor(pred2, levels = c(&#39;insufficient&#39;, &#39;sufficient&#39;), order=TRUE)
tablog = table(data_logtest$sufficient_slp, pred2, dnn=c(&quot;true&quot;,&quot;pre&quot;))
tablog</code></pre>
<pre><code>##               pre
## true           insufficient sufficient
##   insufficient            6        356
##   sufficient              5       1166</code></pre>
<p>From the table, we find: 1. the logistic model works well on
sufficient sleep. <span class="math inline">\(\frac{1166}{1166+5} =
99.57\%\)</span> 2. the logistic model works not well on sufficient
sleep. <span class="math inline">\(\frac{6}{6+356} = 0.02\%\)</span> 3.
the total accuracy of the model prediction is <span
class="math inline">\(\frac{1166+6}{1166+5+6+356} = 76.45\%\)</span></p>
</div>
<div id="xgboost-optimizing-mlr" class="section level3">
<h3>XGBoost (Optimizing MLR)</h3>
<div id="data-1" class="section level4">
<h4>Data</h4>
<pre class="r"><code>clafeats = c(&#39;gender&#39;, &#39;race&#39;, &#39;education_level&#39;, &#39;trouble_slp&#39;, &#39;sleepy_freq&#39;)
dums = dummyVars(~ gender + race + education_level + trouble_slp + sleepy_freq, data = df_mod)
slp_oh = as.data.frame(predict(dums, newdata = df_mod))
slp_df_new =cbind(df_mod[,-c(which(colnames(df_mod) %in% clafeats))],slp_oh)</code></pre>
<pre class="r"><code>set.seed(1)
rtxg = 0.8
subxg = sample(1:nrow(slp_df_new), round(nrow(slp_df_new)*rt))
data_trainxg = slp_df_new[sub,]
data_testxg = slp_df_new[-sub,]</code></pre>
<pre class="r"><code>data_trainx = data_trainxg[,-3]
data_trainy = data.frame(data_trainxg[,3])
names(data_trainy) = c(&#39;label&#39;)
data_testx = data_testxg[,-3]
data_testy = data.frame(data_testxg[,3])
names(data_testy) = c(&#39;label&#39;)</code></pre>
<pre class="r"><code>dtrain = xgb.DMatrix(data = as.matrix(data_trainx), label = data_trainy$label)
dtest = xgb.DMatrix(data = as.matrix(data_testx), label = data_testy$label)</code></pre>
</div>
<div id="training" class="section level4">
<h4>Training</h4>
<pre class="r"><code>xgb1 = xgboost(data=dtrain, booster = &quot;gblinear&quot;, max_depth=5, eta = 0.1, nround = 300, objective = &quot;reg:squarederror&quot;)</code></pre>
<pre><code>## [10:38:48] WARNING: amalgamation/../src/learner.cc:627: 
## Parameters: { &quot;max_depth&quot; } might not be used.
## 
##   This could be a false alarm, with some parameters getting used by language bindings but
##   then being mistakenly passed down to XGBoost core, or some parameter actually being used
##   but getting flagged wrongly here. Please open an issue if you find any such cases.
## 
## 
## [1]  train-rmse:3.477720 
## [2]  train-rmse:2.027141 
## [3]  train-rmse:1.659487 
## [4]  train-rmse:1.581352 
## [5]  train-rmse:1.559438 
## [6]  train-rmse:1.547948 
## [7]  train-rmse:1.539277 
## [8]  train-rmse:1.532361 
## [9]  train-rmse:1.526263 
## [10] train-rmse:1.521100 
## [11] train-rmse:1.516722 
## [12] train-rmse:1.512961 
## [13] train-rmse:1.509716 
## [14] train-rmse:1.507012 
## [15] train-rmse:1.504539 
## [16] train-rmse:1.502341 
## [17] train-rmse:1.500372 
## [18] train-rmse:1.498598 
## [19] train-rmse:1.497006 
## [20] train-rmse:1.495544 
## [21] train-rmse:1.494199 
## [22] train-rmse:1.492931 
## [23] train-rmse:1.491762 
## [24] train-rmse:1.490649 
## [25] train-rmse:1.489616 
## [26] train-rmse:1.488619 
## [27] train-rmse:1.487674 
## [28] train-rmse:1.486761 
## [29] train-rmse:1.485895 
## [30] train-rmse:1.485057 
## [31] train-rmse:1.484250 
## [32] train-rmse:1.483474 
## [33] train-rmse:1.482731 
## [34] train-rmse:1.482011 
## [35] train-rmse:1.481317 
## [36] train-rmse:1.480655 
## [37] train-rmse:1.480006 
## [38] train-rmse:1.479376 
## [39] train-rmse:1.478765 
## [40] train-rmse:1.478173 
## [41] train-rmse:1.477599 
## [42] train-rmse:1.477041 
## [43] train-rmse:1.476507 
## [44] train-rmse:1.475980 
## [45] train-rmse:1.475470 
## [46] train-rmse:1.474971 
## [47] train-rmse:1.474493 
## [48] train-rmse:1.474022 
## [49] train-rmse:1.473562 
## [50] train-rmse:1.473118 
## [51] train-rmse:1.472685 
## [52] train-rmse:1.472265 
## [53] train-rmse:1.471855 
## [54] train-rmse:1.471462 
## [55] train-rmse:1.471074 
## [56] train-rmse:1.470696 
## [57] train-rmse:1.470328 
## [58] train-rmse:1.469967 
## [59] train-rmse:1.469616 
## [60] train-rmse:1.469272 
## [61] train-rmse:1.468939 
## [62] train-rmse:1.468616 
## [63] train-rmse:1.468303 
## [64] train-rmse:1.467999 
## [65] train-rmse:1.467701 
## [66] train-rmse:1.467411 
## [67] train-rmse:1.467129 
## [68] train-rmse:1.466855 
## [69] train-rmse:1.466589 
## [70] train-rmse:1.466325 
## [71] train-rmse:1.466068 
## [72] train-rmse:1.465816 
## [73] train-rmse:1.465570 
## [74] train-rmse:1.465329 
## [75] train-rmse:1.465096 
## [76] train-rmse:1.464869 
## [77] train-rmse:1.464647 
## [78] train-rmse:1.464433 
## [79] train-rmse:1.464226 
## [80] train-rmse:1.464024 
## [81] train-rmse:1.463828 
## [82] train-rmse:1.463636 
## [83] train-rmse:1.463448 
## [84] train-rmse:1.463266 
## [85] train-rmse:1.463087 
## [86] train-rmse:1.462914 
## [87] train-rmse:1.462744 
## [88] train-rmse:1.462579 
## [89] train-rmse:1.462417 
## [90] train-rmse:1.462260 
## [91] train-rmse:1.462106 
## [92] train-rmse:1.461954 
## [93] train-rmse:1.461806 
## [94] train-rmse:1.461662 
## [95] train-rmse:1.461522 
## [96] train-rmse:1.461386 
## [97] train-rmse:1.461253 
## [98] train-rmse:1.461123 
## [99] train-rmse:1.460996 
## [100]    train-rmse:1.460871 
## [101]    train-rmse:1.460750 
## [102]    train-rmse:1.460631 
## [103]    train-rmse:1.460515 
## [104]    train-rmse:1.460402 
## [105]    train-rmse:1.460292 
## [106]    train-rmse:1.460184 
## [107]    train-rmse:1.460079 
## [108]    train-rmse:1.459976 
## [109]    train-rmse:1.459875 
## [110]    train-rmse:1.459777 
## [111]    train-rmse:1.459681 
## [112]    train-rmse:1.459587 
## [113]    train-rmse:1.459495 
## [114]    train-rmse:1.459405 
## [115]    train-rmse:1.459318 
## [116]    train-rmse:1.459232 
## [117]    train-rmse:1.459148 
## [118]    train-rmse:1.459066 
## [119]    train-rmse:1.458986 
## [120]    train-rmse:1.458908 
## [121]    train-rmse:1.458831 
## [122]    train-rmse:1.458756 
## [123]    train-rmse:1.458683 
## [124]    train-rmse:1.458612 
## [125]    train-rmse:1.458542 
## [126]    train-rmse:1.458473 
## [127]    train-rmse:1.458406 
## [128]    train-rmse:1.458340 
## [129]    train-rmse:1.458276 
## [130]    train-rmse:1.458213 
## [131]    train-rmse:1.458151 
## [132]    train-rmse:1.458091 
## [133]    train-rmse:1.458032 
## [134]    train-rmse:1.457974 
## [135]    train-rmse:1.457918 
## [136]    train-rmse:1.457863 
## [137]    train-rmse:1.457808 
## [138]    train-rmse:1.457756 
## [139]    train-rmse:1.457704 
## [140]    train-rmse:1.457653 
## [141]    train-rmse:1.457603 
## [142]    train-rmse:1.457555 
## [143]    train-rmse:1.457507 
## [144]    train-rmse:1.457461 
## [145]    train-rmse:1.457415 
## [146]    train-rmse:1.457370 
## [147]    train-rmse:1.457326 
## [148]    train-rmse:1.457283 
## [149]    train-rmse:1.457241 
## [150]    train-rmse:1.457201 
## [151]    train-rmse:1.457160 
## [152]    train-rmse:1.457121 
## [153]    train-rmse:1.457082 
## [154]    train-rmse:1.457044 
## [155]    train-rmse:1.457007 
## [156]    train-rmse:1.456971 
## [157]    train-rmse:1.456935 
## [158]    train-rmse:1.456900 
## [159]    train-rmse:1.456866 
## [160]    train-rmse:1.456832 
## [161]    train-rmse:1.456799 
## [162]    train-rmse:1.456767 
## [163]    train-rmse:1.456736 
## [164]    train-rmse:1.456705 
## [165]    train-rmse:1.456674 
## [166]    train-rmse:1.456645 
## [167]    train-rmse:1.456616 
## [168]    train-rmse:1.456587 
## [169]    train-rmse:1.456559 
## [170]    train-rmse:1.456532 
## [171]    train-rmse:1.456505 
## [172]    train-rmse:1.456479 
## [173]    train-rmse:1.456453 
## [174]    train-rmse:1.456427 
## [175]    train-rmse:1.456403 
## [176]    train-rmse:1.456378 
## [177]    train-rmse:1.456355 
## [178]    train-rmse:1.456331 
## [179]    train-rmse:1.456308 
## [180]    train-rmse:1.456286 
## [181]    train-rmse:1.456264 
## [182]    train-rmse:1.456242 
## [183]    train-rmse:1.456221 
## [184]    train-rmse:1.456200 
## [185]    train-rmse:1.456180 
## [186]    train-rmse:1.456160 
## [187]    train-rmse:1.456140 
## [188]    train-rmse:1.456121 
## [189]    train-rmse:1.456102 
## [190]    train-rmse:1.456084 
## [191]    train-rmse:1.456066 
## [192]    train-rmse:1.456048 
## [193]    train-rmse:1.456030 
## [194]    train-rmse:1.456013 
## [195]    train-rmse:1.455997 
## [196]    train-rmse:1.455980 
## [197]    train-rmse:1.455964 
## [198]    train-rmse:1.455948 
## [199]    train-rmse:1.455933 
## [200]    train-rmse:1.455917 
## [201]    train-rmse:1.455902 
## [202]    train-rmse:1.455887 
## [203]    train-rmse:1.455873 
## [204]    train-rmse:1.455859 
## [205]    train-rmse:1.455845 
## [206]    train-rmse:1.455832 
## [207]    train-rmse:1.455818 
## [208]    train-rmse:1.455805 
## [209]    train-rmse:1.455792 
## [210]    train-rmse:1.455780 
## [211]    train-rmse:1.455767 
## [212]    train-rmse:1.455755 
## [213]    train-rmse:1.455743 
## [214]    train-rmse:1.455732 
## [215]    train-rmse:1.455720 
## [216]    train-rmse:1.455709 
## [217]    train-rmse:1.455698 
## [218]    train-rmse:1.455687 
## [219]    train-rmse:1.455676 
## [220]    train-rmse:1.455666 
## [221]    train-rmse:1.455656 
## [222]    train-rmse:1.455646 
## [223]    train-rmse:1.455636 
## [224]    train-rmse:1.455626 
## [225]    train-rmse:1.455617 
## [226]    train-rmse:1.455607 
## [227]    train-rmse:1.455598 
## [228]    train-rmse:1.455589 
## [229]    train-rmse:1.455580 
## [230]    train-rmse:1.455571 
## [231]    train-rmse:1.455563 
## [232]    train-rmse:1.455554 
## [233]    train-rmse:1.455546 
## [234]    train-rmse:1.455538 
## [235]    train-rmse:1.455530 
## [236]    train-rmse:1.455522 
## [237]    train-rmse:1.455514 
## [238]    train-rmse:1.455507 
## [239]    train-rmse:1.455500 
## [240]    train-rmse:1.455492 
## [241]    train-rmse:1.455485 
## [242]    train-rmse:1.455479 
## [243]    train-rmse:1.455472 
## [244]    train-rmse:1.455465 
## [245]    train-rmse:1.455458 
## [246]    train-rmse:1.455452 
## [247]    train-rmse:1.455446 
## [248]    train-rmse:1.455439 
## [249]    train-rmse:1.455433 
## [250]    train-rmse:1.455427 
## [251]    train-rmse:1.455421 
## [252]    train-rmse:1.455415 
## [253]    train-rmse:1.455410 
## [254]    train-rmse:1.455404 
## [255]    train-rmse:1.455399 
## [256]    train-rmse:1.455393 
## [257]    train-rmse:1.455388 
## [258]    train-rmse:1.455383 
## [259]    train-rmse:1.455378 
## [260]    train-rmse:1.455373 
## [261]    train-rmse:1.455368 
## [262]    train-rmse:1.455363 
## [263]    train-rmse:1.455358 
## [264]    train-rmse:1.455353 
## [265]    train-rmse:1.455349 
## [266]    train-rmse:1.455344 
## [267]    train-rmse:1.455340 
## [268]    train-rmse:1.455336 
## [269]    train-rmse:1.455331 
## [270]    train-rmse:1.455327 
## [271]    train-rmse:1.455323 
## [272]    train-rmse:1.455319 
## [273]    train-rmse:1.455315 
## [274]    train-rmse:1.455311 
## [275]    train-rmse:1.455308 
## [276]    train-rmse:1.455304 
## [277]    train-rmse:1.455300 
## [278]    train-rmse:1.455297 
## [279]    train-rmse:1.455293 
## [280]    train-rmse:1.455290 
## [281]    train-rmse:1.455286 
## [282]    train-rmse:1.455283 
## [283]    train-rmse:1.455279 
## [284]    train-rmse:1.455276 
## [285]    train-rmse:1.455273 
## [286]    train-rmse:1.455270 
## [287]    train-rmse:1.455267 
## [288]    train-rmse:1.455264 
## [289]    train-rmse:1.455261 
## [290]    train-rmse:1.455258 
## [291]    train-rmse:1.455255 
## [292]    train-rmse:1.455252 
## [293]    train-rmse:1.455250 
## [294]    train-rmse:1.455247 
## [295]    train-rmse:1.455244 
## [296]    train-rmse:1.455241 
## [297]    train-rmse:1.455239 
## [298]    train-rmse:1.455236 
## [299]    train-rmse:1.455234 
## [300]    train-rmse:1.455232</code></pre>
<p>We choose the gblinear booster and reg:squarederror objective for
training XGBoost model and comparing with MLR later.</p>
</div>
<div id="testing-2" class="section level4">
<h4>Testing</h4>
<pre class="r"><code>xgbpred1 = predict(xgb1, dtest)

dxg =  data.frame(
    Error = c(&quot;MSE&quot;, &quot;MAE&quot;, &quot;RMSE&quot;), 
    Value = c(mean((data_testy$label - xgbpred1)^2),
              caret::MAE(data_testy$label, xgbpred1), 
              caret::RMSE(data_testy$label, xgbpred1)))

knitr::kable(dxg)</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">Error</th>
<th align="right">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">MSE</td>
<td align="right">2.111928</td>
</tr>
<tr class="even">
<td align="left">MAE</td>
<td align="right">1.089888</td>
</tr>
<tr class="odd">
<td align="left">RMSE</td>
<td align="right">1.453247</td>
</tr>
</tbody>
</table>
<p>We find the table above for XGBoost shows the similar statistical
measurement results with the linear model. We claim XGBoost model using
gblinear booster here does not simply work better than MLR model.</p>
<pre class="r"><code>xgb_imp1 = xgb.importance(feature_names = xgb1$feature_names, model = xgb1)
xgb.ggplot.importance(xgb_imp1, top_n =10, n_clusters = 2)</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-25-1.png" width="768" /></p>
<p>The feature importance plot of XGBoost model indicates some levels of
education_level and race factors are much important. Meanwhile, the
result differs from MLR models as race:Mexican American and
race:Non-Hispanic white are also important.</p>
</div>
</div>
<div id="xgboost-optimizing-logistic" class="section level3">
<h3>XGBoost (Optimizing Logistic)</h3>
<div id="data-2" class="section level4">
<h4>Data</h4>
<pre class="r"><code>slp_df_new2 = slp_df_new %&gt;% 
  mutate(sufficient_slp = ifelse((ave_slp_hr &gt;= 7), 1, 0)) %&gt;% 
  select(-ave_slp_hr)</code></pre>
<pre class="r"><code>set.seed(123)
rtxg = 0.8
subxg = sample(1:nrow(slp_df_new2), round(nrow(slp_df_new2)*rt))
data_trainxg = slp_df_new2[sub,]
data_testxg = slp_df_new2[-sub,]
dim(data_trainxg)</code></pre>
<pre><code>## [1] 6132   25</code></pre>
<pre class="r"><code>dim(data_testxg)</code></pre>
<pre><code>## [1] 1533   25</code></pre>
<pre class="r"><code>data_trainx = data_trainxg[,-25]
data_trainy = data.frame(data_trainxg[,25])
names(data_trainy) = c(&#39;label&#39;)
data_testx = data_testxg[,-25]
data_testy = data.frame(data_testxg[,25])
names(data_testy) = c(&#39;label&#39;)</code></pre>
<pre class="r"><code>dtrain = xgb.DMatrix(data = as.matrix(data_trainx), label = data_trainy$label)
dtest = xgb.DMatrix(data = as.matrix(data_testx), label = data_testy$label)</code></pre>
</div>
<div id="training-1" class="section level4">
<h4>Training</h4>
<pre class="r"><code>xgb2 = xgboost(data = dtrain, booster = &quot;gbtree&quot;, max.depth = 10, nround = 2000, objective = &quot;binary:logistic&quot;)</code></pre>
<pre><code>## [1]  train-logloss:0.595103 
## [2]  train-logloss:0.533724 
## [3]  train-logloss:0.493842 
## [4]  train-logloss:0.460196 
## [5]  train-logloss:0.435977 
## [6]  train-logloss:0.416258 
## [7]  train-logloss:0.399244 
## [8]  train-logloss:0.385926 
## [9]  train-logloss:0.374759 
## [10] train-logloss:0.366774 
## [11] train-logloss:0.358857 
## [12] train-logloss:0.350756 
## [13] train-logloss:0.342906 
## [14] train-logloss:0.336685 
## [15] train-logloss:0.330592 
## [16] train-logloss:0.323608 
## [17] train-logloss:0.317016 
## [18] train-logloss:0.310800 
## [19] train-logloss:0.303157 
## [20] train-logloss:0.294666 
## [21] train-logloss:0.292984 
## [22] train-logloss:0.288325 
## [23] train-logloss:0.282724 
## [24] train-logloss:0.279133 
## [25] train-logloss:0.277500 
## [26] train-logloss:0.273079 
## [27] train-logloss:0.269688 
## [28] train-logloss:0.264441 
## [29] train-logloss:0.260404 
## [30] train-logloss:0.253897 
## [31] train-logloss:0.249394 
## [32] train-logloss:0.242178 
## [33] train-logloss:0.238214 
## [34] train-logloss:0.234103 
## [35] train-logloss:0.229644 
## [36] train-logloss:0.227190 
## [37] train-logloss:0.224585 
## [38] train-logloss:0.222979 
## [39] train-logloss:0.220366 
## [40] train-logloss:0.216074 
## [41] train-logloss:0.213704 
## [42] train-logloss:0.211118 
## [43] train-logloss:0.209022 
## [44] train-logloss:0.207148 
## [45] train-logloss:0.205041 
## [46] train-logloss:0.203200 
## [47] train-logloss:0.202752 
## [48] train-logloss:0.202074 
## [49] train-logloss:0.198164 
## [50] train-logloss:0.195927 
## [51] train-logloss:0.193988 
## [52] train-logloss:0.192572 
## [53] train-logloss:0.190629 
## [54] train-logloss:0.187560 
## [55] train-logloss:0.182883 
## [56] train-logloss:0.178936 
## [57] train-logloss:0.177336 
## [58] train-logloss:0.175200 
## [59] train-logloss:0.173411 
## [60] train-logloss:0.172121 
## [61] train-logloss:0.171201 
## [62] train-logloss:0.170574 
## [63] train-logloss:0.169082 
## [64] train-logloss:0.167451 
## [65] train-logloss:0.166425 
## [66] train-logloss:0.164137 
## [67] train-logloss:0.161914 
## [68] train-logloss:0.161142 
## [69] train-logloss:0.159611 
## [70] train-logloss:0.157487 
## [71] train-logloss:0.155825 
## [72] train-logloss:0.155288 
## [73] train-logloss:0.152466 
## [74] train-logloss:0.150597 
## [75] train-logloss:0.148940 
## [76] train-logloss:0.148298 
## [77] train-logloss:0.146376 
## [78] train-logloss:0.145403 
## [79] train-logloss:0.144831 
## [80] train-logloss:0.143580 
## [81] train-logloss:0.142904 
## [82] train-logloss:0.141494 
## [83] train-logloss:0.140831 
## [84] train-logloss:0.140252 
## [85] train-logloss:0.139912 
## [86] train-logloss:0.139315 
## [87] train-logloss:0.139072 
## [88] train-logloss:0.138024 
## [89] train-logloss:0.137538 
## [90] train-logloss:0.135859 
## [91] train-logloss:0.133533 
## [92] train-logloss:0.132895 
## [93] train-logloss:0.131592 
## [94] train-logloss:0.131180 
## [95] train-logloss:0.130041 
## [96] train-logloss:0.128861 
## [97] train-logloss:0.127563 
## [98] train-logloss:0.126176 
## [99] train-logloss:0.125991 
## [100]    train-logloss:0.124417 
## [101]    train-logloss:0.123124 
## [102]    train-logloss:0.122835 
## [103]    train-logloss:0.122412 
## [104]    train-logloss:0.121938 
## [105]    train-logloss:0.120869 
## [106]    train-logloss:0.120297 
## [107]    train-logloss:0.120037 
## [108]    train-logloss:0.119792 
## [109]    train-logloss:0.119515 
## [110]    train-logloss:0.119245 
## [111]    train-logloss:0.118409 
## [112]    train-logloss:0.118183 
## [113]    train-logloss:0.117582 
## [114]    train-logloss:0.115911 
## [115]    train-logloss:0.114459 
## [116]    train-logloss:0.113657 
## [117]    train-logloss:0.112834 
## [118]    train-logloss:0.112129 
## [119]    train-logloss:0.111786 
## [120]    train-logloss:0.111168 
## [121]    train-logloss:0.110780 
## [122]    train-logloss:0.110356 
## [123]    train-logloss:0.110153 
## [124]    train-logloss:0.109294 
## [125]    train-logloss:0.108302 
## [126]    train-logloss:0.107762 
## [127]    train-logloss:0.107163 
## [128]    train-logloss:0.105914 
## [129]    train-logloss:0.105150 
## [130]    train-logloss:0.104294 
## [131]    train-logloss:0.103625 
## [132]    train-logloss:0.102905 
## [133]    train-logloss:0.102122 
## [134]    train-logloss:0.101202 
## [135]    train-logloss:0.100827 
## [136]    train-logloss:0.100500 
## [137]    train-logloss:0.100193 
## [138]    train-logloss:0.100056 
## [139]    train-logloss:0.099874 
## [140]    train-logloss:0.099663 
## [141]    train-logloss:0.099178 
## [142]    train-logloss:0.098432 
## [143]    train-logloss:0.097834 
## [144]    train-logloss:0.097484 
## [145]    train-logloss:0.097184 
## [146]    train-logloss:0.096417 
## [147]    train-logloss:0.096057 
## [148]    train-logloss:0.095251 
## [149]    train-logloss:0.094579 
## [150]    train-logloss:0.093663 
## [151]    train-logloss:0.093000 
## [152]    train-logloss:0.092812 
## [153]    train-logloss:0.092529 
## [154]    train-logloss:0.092084 
## [155]    train-logloss:0.091048 
## [156]    train-logloss:0.089923 
## [157]    train-logloss:0.089581 
## [158]    train-logloss:0.089055 
## [159]    train-logloss:0.088853 
## [160]    train-logloss:0.088687 
## [161]    train-logloss:0.088273 
## [162]    train-logloss:0.087994 
## [163]    train-logloss:0.087561 
## [164]    train-logloss:0.086696 
## [165]    train-logloss:0.086590 
## [166]    train-logloss:0.086111 
## [167]    train-logloss:0.085334 
## [168]    train-logloss:0.084968 
## [169]    train-logloss:0.084609 
## [170]    train-logloss:0.084414 
## [171]    train-logloss:0.083755 
## [172]    train-logloss:0.083297 
## [173]    train-logloss:0.082799 
## [174]    train-logloss:0.082588 
## [175]    train-logloss:0.082282 
## [176]    train-logloss:0.082054 
## [177]    train-logloss:0.081886 
## [178]    train-logloss:0.081667 
## [179]    train-logloss:0.081503 
## [180]    train-logloss:0.081355 
## [181]    train-logloss:0.081249 
## [182]    train-logloss:0.080836 
## [183]    train-logloss:0.080593 
## [184]    train-logloss:0.080537 
## [185]    train-logloss:0.080096 
## [186]    train-logloss:0.079892 
## [187]    train-logloss:0.079539 
## [188]    train-logloss:0.079110 
## [189]    train-logloss:0.078746 
## [190]    train-logloss:0.078072 
## [191]    train-logloss:0.077705 
## [192]    train-logloss:0.076572 
## [193]    train-logloss:0.076094 
## [194]    train-logloss:0.075842 
## [195]    train-logloss:0.075361 
## [196]    train-logloss:0.075083 
## [197]    train-logloss:0.074735 
## [198]    train-logloss:0.074221 
## [199]    train-logloss:0.074035 
## [200]    train-logloss:0.073868 
## [201]    train-logloss:0.073606 
## [202]    train-logloss:0.073394 
## [203]    train-logloss:0.072568 
## [204]    train-logloss:0.072354 
## [205]    train-logloss:0.072181 
## [206]    train-logloss:0.071658 
## [207]    train-logloss:0.071027 
## [208]    train-logloss:0.070471 
## [209]    train-logloss:0.070288 
## [210]    train-logloss:0.069879 
## [211]    train-logloss:0.069266 
## [212]    train-logloss:0.069129 
## [213]    train-logloss:0.068916 
## [214]    train-logloss:0.068822 
## [215]    train-logloss:0.068745 
## [216]    train-logloss:0.068477 
## [217]    train-logloss:0.068115 
## [218]    train-logloss:0.067813 
## [219]    train-logloss:0.067749 
## [220]    train-logloss:0.067153 
## [221]    train-logloss:0.066844 
## [222]    train-logloss:0.066629 
## [223]    train-logloss:0.066491 
## [224]    train-logloss:0.065874 
## [225]    train-logloss:0.065567 
## [226]    train-logloss:0.065369 
## [227]    train-logloss:0.065060 
## [228]    train-logloss:0.064792 
## [229]    train-logloss:0.064406 
## [230]    train-logloss:0.064006 
## [231]    train-logloss:0.063743 
## [232]    train-logloss:0.063603 
## [233]    train-logloss:0.063065 
## [234]    train-logloss:0.062818 
## [235]    train-logloss:0.062459 
## [236]    train-logloss:0.062154 
## [237]    train-logloss:0.061956 
## [238]    train-logloss:0.061480 
## [239]    train-logloss:0.061286 
## [240]    train-logloss:0.061110 
## [241]    train-logloss:0.060894 
## [242]    train-logloss:0.060744 
## [243]    train-logloss:0.060630 
## [244]    train-logloss:0.060134 
## [245]    train-logloss:0.059707 
## [246]    train-logloss:0.059521 
## [247]    train-logloss:0.059351 
## [248]    train-logloss:0.059254 
## [249]    train-logloss:0.058993 
## [250]    train-logloss:0.058915 
## [251]    train-logloss:0.058702 
## [252]    train-logloss:0.058557 
## [253]    train-logloss:0.058386 
## [254]    train-logloss:0.058219 
## [255]    train-logloss:0.058054 
## [256]    train-logloss:0.057932 
## [257]    train-logloss:0.057740 
## [258]    train-logloss:0.057538 
## [259]    train-logloss:0.057360 
## [260]    train-logloss:0.057223 
## [261]    train-logloss:0.057196 
## [262]    train-logloss:0.057062 
## [263]    train-logloss:0.057014 
## [264]    train-logloss:0.056909 
## [265]    train-logloss:0.056778 
## [266]    train-logloss:0.056699 
## [267]    train-logloss:0.056615 
## [268]    train-logloss:0.056492 
## [269]    train-logloss:0.056423 
## [270]    train-logloss:0.056162 
## [271]    train-logloss:0.056067 
## [272]    train-logloss:0.055881 
## [273]    train-logloss:0.055560 
## [274]    train-logloss:0.055460 
## [275]    train-logloss:0.055402 
## [276]    train-logloss:0.055267 
## [277]    train-logloss:0.054943 
## [278]    train-logloss:0.054652 
## [279]    train-logloss:0.054507 
## [280]    train-logloss:0.054310 
## [281]    train-logloss:0.054027 
## [282]    train-logloss:0.053847 
## [283]    train-logloss:0.053691 
## [284]    train-logloss:0.053483 
## [285]    train-logloss:0.053347 
## [286]    train-logloss:0.053234 
## [287]    train-logloss:0.053098 
## [288]    train-logloss:0.052970 
## [289]    train-logloss:0.052881 
## [290]    train-logloss:0.052818 
## [291]    train-logloss:0.052626 
## [292]    train-logloss:0.052544 
## [293]    train-logloss:0.052478 
## [294]    train-logloss:0.052374 
## [295]    train-logloss:0.052243 
## [296]    train-logloss:0.052166 
## [297]    train-logloss:0.052081 
## [298]    train-logloss:0.052063 
## [299]    train-logloss:0.051991 
## [300]    train-logloss:0.051871 
## [301]    train-logloss:0.051814 
## [302]    train-logloss:0.051673 
## [303]    train-logloss:0.051540 
## [304]    train-logloss:0.051455 
## [305]    train-logloss:0.051351 
## [306]    train-logloss:0.051267 
## [307]    train-logloss:0.051244 
## [308]    train-logloss:0.051143 
## [309]    train-logloss:0.050904 
## [310]    train-logloss:0.050686 
## [311]    train-logloss:0.050579 
## [312]    train-logloss:0.050485 
## [313]    train-logloss:0.050344 
## [314]    train-logloss:0.050161 
## [315]    train-logloss:0.049914 
## [316]    train-logloss:0.049612 
## [317]    train-logloss:0.049445 
## [318]    train-logloss:0.049264 
## [319]    train-logloss:0.049131 
## [320]    train-logloss:0.048938 
## [321]    train-logloss:0.048823 
## [322]    train-logloss:0.048574 
## [323]    train-logloss:0.048340 
## [324]    train-logloss:0.048130 
## [325]    train-logloss:0.047894 
## [326]    train-logloss:0.047806 
## [327]    train-logloss:0.047727 
## [328]    train-logloss:0.047611 
## [329]    train-logloss:0.047453 
## [330]    train-logloss:0.047283 
## [331]    train-logloss:0.047259 
## [332]    train-logloss:0.047125 
## [333]    train-logloss:0.047040 
## [334]    train-logloss:0.046925 
## [335]    train-logloss:0.046852 
## [336]    train-logloss:0.046720 
## [337]    train-logloss:0.046612 
## [338]    train-logloss:0.046460 
## [339]    train-logloss:0.046408 
## [340]    train-logloss:0.046332 
## [341]    train-logloss:0.046208 
## [342]    train-logloss:0.046105 
## [343]    train-logloss:0.045982 
## [344]    train-logloss:0.045946 
## [345]    train-logloss:0.045877 
## [346]    train-logloss:0.045629 
## [347]    train-logloss:0.045551 
## [348]    train-logloss:0.045331 
## [349]    train-logloss:0.045238 
## [350]    train-logloss:0.045183 
## [351]    train-logloss:0.045148 
## [352]    train-logloss:0.045111 
## [353]    train-logloss:0.045046 
## [354]    train-logloss:0.045005 
## [355]    train-logloss:0.044930 
## [356]    train-logloss:0.044888 
## [357]    train-logloss:0.044639 
## [358]    train-logloss:0.044540 
## [359]    train-logloss:0.044407 
## [360]    train-logloss:0.044277 
## [361]    train-logloss:0.043993 
## [362]    train-logloss:0.043909 
## [363]    train-logloss:0.043860 
## [364]    train-logloss:0.043793 
## [365]    train-logloss:0.043698 
## [366]    train-logloss:0.043630 
## [367]    train-logloss:0.043600 
## [368]    train-logloss:0.043474 
## [369]    train-logloss:0.043390 
## [370]    train-logloss:0.043261 
## [371]    train-logloss:0.043155 
## [372]    train-logloss:0.043083 
## [373]    train-logloss:0.043026 
## [374]    train-logloss:0.042899 
## [375]    train-logloss:0.042735 
## [376]    train-logloss:0.042488 
## [377]    train-logloss:0.042388 
## [378]    train-logloss:0.042210 
## [379]    train-logloss:0.042117 
## [380]    train-logloss:0.041946 
## [381]    train-logloss:0.041889 
## [382]    train-logloss:0.041822 
## [383]    train-logloss:0.041774 
## [384]    train-logloss:0.041733 
## [385]    train-logloss:0.041712 
## [386]    train-logloss:0.041671 
## [387]    train-logloss:0.041573 
## [388]    train-logloss:0.041472 
## [389]    train-logloss:0.041348 
## [390]    train-logloss:0.041292 
## [391]    train-logloss:0.041230 
## [392]    train-logloss:0.041170 
## [393]    train-logloss:0.041076 
## [394]    train-logloss:0.041046 
## [395]    train-logloss:0.040984 
## [396]    train-logloss:0.040875 
## [397]    train-logloss:0.040758 
## [398]    train-logloss:0.040638 
## [399]    train-logloss:0.040554 
## [400]    train-logloss:0.040536 
## [401]    train-logloss:0.040492 
## [402]    train-logloss:0.040448 
## [403]    train-logloss:0.040411 
## [404]    train-logloss:0.040370 
## [405]    train-logloss:0.040347 
## [406]    train-logloss:0.040314 
## [407]    train-logloss:0.040279 
## [408]    train-logloss:0.040198 
## [409]    train-logloss:0.040156 
## [410]    train-logloss:0.040130 
## [411]    train-logloss:0.040114 
## [412]    train-logloss:0.040106 
## [413]    train-logloss:0.040048 
## [414]    train-logloss:0.039954 
## [415]    train-logloss:0.039935 
## [416]    train-logloss:0.039922 
## [417]    train-logloss:0.039763 
## [418]    train-logloss:0.039623 
## [419]    train-logloss:0.039562 
## [420]    train-logloss:0.039419 
## [421]    train-logloss:0.039353 
## [422]    train-logloss:0.039312 
## [423]    train-logloss:0.039239 
## [424]    train-logloss:0.039119 
## [425]    train-logloss:0.039015 
## [426]    train-logloss:0.038954 
## [427]    train-logloss:0.038883 
## [428]    train-logloss:0.038834 
## [429]    train-logloss:0.038786 
## [430]    train-logloss:0.038759 
## [431]    train-logloss:0.038727 
## [432]    train-logloss:0.038670 
## [433]    train-logloss:0.038603 
## [434]    train-logloss:0.038593 
## [435]    train-logloss:0.038513 
## [436]    train-logloss:0.038481 
## [437]    train-logloss:0.038447 
## [438]    train-logloss:0.038403 
## [439]    train-logloss:0.038390 
## [440]    train-logloss:0.038362 
## [441]    train-logloss:0.038306 
## [442]    train-logloss:0.038285 
## [443]    train-logloss:0.038254 
## [444]    train-logloss:0.038190 
## [445]    train-logloss:0.038031 
## [446]    train-logloss:0.037915 
## [447]    train-logloss:0.037852 
## [448]    train-logloss:0.037811 
## [449]    train-logloss:0.037718 
## [450]    train-logloss:0.037649 
## [451]    train-logloss:0.037558 
## [452]    train-logloss:0.037526 
## [453]    train-logloss:0.037488 
## [454]    train-logloss:0.037388 
## [455]    train-logloss:0.037275 
## [456]    train-logloss:0.037197 
## [457]    train-logloss:0.037141 
## [458]    train-logloss:0.036968 
## [459]    train-logloss:0.036926 
## [460]    train-logloss:0.036830 
## [461]    train-logloss:0.036719 
## [462]    train-logloss:0.036608 
## [463]    train-logloss:0.036555 
## [464]    train-logloss:0.036459 
## [465]    train-logloss:0.036365 
## [466]    train-logloss:0.036278 
## [467]    train-logloss:0.036196 
## [468]    train-logloss:0.036142 
## [469]    train-logloss:0.035949 
## [470]    train-logloss:0.035890 
## [471]    train-logloss:0.035809 
## [472]    train-logloss:0.035751 
## [473]    train-logloss:0.035687 
## [474]    train-logloss:0.035626 
## [475]    train-logloss:0.035569 
## [476]    train-logloss:0.035541 
## [477]    train-logloss:0.035462 
## [478]    train-logloss:0.035418 
## [479]    train-logloss:0.035368 
## [480]    train-logloss:0.035330 
## [481]    train-logloss:0.035271 
## [482]    train-logloss:0.035198 
## [483]    train-logloss:0.035087 
## [484]    train-logloss:0.035029 
## [485]    train-logloss:0.034990 
## [486]    train-logloss:0.034959 
## [487]    train-logloss:0.034925 
## [488]    train-logloss:0.034892 
## [489]    train-logloss:0.034863 
## [490]    train-logloss:0.034769 
## [491]    train-logloss:0.034736 
## [492]    train-logloss:0.034719 
## [493]    train-logloss:0.034686 
## [494]    train-logloss:0.034584 
## [495]    train-logloss:0.034527 
## [496]    train-logloss:0.034489 
## [497]    train-logloss:0.034436 
## [498]    train-logloss:0.034413 
## [499]    train-logloss:0.034352 
## [500]    train-logloss:0.034297 
## [501]    train-logloss:0.034280 
## [502]    train-logloss:0.034238 
## [503]    train-logloss:0.034217 
## [504]    train-logloss:0.034119 
## [505]    train-logloss:0.034089 
## [506]    train-logloss:0.034012 
## [507]    train-logloss:0.033891 
## [508]    train-logloss:0.033871 
## [509]    train-logloss:0.033718 
## [510]    train-logloss:0.033644 
## [511]    train-logloss:0.033532 
## [512]    train-logloss:0.033504 
## [513]    train-logloss:0.033486 
## [514]    train-logloss:0.033463 
## [515]    train-logloss:0.033370 
## [516]    train-logloss:0.033362 
## [517]    train-logloss:0.033277 
## [518]    train-logloss:0.033243 
## [519]    train-logloss:0.033199 
## [520]    train-logloss:0.033102 
## [521]    train-logloss:0.033000 
## [522]    train-logloss:0.032900 
## [523]    train-logloss:0.032839 
## [524]    train-logloss:0.032799 
## [525]    train-logloss:0.032772 
## [526]    train-logloss:0.032713 
## [527]    train-logloss:0.032665 
## [528]    train-logloss:0.032608 
## [529]    train-logloss:0.032564 
## [530]    train-logloss:0.032551 
## [531]    train-logloss:0.032510 
## [532]    train-logloss:0.032464 
## [533]    train-logloss:0.032381 
## [534]    train-logloss:0.032335 
## [535]    train-logloss:0.032269 
## [536]    train-logloss:0.032203 
## [537]    train-logloss:0.032134 
## [538]    train-logloss:0.032117 
## [539]    train-logloss:0.032096 
## [540]    train-logloss:0.032086 
## [541]    train-logloss:0.032067 
## [542]    train-logloss:0.032052 
## [543]    train-logloss:0.032030 
## [544]    train-logloss:0.031988 
## [545]    train-logloss:0.031959 
## [546]    train-logloss:0.031930 
## [547]    train-logloss:0.031910 
## [548]    train-logloss:0.031865 
## [549]    train-logloss:0.031845 
## [550]    train-logloss:0.031735 
## [551]    train-logloss:0.031679 
## [552]    train-logloss:0.031617 
## [553]    train-logloss:0.031579 
## [554]    train-logloss:0.031561 
## [555]    train-logloss:0.031507 
## [556]    train-logloss:0.031480 
## [557]    train-logloss:0.031461 
## [558]    train-logloss:0.031391 
## [559]    train-logloss:0.031284 
## [560]    train-logloss:0.031243 
## [561]    train-logloss:0.031197 
## [562]    train-logloss:0.031098 
## [563]    train-logloss:0.031043 
## [564]    train-logloss:0.031025 
## [565]    train-logloss:0.030984 
## [566]    train-logloss:0.030945 
## [567]    train-logloss:0.030907 
## [568]    train-logloss:0.030895 
## [569]    train-logloss:0.030836 
## [570]    train-logloss:0.030783 
## [571]    train-logloss:0.030735 
## [572]    train-logloss:0.030683 
## [573]    train-logloss:0.030641 
## [574]    train-logloss:0.030603 
## [575]    train-logloss:0.030556 
## [576]    train-logloss:0.030530 
## [577]    train-logloss:0.030515 
## [578]    train-logloss:0.030453 
## [579]    train-logloss:0.030373 
## [580]    train-logloss:0.030349 
## [581]    train-logloss:0.030315 
## [582]    train-logloss:0.030283 
## [583]    train-logloss:0.030258 
## [584]    train-logloss:0.030197 
## [585]    train-logloss:0.030164 
## [586]    train-logloss:0.030124 
## [587]    train-logloss:0.030092 
## [588]    train-logloss:0.030023 
## [589]    train-logloss:0.029981 
## [590]    train-logloss:0.029958 
## [591]    train-logloss:0.029936 
## [592]    train-logloss:0.029865 
## [593]    train-logloss:0.029820 
## [594]    train-logloss:0.029766 
## [595]    train-logloss:0.029701 
## [596]    train-logloss:0.029683 
## [597]    train-logloss:0.029657 
## [598]    train-logloss:0.029629 
## [599]    train-logloss:0.029604 
## [600]    train-logloss:0.029573 
## [601]    train-logloss:0.029538 
## [602]    train-logloss:0.029528 
## [603]    train-logloss:0.029522 
## [604]    train-logloss:0.029520 
## [605]    train-logloss:0.029494 
## [606]    train-logloss:0.029427 
## [607]    train-logloss:0.029405 
## [608]    train-logloss:0.029363 
## [609]    train-logloss:0.029316 
## [610]    train-logloss:0.029281 
## [611]    train-logloss:0.029237 
## [612]    train-logloss:0.029205 
## [613]    train-logloss:0.029191 
## [614]    train-logloss:0.029174 
## [615]    train-logloss:0.029172 
## [616]    train-logloss:0.029150 
## [617]    train-logloss:0.029137 
## [618]    train-logloss:0.029130 
## [619]    train-logloss:0.029122 
## [620]    train-logloss:0.029098 
## [621]    train-logloss:0.029058 
## [622]    train-logloss:0.029024 
## [623]    train-logloss:0.029015 
## [624]    train-logloss:0.028986 
## [625]    train-logloss:0.028962 
## [626]    train-logloss:0.028956 
## [627]    train-logloss:0.028938 
## [628]    train-logloss:0.028928 
## [629]    train-logloss:0.028911 
## [630]    train-logloss:0.028855 
## [631]    train-logloss:0.028824 
## [632]    train-logloss:0.028801 
## [633]    train-logloss:0.028767 
## [634]    train-logloss:0.028743 
## [635]    train-logloss:0.028727 
## [636]    train-logloss:0.028675 
## [637]    train-logloss:0.028656 
## [638]    train-logloss:0.028630 
## [639]    train-logloss:0.028606 
## [640]    train-logloss:0.028580 
## [641]    train-logloss:0.028565 
## [642]    train-logloss:0.028502 
## [643]    train-logloss:0.028486 
## [644]    train-logloss:0.028463 
## [645]    train-logloss:0.028444 
## [646]    train-logloss:0.028411 
## [647]    train-logloss:0.028388 
## [648]    train-logloss:0.028377 
## [649]    train-logloss:0.028361 
## [650]    train-logloss:0.028336 
## [651]    train-logloss:0.028330 
## [652]    train-logloss:0.028304 
## [653]    train-logloss:0.028288 
## [654]    train-logloss:0.028265 
## [655]    train-logloss:0.028231 
## [656]    train-logloss:0.028221 
## [657]    train-logloss:0.028207 
## [658]    train-logloss:0.028193 
## [659]    train-logloss:0.028168 
## [660]    train-logloss:0.028151 
## [661]    train-logloss:0.028087 
## [662]    train-logloss:0.028058 
## [663]    train-logloss:0.028051 
## [664]    train-logloss:0.028040 
## [665]    train-logloss:0.028037 
## [666]    train-logloss:0.027998 
## [667]    train-logloss:0.027989 
## [668]    train-logloss:0.027962 
## [669]    train-logloss:0.027955 
## [670]    train-logloss:0.027929 
## [671]    train-logloss:0.027925 
## [672]    train-logloss:0.027903 
## [673]    train-logloss:0.027879 
## [674]    train-logloss:0.027868 
## [675]    train-logloss:0.027859 
## [676]    train-logloss:0.027848 
## [677]    train-logloss:0.027812 
## [678]    train-logloss:0.027793 
## [679]    train-logloss:0.027775 
## [680]    train-logloss:0.027728 
## [681]    train-logloss:0.027712 
## [682]    train-logloss:0.027669 
## [683]    train-logloss:0.027649 
## [684]    train-logloss:0.027623 
## [685]    train-logloss:0.027592 
## [686]    train-logloss:0.027553 
## [687]    train-logloss:0.027517 
## [688]    train-logloss:0.027472 
## [689]    train-logloss:0.027450 
## [690]    train-logloss:0.027414 
## [691]    train-logloss:0.027402 
## [692]    train-logloss:0.027375 
## [693]    train-logloss:0.027352 
## [694]    train-logloss:0.027340 
## [695]    train-logloss:0.027314 
## [696]    train-logloss:0.027308 
## [697]    train-logloss:0.027290 
## [698]    train-logloss:0.027250 
## [699]    train-logloss:0.027223 
## [700]    train-logloss:0.027194 
## [701]    train-logloss:0.027177 
## [702]    train-logloss:0.027167 
## [703]    train-logloss:0.027130 
## [704]    train-logloss:0.027110 
## [705]    train-logloss:0.027091 
## [706]    train-logloss:0.027070 
## [707]    train-logloss:0.027055 
## [708]    train-logloss:0.027039 
## [709]    train-logloss:0.027011 
## [710]    train-logloss:0.026990 
## [711]    train-logloss:0.026947 
## [712]    train-logloss:0.026921 
## [713]    train-logloss:0.026900 
## [714]    train-logloss:0.026863 
## [715]    train-logloss:0.026823 
## [716]    train-logloss:0.026807 
## [717]    train-logloss:0.026799 
## [718]    train-logloss:0.026775 
## [719]    train-logloss:0.026747 
## [720]    train-logloss:0.026670 
## [721]    train-logloss:0.026630 
## [722]    train-logloss:0.026567 
## [723]    train-logloss:0.026519 
## [724]    train-logloss:0.026499 
## [725]    train-logloss:0.026439 
## [726]    train-logloss:0.026418 
## [727]    train-logloss:0.026400 
## [728]    train-logloss:0.026354 
## [729]    train-logloss:0.026319 
## [730]    train-logloss:0.026290 
## [731]    train-logloss:0.026244 
## [732]    train-logloss:0.026209 
## [733]    train-logloss:0.026165 
## [734]    train-logloss:0.026147 
## [735]    train-logloss:0.026130 
## [736]    train-logloss:0.026100 
## [737]    train-logloss:0.026074 
## [738]    train-logloss:0.026045 
## [739]    train-logloss:0.026005 
## [740]    train-logloss:0.025951 
## [741]    train-logloss:0.025918 
## [742]    train-logloss:0.025903 
## [743]    train-logloss:0.025879 
## [744]    train-logloss:0.025858 
## [745]    train-logloss:0.025825 
## [746]    train-logloss:0.025808 
## [747]    train-logloss:0.025771 
## [748]    train-logloss:0.025753 
## [749]    train-logloss:0.025741 
## [750]    train-logloss:0.025733 
## [751]    train-logloss:0.025711 
## [752]    train-logloss:0.025677 
## [753]    train-logloss:0.025639 
## [754]    train-logloss:0.025622 
## [755]    train-logloss:0.025590 
## [756]    train-logloss:0.025583 
## [757]    train-logloss:0.025562 
## [758]    train-logloss:0.025515 
## [759]    train-logloss:0.025488 
## [760]    train-logloss:0.025460 
## [761]    train-logloss:0.025445 
## [762]    train-logloss:0.025426 
## [763]    train-logloss:0.025405 
## [764]    train-logloss:0.025390 
## [765]    train-logloss:0.025367 
## [766]    train-logloss:0.025360 
## [767]    train-logloss:0.025351 
## [768]    train-logloss:0.025321 
## [769]    train-logloss:0.025299 
## [770]    train-logloss:0.025282 
## [771]    train-logloss:0.025276 
## [772]    train-logloss:0.025257 
## [773]    train-logloss:0.025248 
## [774]    train-logloss:0.025242 
## [775]    train-logloss:0.025221 
## [776]    train-logloss:0.025187 
## [777]    train-logloss:0.025177 
## [778]    train-logloss:0.025142 
## [779]    train-logloss:0.025105 
## [780]    train-logloss:0.025084 
## [781]    train-logloss:0.025068 
## [782]    train-logloss:0.025041 
## [783]    train-logloss:0.025016 
## [784]    train-logloss:0.024974 
## [785]    train-logloss:0.024958 
## [786]    train-logloss:0.024942 
## [787]    train-logloss:0.024911 
## [788]    train-logloss:0.024897 
## [789]    train-logloss:0.024876 
## [790]    train-logloss:0.024870 
## [791]    train-logloss:0.024866 
## [792]    train-logloss:0.024847 
## [793]    train-logloss:0.024838 
## [794]    train-logloss:0.024808 
## [795]    train-logloss:0.024769 
## [796]    train-logloss:0.024764 
## [797]    train-logloss:0.024761 
## [798]    train-logloss:0.024735 
## [799]    train-logloss:0.024727 
## [800]    train-logloss:0.024717 
## [801]    train-logloss:0.024706 
## [802]    train-logloss:0.024693 
## [803]    train-logloss:0.024676 
## [804]    train-logloss:0.024662 
## [805]    train-logloss:0.024646 
## [806]    train-logloss:0.024639 
## [807]    train-logloss:0.024625 
## [808]    train-logloss:0.024614 
## [809]    train-logloss:0.024604 
## [810]    train-logloss:0.024592 
## [811]    train-logloss:0.024575 
## [812]    train-logloss:0.024562 
## [813]    train-logloss:0.024541 
## [814]    train-logloss:0.024527 
## [815]    train-logloss:0.024517 
## [816]    train-logloss:0.024500 
## [817]    train-logloss:0.024490 
## [818]    train-logloss:0.024471 
## [819]    train-logloss:0.024456 
## [820]    train-logloss:0.024447 
## [821]    train-logloss:0.024424 
## [822]    train-logloss:0.024412 
## [823]    train-logloss:0.024405 
## [824]    train-logloss:0.024389 
## [825]    train-logloss:0.024379 
## [826]    train-logloss:0.024328 
## [827]    train-logloss:0.024320 
## [828]    train-logloss:0.024302 
## [829]    train-logloss:0.024276 
## [830]    train-logloss:0.024269 
## [831]    train-logloss:0.024258 
## [832]    train-logloss:0.024244 
## [833]    train-logloss:0.024225 
## [834]    train-logloss:0.024219 
## [835]    train-logloss:0.024193 
## [836]    train-logloss:0.024182 
## [837]    train-logloss:0.024172 
## [838]    train-logloss:0.024160 
## [839]    train-logloss:0.024142 
## [840]    train-logloss:0.024115 
## [841]    train-logloss:0.024086 
## [842]    train-logloss:0.024062 
## [843]    train-logloss:0.024048 
## [844]    train-logloss:0.024031 
## [845]    train-logloss:0.024012 
## [846]    train-logloss:0.024007 
## [847]    train-logloss:0.023997 
## [848]    train-logloss:0.023987 
## [849]    train-logloss:0.023959 
## [850]    train-logloss:0.023951 
## [851]    train-logloss:0.023945 
## [852]    train-logloss:0.023930 
## [853]    train-logloss:0.023922 
## [854]    train-logloss:0.023917 
## [855]    train-logloss:0.023908 
## [856]    train-logloss:0.023893 
## [857]    train-logloss:0.023888 
## [858]    train-logloss:0.023880 
## [859]    train-logloss:0.023862 
## [860]    train-logloss:0.023847 
## [861]    train-logloss:0.023842 
## [862]    train-logloss:0.023832 
## [863]    train-logloss:0.023817 
## [864]    train-logloss:0.023811 
## [865]    train-logloss:0.023787 
## [866]    train-logloss:0.023762 
## [867]    train-logloss:0.023748 
## [868]    train-logloss:0.023717 
## [869]    train-logloss:0.023705 
## [870]    train-logloss:0.023699 
## [871]    train-logloss:0.023661 
## [872]    train-logloss:0.023652 
## [873]    train-logloss:0.023644 
## [874]    train-logloss:0.023630 
## [875]    train-logloss:0.023620 
## [876]    train-logloss:0.023610 
## [877]    train-logloss:0.023603 
## [878]    train-logloss:0.023599 
## [879]    train-logloss:0.023574 
## [880]    train-logloss:0.023561 
## [881]    train-logloss:0.023543 
## [882]    train-logloss:0.023527 
## [883]    train-logloss:0.023487 
## [884]    train-logloss:0.023466 
## [885]    train-logloss:0.023448 
## [886]    train-logloss:0.023434 
## [887]    train-logloss:0.023412 
## [888]    train-logloss:0.023371 
## [889]    train-logloss:0.023338 
## [890]    train-logloss:0.023323 
## [891]    train-logloss:0.023272 
## [892]    train-logloss:0.023249 
## [893]    train-logloss:0.023220 
## [894]    train-logloss:0.023194 
## [895]    train-logloss:0.023177 
## [896]    train-logloss:0.023167 
## [897]    train-logloss:0.023145 
## [898]    train-logloss:0.023124 
## [899]    train-logloss:0.023101 
## [900]    train-logloss:0.023080 
## [901]    train-logloss:0.023057 
## [902]    train-logloss:0.023036 
## [903]    train-logloss:0.023019 
## [904]    train-logloss:0.023008 
## [905]    train-logloss:0.022997 
## [906]    train-logloss:0.022992 
## [907]    train-logloss:0.022970 
## [908]    train-logloss:0.022960 
## [909]    train-logloss:0.022937 
## [910]    train-logloss:0.022928 
## [911]    train-logloss:0.022908 
## [912]    train-logloss:0.022897 
## [913]    train-logloss:0.022878 
## [914]    train-logloss:0.022852 
## [915]    train-logloss:0.022838 
## [916]    train-logloss:0.022834 
## [917]    train-logloss:0.022828 
## [918]    train-logloss:0.022798 
## [919]    train-logloss:0.022792 
## [920]    train-logloss:0.022785 
## [921]    train-logloss:0.022758 
## [922]    train-logloss:0.022747 
## [923]    train-logloss:0.022726 
## [924]    train-logloss:0.022711 
## [925]    train-logloss:0.022701 
## [926]    train-logloss:0.022695 
## [927]    train-logloss:0.022689 
## [928]    train-logloss:0.022678 
## [929]    train-logloss:0.022659 
## [930]    train-logloss:0.022648 
## [931]    train-logloss:0.022632 
## [932]    train-logloss:0.022617 
## [933]    train-logloss:0.022610 
## [934]    train-logloss:0.022603 
## [935]    train-logloss:0.022600 
## [936]    train-logloss:0.022595 
## [937]    train-logloss:0.022579 
## [938]    train-logloss:0.022558 
## [939]    train-logloss:0.022543 
## [940]    train-logloss:0.022536 
## [941]    train-logloss:0.022531 
## [942]    train-logloss:0.022526 
## [943]    train-logloss:0.022519 
## [944]    train-logloss:0.022501 
## [945]    train-logloss:0.022466 
## [946]    train-logloss:0.022439 
## [947]    train-logloss:0.022417 
## [948]    train-logloss:0.022401 
## [949]    train-logloss:0.022389 
## [950]    train-logloss:0.022376 
## [951]    train-logloss:0.022365 
## [952]    train-logloss:0.022357 
## [953]    train-logloss:0.022335 
## [954]    train-logloss:0.022316 
## [955]    train-logloss:0.022309 
## [956]    train-logloss:0.022306 
## [957]    train-logloss:0.022294 
## [958]    train-logloss:0.022276 
## [959]    train-logloss:0.022270 
## [960]    train-logloss:0.022262 
## [961]    train-logloss:0.022252 
## [962]    train-logloss:0.022249 
## [963]    train-logloss:0.022235 
## [964]    train-logloss:0.022233 
## [965]    train-logloss:0.022222 
## [966]    train-logloss:0.022217 
## [967]    train-logloss:0.022199 
## [968]    train-logloss:0.022190 
## [969]    train-logloss:0.022172 
## [970]    train-logloss:0.022161 
## [971]    train-logloss:0.022156 
## [972]    train-logloss:0.022151 
## [973]    train-logloss:0.022133 
## [974]    train-logloss:0.022121 
## [975]    train-logloss:0.022090 
## [976]    train-logloss:0.022076 
## [977]    train-logloss:0.022046 
## [978]    train-logloss:0.022030 
## [979]    train-logloss:0.022013 
## [980]    train-logloss:0.022001 
## [981]    train-logloss:0.021982 
## [982]    train-logloss:0.021964 
## [983]    train-logloss:0.021951 
## [984]    train-logloss:0.021937 
## [985]    train-logloss:0.021919 
## [986]    train-logloss:0.021903 
## [987]    train-logloss:0.021889 
## [988]    train-logloss:0.021880 
## [989]    train-logloss:0.021868 
## [990]    train-logloss:0.021865 
## [991]    train-logloss:0.021845 
## [992]    train-logloss:0.021836 
## [993]    train-logloss:0.021822 
## [994]    train-logloss:0.021812 
## [995]    train-logloss:0.021792 
## [996]    train-logloss:0.021779 
## [997]    train-logloss:0.021770 
## [998]    train-logloss:0.021766 
## [999]    train-logloss:0.021754 
## [1000]   train-logloss:0.021747 
## [1001]   train-logloss:0.021741 
## [1002]   train-logloss:0.021738 
## [1003]   train-logloss:0.021732 
## [1004]   train-logloss:0.021721 
## [1005]   train-logloss:0.021704 
## [1006]   train-logloss:0.021679 
## [1007]   train-logloss:0.021662 
## [1008]   train-logloss:0.021641 
## [1009]   train-logloss:0.021631 
## [1010]   train-logloss:0.021614 
## [1011]   train-logloss:0.021601 
## [1012]   train-logloss:0.021591 
## [1013]   train-logloss:0.021586 
## [1014]   train-logloss:0.021580 
## [1015]   train-logloss:0.021564 
## [1016]   train-logloss:0.021556 
## [1017]   train-logloss:0.021550 
## [1018]   train-logloss:0.021541 
## [1019]   train-logloss:0.021536 
## [1020]   train-logloss:0.021519 
## [1021]   train-logloss:0.021490 
## [1022]   train-logloss:0.021474 
## [1023]   train-logloss:0.021463 
## [1024]   train-logloss:0.021444 
## [1025]   train-logloss:0.021433 
## [1026]   train-logloss:0.021423 
## [1027]   train-logloss:0.021411 
## [1028]   train-logloss:0.021398 
## [1029]   train-logloss:0.021395 
## [1030]   train-logloss:0.021387 
## [1031]   train-logloss:0.021377 
## [1032]   train-logloss:0.021372 
## [1033]   train-logloss:0.021364 
## [1034]   train-logloss:0.021353 
## [1035]   train-logloss:0.021326 
## [1036]   train-logloss:0.021314 
## [1037]   train-logloss:0.021292 
## [1038]   train-logloss:0.021286 
## [1039]   train-logloss:0.021258 
## [1040]   train-logloss:0.021249 
## [1041]   train-logloss:0.021230 
## [1042]   train-logloss:0.021222 
## [1043]   train-logloss:0.021216 
## [1044]   train-logloss:0.021210 
## [1045]   train-logloss:0.021198 
## [1046]   train-logloss:0.021190 
## [1047]   train-logloss:0.021171 
## [1048]   train-logloss:0.021162 
## [1049]   train-logloss:0.021150 
## [1050]   train-logloss:0.021139 
## [1051]   train-logloss:0.021124 
## [1052]   train-logloss:0.021107 
## [1053]   train-logloss:0.021096 
## [1054]   train-logloss:0.021086 
## [1055]   train-logloss:0.021065 
## [1056]   train-logloss:0.021057 
## [1057]   train-logloss:0.021049 
## [1058]   train-logloss:0.021023 
## [1059]   train-logloss:0.021003 
## [1060]   train-logloss:0.020994 
## [1061]   train-logloss:0.020967 
## [1062]   train-logloss:0.020960 
## [1063]   train-logloss:0.020937 
## [1064]   train-logloss:0.020914 
## [1065]   train-logloss:0.020899 
## [1066]   train-logloss:0.020892 
## [1067]   train-logloss:0.020874 
## [1068]   train-logloss:0.020863 
## [1069]   train-logloss:0.020841 
## [1070]   train-logloss:0.020834 
## [1071]   train-logloss:0.020830 
## [1072]   train-logloss:0.020812 
## [1073]   train-logloss:0.020805 
## [1074]   train-logloss:0.020784 
## [1075]   train-logloss:0.020773 
## [1076]   train-logloss:0.020771 
## [1077]   train-logloss:0.020757 
## [1078]   train-logloss:0.020748 
## [1079]   train-logloss:0.020746 
## [1080]   train-logloss:0.020744 
## [1081]   train-logloss:0.020735 
## [1082]   train-logloss:0.020730 
## [1083]   train-logloss:0.020715 
## [1084]   train-logloss:0.020710 
## [1085]   train-logloss:0.020706 
## [1086]   train-logloss:0.020699 
## [1087]   train-logloss:0.020697 
## [1088]   train-logloss:0.020684 
## [1089]   train-logloss:0.020671 
## [1090]   train-logloss:0.020663 
## [1091]   train-logloss:0.020657 
## [1092]   train-logloss:0.020647 
## [1093]   train-logloss:0.020639 
## [1094]   train-logloss:0.020637 
## [1095]   train-logloss:0.020627 
## [1096]   train-logloss:0.020610 
## [1097]   train-logloss:0.020595 
## [1098]   train-logloss:0.020589 
## [1099]   train-logloss:0.020581 
## [1100]   train-logloss:0.020572 
## [1101]   train-logloss:0.020561 
## [1102]   train-logloss:0.020553 
## [1103]   train-logloss:0.020542 
## [1104]   train-logloss:0.020523 
## [1105]   train-logloss:0.020520 
## [1106]   train-logloss:0.020518 
## [1107]   train-logloss:0.020512 
## [1108]   train-logloss:0.020505 
## [1109]   train-logloss:0.020495 
## [1110]   train-logloss:0.020489 
## [1111]   train-logloss:0.020485 
## [1112]   train-logloss:0.020477 
## [1113]   train-logloss:0.020457 
## [1114]   train-logloss:0.020449 
## [1115]   train-logloss:0.020438 
## [1116]   train-logloss:0.020428 
## [1117]   train-logloss:0.020423 
## [1118]   train-logloss:0.020407 
## [1119]   train-logloss:0.020395 
## [1120]   train-logloss:0.020389 
## [1121]   train-logloss:0.020384 
## [1122]   train-logloss:0.020373 
## [1123]   train-logloss:0.020357 
## [1124]   train-logloss:0.020354 
## [1125]   train-logloss:0.020350 
## [1126]   train-logloss:0.020340 
## [1127]   train-logloss:0.020330 
## [1128]   train-logloss:0.020308 
## [1129]   train-logloss:0.020302 
## [1130]   train-logloss:0.020293 
## [1131]   train-logloss:0.020278 
## [1132]   train-logloss:0.020267 
## [1133]   train-logloss:0.020250 
## [1134]   train-logloss:0.020243 
## [1135]   train-logloss:0.020224 
## [1136]   train-logloss:0.020213 
## [1137]   train-logloss:0.020195 
## [1138]   train-logloss:0.020181 
## [1139]   train-logloss:0.020174 
## [1140]   train-logloss:0.020169 
## [1141]   train-logloss:0.020159 
## [1142]   train-logloss:0.020151 
## [1143]   train-logloss:0.020150 
## [1144]   train-logloss:0.020146 
## [1145]   train-logloss:0.020138 
## [1146]   train-logloss:0.020125 
## [1147]   train-logloss:0.020116 
## [1148]   train-logloss:0.020099 
## [1149]   train-logloss:0.020077 
## [1150]   train-logloss:0.020070 
## [1151]   train-logloss:0.020063 
## [1152]   train-logloss:0.020046 
## [1153]   train-logloss:0.020040 
## [1154]   train-logloss:0.020025 
## [1155]   train-logloss:0.020019 
## [1156]   train-logloss:0.020004 
## [1157]   train-logloss:0.019988 
## [1158]   train-logloss:0.019980 
## [1159]   train-logloss:0.019974 
## [1160]   train-logloss:0.019969 
## [1161]   train-logloss:0.019959 
## [1162]   train-logloss:0.019955 
## [1163]   train-logloss:0.019947 
## [1164]   train-logloss:0.019946 
## [1165]   train-logloss:0.019935 
## [1166]   train-logloss:0.019925 
## [1167]   train-logloss:0.019910 
## [1168]   train-logloss:0.019902 
## [1169]   train-logloss:0.019897 
## [1170]   train-logloss:0.019888 
## [1171]   train-logloss:0.019881 
## [1172]   train-logloss:0.019873 
## [1173]   train-logloss:0.019858 
## [1174]   train-logloss:0.019852 
## [1175]   train-logloss:0.019847 
## [1176]   train-logloss:0.019844 
## [1177]   train-logloss:0.019828 
## [1178]   train-logloss:0.019825 
## [1179]   train-logloss:0.019813 
## [1180]   train-logloss:0.019808 
## [1181]   train-logloss:0.019796 
## [1182]   train-logloss:0.019792 
## [1183]   train-logloss:0.019783 
## [1184]   train-logloss:0.019778 
## [1185]   train-logloss:0.019773 
## [1186]   train-logloss:0.019761 
## [1187]   train-logloss:0.019751 
## [1188]   train-logloss:0.019727 
## [1189]   train-logloss:0.019712 
## [1190]   train-logloss:0.019699 
## [1191]   train-logloss:0.019690 
## [1192]   train-logloss:0.019685 
## [1193]   train-logloss:0.019677 
## [1194]   train-logloss:0.019672 
## [1195]   train-logloss:0.019665 
## [1196]   train-logloss:0.019645 
## [1197]   train-logloss:0.019636 
## [1198]   train-logloss:0.019622 
## [1199]   train-logloss:0.019607 
## [1200]   train-logloss:0.019603 
## [1201]   train-logloss:0.019595 
## [1202]   train-logloss:0.019585 
## [1203]   train-logloss:0.019577 
## [1204]   train-logloss:0.019571 
## [1205]   train-logloss:0.019565 
## [1206]   train-logloss:0.019557 
## [1207]   train-logloss:0.019549 
## [1208]   train-logloss:0.019544 
## [1209]   train-logloss:0.019531 
## [1210]   train-logloss:0.019525 
## [1211]   train-logloss:0.019519 
## [1212]   train-logloss:0.019512 
## [1213]   train-logloss:0.019502 
## [1214]   train-logloss:0.019489 
## [1215]   train-logloss:0.019482 
## [1216]   train-logloss:0.019479 
## [1217]   train-logloss:0.019463 
## [1218]   train-logloss:0.019461 
## [1219]   train-logloss:0.019458 
## [1220]   train-logloss:0.019452 
## [1221]   train-logloss:0.019445 
## [1222]   train-logloss:0.019438 
## [1223]   train-logloss:0.019427 
## [1224]   train-logloss:0.019425 
## [1225]   train-logloss:0.019418 
## [1226]   train-logloss:0.019414 
## [1227]   train-logloss:0.019411 
## [1228]   train-logloss:0.019400 
## [1229]   train-logloss:0.019391 
## [1230]   train-logloss:0.019381 
## [1231]   train-logloss:0.019372 
## [1232]   train-logloss:0.019364 
## [1233]   train-logloss:0.019350 
## [1234]   train-logloss:0.019332 
## [1235]   train-logloss:0.019324 
## [1236]   train-logloss:0.019315 
## [1237]   train-logloss:0.019309 
## [1238]   train-logloss:0.019301 
## [1239]   train-logloss:0.019297 
## [1240]   train-logloss:0.019291 
## [1241]   train-logloss:0.019285 
## [1242]   train-logloss:0.019276 
## [1243]   train-logloss:0.019272 
## [1244]   train-logloss:0.019268 
## [1245]   train-logloss:0.019264 
## [1246]   train-logloss:0.019256 
## [1247]   train-logloss:0.019251 
## [1248]   train-logloss:0.019244 
## [1249]   train-logloss:0.019238 
## [1250]   train-logloss:0.019231 
## [1251]   train-logloss:0.019224 
## [1252]   train-logloss:0.019215 
## [1253]   train-logloss:0.019209 
## [1254]   train-logloss:0.019204 
## [1255]   train-logloss:0.019194 
## [1256]   train-logloss:0.019183 
## [1257]   train-logloss:0.019179 
## [1258]   train-logloss:0.019172 
## [1259]   train-logloss:0.019170 
## [1260]   train-logloss:0.019165 
## [1261]   train-logloss:0.019154 
## [1262]   train-logloss:0.019142 
## [1263]   train-logloss:0.019135 
## [1264]   train-logloss:0.019127 
## [1265]   train-logloss:0.019119 
## [1266]   train-logloss:0.019116 
## [1267]   train-logloss:0.019112 
## [1268]   train-logloss:0.019111 
## [1269]   train-logloss:0.019098 
## [1270]   train-logloss:0.019083 
## [1271]   train-logloss:0.019078 
## [1272]   train-logloss:0.019076 
## [1273]   train-logloss:0.019071 
## [1274]   train-logloss:0.019064 
## [1275]   train-logloss:0.019056 
## [1276]   train-logloss:0.019046 
## [1277]   train-logloss:0.019043 
## [1278]   train-logloss:0.019039 
## [1279]   train-logloss:0.019029 
## [1280]   train-logloss:0.019021 
## [1281]   train-logloss:0.019011 
## [1282]   train-logloss:0.019006 
## [1283]   train-logloss:0.019001 
## [1284]   train-logloss:0.018997 
## [1285]   train-logloss:0.018991 
## [1286]   train-logloss:0.018980 
## [1287]   train-logloss:0.018968 
## [1288]   train-logloss:0.018952 
## [1289]   train-logloss:0.018942 
## [1290]   train-logloss:0.018937 
## [1291]   train-logloss:0.018924 
## [1292]   train-logloss:0.018912 
## [1293]   train-logloss:0.018905 
## [1294]   train-logloss:0.018899 
## [1295]   train-logloss:0.018890 
## [1296]   train-logloss:0.018880 
## [1297]   train-logloss:0.018875 
## [1298]   train-logloss:0.018865 
## [1299]   train-logloss:0.018852 
## [1300]   train-logloss:0.018842 
## [1301]   train-logloss:0.018834 
## [1302]   train-logloss:0.018822 
## [1303]   train-logloss:0.018807 
## [1304]   train-logloss:0.018801 
## [1305]   train-logloss:0.018798 
## [1306]   train-logloss:0.018797 
## [1307]   train-logloss:0.018792 
## [1308]   train-logloss:0.018786 
## [1309]   train-logloss:0.018776 
## [1310]   train-logloss:0.018767 
## [1311]   train-logloss:0.018760 
## [1312]   train-logloss:0.018752 
## [1313]   train-logloss:0.018734 
## [1314]   train-logloss:0.018713 
## [1315]   train-logloss:0.018707 
## [1316]   train-logloss:0.018705 
## [1317]   train-logloss:0.018703 
## [1318]   train-logloss:0.018696 
## [1319]   train-logloss:0.018692 
## [1320]   train-logloss:0.018686 
## [1321]   train-logloss:0.018680 
## [1322]   train-logloss:0.018672 
## [1323]   train-logloss:0.018660 
## [1324]   train-logloss:0.018651 
## [1325]   train-logloss:0.018649 
## [1326]   train-logloss:0.018638 
## [1327]   train-logloss:0.018622 
## [1328]   train-logloss:0.018610 
## [1329]   train-logloss:0.018597 
## [1330]   train-logloss:0.018586 
## [1331]   train-logloss:0.018579 
## [1332]   train-logloss:0.018575 
## [1333]   train-logloss:0.018572 
## [1334]   train-logloss:0.018568 
## [1335]   train-logloss:0.018561 
## [1336]   train-logloss:0.018552 
## [1337]   train-logloss:0.018544 
## [1338]   train-logloss:0.018539 
## [1339]   train-logloss:0.018529 
## [1340]   train-logloss:0.018517 
## [1341]   train-logloss:0.018506 
## [1342]   train-logloss:0.018502 
## [1343]   train-logloss:0.018497 
## [1344]   train-logloss:0.018487 
## [1345]   train-logloss:0.018477 
## [1346]   train-logloss:0.018470 
## [1347]   train-logloss:0.018465 
## [1348]   train-logloss:0.018454 
## [1349]   train-logloss:0.018440 
## [1350]   train-logloss:0.018434 
## [1351]   train-logloss:0.018428 
## [1352]   train-logloss:0.018424 
## [1353]   train-logloss:0.018418 
## [1354]   train-logloss:0.018415 
## [1355]   train-logloss:0.018407 
## [1356]   train-logloss:0.018396 
## [1357]   train-logloss:0.018384 
## [1358]   train-logloss:0.018380 
## [1359]   train-logloss:0.018374 
## [1360]   train-logloss:0.018368 
## [1361]   train-logloss:0.018364 
## [1362]   train-logloss:0.018362 
## [1363]   train-logloss:0.018353 
## [1364]   train-logloss:0.018339 
## [1365]   train-logloss:0.018328 
## [1366]   train-logloss:0.018318 
## [1367]   train-logloss:0.018304 
## [1368]   train-logloss:0.018299 
## [1369]   train-logloss:0.018290 
## [1370]   train-logloss:0.018285 
## [1371]   train-logloss:0.018274 
## [1372]   train-logloss:0.018262 
## [1373]   train-logloss:0.018248 
## [1374]   train-logloss:0.018239 
## [1375]   train-logloss:0.018231 
## [1376]   train-logloss:0.018213 
## [1377]   train-logloss:0.018207 
## [1378]   train-logloss:0.018192 
## [1379]   train-logloss:0.018187 
## [1380]   train-logloss:0.018172 
## [1381]   train-logloss:0.018162 
## [1382]   train-logloss:0.018152 
## [1383]   train-logloss:0.018135 
## [1384]   train-logloss:0.018127 
## [1385]   train-logloss:0.018118 
## [1386]   train-logloss:0.018113 
## [1387]   train-logloss:0.018108 
## [1388]   train-logloss:0.018099 
## [1389]   train-logloss:0.018095 
## [1390]   train-logloss:0.018087 
## [1391]   train-logloss:0.018080 
## [1392]   train-logloss:0.018073 
## [1393]   train-logloss:0.018068 
## [1394]   train-logloss:0.018062 
## [1395]   train-logloss:0.018053 
## [1396]   train-logloss:0.018049 
## [1397]   train-logloss:0.018044 
## [1398]   train-logloss:0.018035 
## [1399]   train-logloss:0.018023 
## [1400]   train-logloss:0.018013 
## [1401]   train-logloss:0.018001 
## [1402]   train-logloss:0.017995 
## [1403]   train-logloss:0.017988 
## [1404]   train-logloss:0.017984 
## [1405]   train-logloss:0.017979 
## [1406]   train-logloss:0.017976 
## [1407]   train-logloss:0.017973 
## [1408]   train-logloss:0.017967 
## [1409]   train-logloss:0.017959 
## [1410]   train-logloss:0.017954 
## [1411]   train-logloss:0.017946 
## [1412]   train-logloss:0.017938 
## [1413]   train-logloss:0.017930 
## [1414]   train-logloss:0.017916 
## [1415]   train-logloss:0.017910 
## [1416]   train-logloss:0.017903 
## [1417]   train-logloss:0.017897 
## [1418]   train-logloss:0.017889 
## [1419]   train-logloss:0.017883 
## [1420]   train-logloss:0.017869 
## [1421]   train-logloss:0.017863 
## [1422]   train-logloss:0.017860 
## [1423]   train-logloss:0.017856 
## [1424]   train-logloss:0.017848 
## [1425]   train-logloss:0.017843 
## [1426]   train-logloss:0.017840 
## [1427]   train-logloss:0.017830 
## [1428]   train-logloss:0.017820 
## [1429]   train-logloss:0.017812 
## [1430]   train-logloss:0.017807 
## [1431]   train-logloss:0.017798 
## [1432]   train-logloss:0.017790 
## [1433]   train-logloss:0.017787 
## [1434]   train-logloss:0.017780 
## [1435]   train-logloss:0.017776 
## [1436]   train-logloss:0.017775 
## [1437]   train-logloss:0.017768 
## [1438]   train-logloss:0.017762 
## [1439]   train-logloss:0.017753 
## [1440]   train-logloss:0.017750 
## [1441]   train-logloss:0.017742 
## [1442]   train-logloss:0.017737 
## [1443]   train-logloss:0.017732 
## [1444]   train-logloss:0.017726 
## [1445]   train-logloss:0.017712 
## [1446]   train-logloss:0.017701 
## [1447]   train-logloss:0.017697 
## [1448]   train-logloss:0.017691 
## [1449]   train-logloss:0.017688 
## [1450]   train-logloss:0.017682 
## [1451]   train-logloss:0.017678 
## [1452]   train-logloss:0.017675 
## [1453]   train-logloss:0.017666 
## [1454]   train-logloss:0.017659 
## [1455]   train-logloss:0.017655 
## [1456]   train-logloss:0.017651 
## [1457]   train-logloss:0.017649 
## [1458]   train-logloss:0.017644 
## [1459]   train-logloss:0.017643 
## [1460]   train-logloss:0.017637 
## [1461]   train-logloss:0.017635 
## [1462]   train-logloss:0.017628 
## [1463]   train-logloss:0.017621 
## [1464]   train-logloss:0.017613 
## [1465]   train-logloss:0.017600 
## [1466]   train-logloss:0.017587 
## [1467]   train-logloss:0.017582 
## [1468]   train-logloss:0.017578 
## [1469]   train-logloss:0.017572 
## [1470]   train-logloss:0.017568 
## [1471]   train-logloss:0.017565 
## [1472]   train-logloss:0.017562 
## [1473]   train-logloss:0.017560 
## [1474]   train-logloss:0.017556 
## [1475]   train-logloss:0.017552 
## [1476]   train-logloss:0.017543 
## [1477]   train-logloss:0.017539 
## [1478]   train-logloss:0.017533 
## [1479]   train-logloss:0.017526 
## [1480]   train-logloss:0.017521 
## [1481]   train-logloss:0.017517 
## [1482]   train-logloss:0.017513 
## [1483]   train-logloss:0.017510 
## [1484]   train-logloss:0.017506 
## [1485]   train-logloss:0.017503 
## [1486]   train-logloss:0.017496 
## [1487]   train-logloss:0.017485 
## [1488]   train-logloss:0.017477 
## [1489]   train-logloss:0.017469 
## [1490]   train-logloss:0.017464 
## [1491]   train-logloss:0.017452 
## [1492]   train-logloss:0.017447 
## [1493]   train-logloss:0.017438 
## [1494]   train-logloss:0.017427 
## [1495]   train-logloss:0.017424 
## [1496]   train-logloss:0.017419 
## [1497]   train-logloss:0.017409 
## [1498]   train-logloss:0.017400 
## [1499]   train-logloss:0.017392 
## [1500]   train-logloss:0.017382 
## [1501]   train-logloss:0.017379 
## [1502]   train-logloss:0.017375 
## [1503]   train-logloss:0.017364 
## [1504]   train-logloss:0.017355 
## [1505]   train-logloss:0.017349 
## [1506]   train-logloss:0.017340 
## [1507]   train-logloss:0.017330 
## [1508]   train-logloss:0.017320 
## [1509]   train-logloss:0.017315 
## [1510]   train-logloss:0.017306 
## [1511]   train-logloss:0.017296 
## [1512]   train-logloss:0.017289 
## [1513]   train-logloss:0.017287 
## [1514]   train-logloss:0.017280 
## [1515]   train-logloss:0.017274 
## [1516]   train-logloss:0.017269 
## [1517]   train-logloss:0.017267 
## [1518]   train-logloss:0.017257 
## [1519]   train-logloss:0.017250 
## [1520]   train-logloss:0.017245 
## [1521]   train-logloss:0.017241 
## [1522]   train-logloss:0.017239 
## [1523]   train-logloss:0.017230 
## [1524]   train-logloss:0.017221 
## [1525]   train-logloss:0.017217 
## [1526]   train-logloss:0.017212 
## [1527]   train-logloss:0.017210 
## [1528]   train-logloss:0.017205 
## [1529]   train-logloss:0.017200 
## [1530]   train-logloss:0.017195 
## [1531]   train-logloss:0.017193 
## [1532]   train-logloss:0.017190 
## [1533]   train-logloss:0.017183 
## [1534]   train-logloss:0.017179 
## [1535]   train-logloss:0.017176 
## [1536]   train-logloss:0.017168 
## [1537]   train-logloss:0.017161 
## [1538]   train-logloss:0.017159 
## [1539]   train-logloss:0.017157 
## [1540]   train-logloss:0.017156 
## [1541]   train-logloss:0.017151 
## [1542]   train-logloss:0.017146 
## [1543]   train-logloss:0.017144 
## [1544]   train-logloss:0.017137 
## [1545]   train-logloss:0.017134 
## [1546]   train-logloss:0.017131 
## [1547]   train-logloss:0.017125 
## [1548]   train-logloss:0.017123 
## [1549]   train-logloss:0.017120 
## [1550]   train-logloss:0.017114 
## [1551]   train-logloss:0.017110 
## [1552]   train-logloss:0.017106 
## [1553]   train-logloss:0.017101 
## [1554]   train-logloss:0.017094 
## [1555]   train-logloss:0.017087 
## [1556]   train-logloss:0.017083 
## [1557]   train-logloss:0.017078 
## [1558]   train-logloss:0.017073 
## [1559]   train-logloss:0.017062 
## [1560]   train-logloss:0.017059 
## [1561]   train-logloss:0.017057 
## [1562]   train-logloss:0.017055 
## [1563]   train-logloss:0.017047 
## [1564]   train-logloss:0.017043 
## [1565]   train-logloss:0.017038 
## [1566]   train-logloss:0.017033 
## [1567]   train-logloss:0.017028 
## [1568]   train-logloss:0.017025 
## [1569]   train-logloss:0.017021 
## [1570]   train-logloss:0.017019 
## [1571]   train-logloss:0.017016 
## [1572]   train-logloss:0.017011 
## [1573]   train-logloss:0.017002 
## [1574]   train-logloss:0.016993 
## [1575]   train-logloss:0.016986 
## [1576]   train-logloss:0.016984 
## [1577]   train-logloss:0.016975 
## [1578]   train-logloss:0.016972 
## [1579]   train-logloss:0.016966 
## [1580]   train-logloss:0.016963 
## [1581]   train-logloss:0.016960 
## [1582]   train-logloss:0.016954 
## [1583]   train-logloss:0.016946 
## [1584]   train-logloss:0.016940 
## [1585]   train-logloss:0.016933 
## [1586]   train-logloss:0.016928 
## [1587]   train-logloss:0.016923 
## [1588]   train-logloss:0.016921 
## [1589]   train-logloss:0.016917 
## [1590]   train-logloss:0.016913 
## [1591]   train-logloss:0.016905 
## [1592]   train-logloss:0.016898 
## [1593]   train-logloss:0.016884 
## [1594]   train-logloss:0.016879 
## [1595]   train-logloss:0.016876 
## [1596]   train-logloss:0.016872 
## [1597]   train-logloss:0.016870 
## [1598]   train-logloss:0.016867 
## [1599]   train-logloss:0.016864 
## [1600]   train-logloss:0.016862 
## [1601]   train-logloss:0.016860 
## [1602]   train-logloss:0.016854 
## [1603]   train-logloss:0.016852 
## [1604]   train-logloss:0.016850 
## [1605]   train-logloss:0.016846 
## [1606]   train-logloss:0.016839 
## [1607]   train-logloss:0.016836 
## [1608]   train-logloss:0.016826 
## [1609]   train-logloss:0.016824 
## [1610]   train-logloss:0.016817 
## [1611]   train-logloss:0.016813 
## [1612]   train-logloss:0.016808 
## [1613]   train-logloss:0.016800 
## [1614]   train-logloss:0.016790 
## [1615]   train-logloss:0.016785 
## [1616]   train-logloss:0.016782 
## [1617]   train-logloss:0.016778 
## [1618]   train-logloss:0.016775 
## [1619]   train-logloss:0.016771 
## [1620]   train-logloss:0.016761 
## [1621]   train-logloss:0.016755 
## [1622]   train-logloss:0.016752 
## [1623]   train-logloss:0.016750 
## [1624]   train-logloss:0.016746 
## [1625]   train-logloss:0.016744 
## [1626]   train-logloss:0.016742 
## [1627]   train-logloss:0.016740 
## [1628]   train-logloss:0.016734 
## [1629]   train-logloss:0.016728 
## [1630]   train-logloss:0.016724 
## [1631]   train-logloss:0.016720 
## [1632]   train-logloss:0.016713 
## [1633]   train-logloss:0.016709 
## [1634]   train-logloss:0.016706 
## [1635]   train-logloss:0.016702 
## [1636]   train-logloss:0.016698 
## [1637]   train-logloss:0.016691 
## [1638]   train-logloss:0.016688 
## [1639]   train-logloss:0.016682 
## [1640]   train-logloss:0.016679 
## [1641]   train-logloss:0.016673 
## [1642]   train-logloss:0.016670 
## [1643]   train-logloss:0.016669 
## [1644]   train-logloss:0.016667 
## [1645]   train-logloss:0.016664 
## [1646]   train-logloss:0.016661 
## [1647]   train-logloss:0.016656 
## [1648]   train-logloss:0.016653 
## [1649]   train-logloss:0.016652 
## [1650]   train-logloss:0.016650 
## [1651]   train-logloss:0.016646 
## [1652]   train-logloss:0.016642 
## [1653]   train-logloss:0.016635 
## [1654]   train-logloss:0.016631 
## [1655]   train-logloss:0.016623 
## [1656]   train-logloss:0.016615 
## [1657]   train-logloss:0.016609 
## [1658]   train-logloss:0.016599 
## [1659]   train-logloss:0.016590 
## [1660]   train-logloss:0.016585 
## [1661]   train-logloss:0.016581 
## [1662]   train-logloss:0.016579 
## [1663]   train-logloss:0.016576 
## [1664]   train-logloss:0.016573 
## [1665]   train-logloss:0.016570 
## [1666]   train-logloss:0.016567 
## [1667]   train-logloss:0.016563 
## [1668]   train-logloss:0.016560 
## [1669]   train-logloss:0.016555 
## [1670]   train-logloss:0.016550 
## [1671]   train-logloss:0.016548 
## [1672]   train-logloss:0.016543 
## [1673]   train-logloss:0.016539 
## [1674]   train-logloss:0.016534 
## [1675]   train-logloss:0.016530 
## [1676]   train-logloss:0.016526 
## [1677]   train-logloss:0.016519 
## [1678]   train-logloss:0.016514 
## [1679]   train-logloss:0.016509 
## [1680]   train-logloss:0.016506 
## [1681]   train-logloss:0.016502 
## [1682]   train-logloss:0.016496 
## [1683]   train-logloss:0.016491 
## [1684]   train-logloss:0.016485 
## [1685]   train-logloss:0.016477 
## [1686]   train-logloss:0.016475 
## [1687]   train-logloss:0.016469 
## [1688]   train-logloss:0.016464 
## [1689]   train-logloss:0.016457 
## [1690]   train-logloss:0.016455 
## [1691]   train-logloss:0.016451 
## [1692]   train-logloss:0.016446 
## [1693]   train-logloss:0.016444 
## [1694]   train-logloss:0.016442 
## [1695]   train-logloss:0.016441 
## [1696]   train-logloss:0.016439 
## [1697]   train-logloss:0.016435 
## [1698]   train-logloss:0.016432 
## [1699]   train-logloss:0.016431 
## [1700]   train-logloss:0.016428 
## [1701]   train-logloss:0.016423 
## [1702]   train-logloss:0.016419 
## [1703]   train-logloss:0.016413 
## [1704]   train-logloss:0.016412 
## [1705]   train-logloss:0.016409 
## [1706]   train-logloss:0.016407 
## [1707]   train-logloss:0.016405 
## [1708]   train-logloss:0.016401 
## [1709]   train-logloss:0.016393 
## [1710]   train-logloss:0.016385 
## [1711]   train-logloss:0.016376 
## [1712]   train-logloss:0.016370 
## [1713]   train-logloss:0.016364 
## [1714]   train-logloss:0.016359 
## [1715]   train-logloss:0.016355 
## [1716]   train-logloss:0.016350 
## [1717]   train-logloss:0.016344 
## [1718]   train-logloss:0.016341 
## [1719]   train-logloss:0.016336 
## [1720]   train-logloss:0.016335 
## [1721]   train-logloss:0.016329 
## [1722]   train-logloss:0.016326 
## [1723]   train-logloss:0.016325 
## [1724]   train-logloss:0.016322 
## [1725]   train-logloss:0.016317 
## [1726]   train-logloss:0.016315 
## [1727]   train-logloss:0.016309 
## [1728]   train-logloss:0.016307 
## [1729]   train-logloss:0.016305 
## [1730]   train-logloss:0.016302 
## [1731]   train-logloss:0.016300 
## [1732]   train-logloss:0.016296 
## [1733]   train-logloss:0.016290 
## [1734]   train-logloss:0.016286 
## [1735]   train-logloss:0.016283 
## [1736]   train-logloss:0.016279 
## [1737]   train-logloss:0.016273 
## [1738]   train-logloss:0.016264 
## [1739]   train-logloss:0.016262 
## [1740]   train-logloss:0.016258 
## [1741]   train-logloss:0.016252 
## [1742]   train-logloss:0.016251 
## [1743]   train-logloss:0.016248 
## [1744]   train-logloss:0.016239 
## [1745]   train-logloss:0.016237 
## [1746]   train-logloss:0.016235 
## [1747]   train-logloss:0.016225 
## [1748]   train-logloss:0.016219 
## [1749]   train-logloss:0.016213 
## [1750]   train-logloss:0.016208 
## [1751]   train-logloss:0.016200 
## [1752]   train-logloss:0.016191 
## [1753]   train-logloss:0.016188 
## [1754]   train-logloss:0.016183 
## [1755]   train-logloss:0.016180 
## [1756]   train-logloss:0.016175 
## [1757]   train-logloss:0.016169 
## [1758]   train-logloss:0.016166 
## [1759]   train-logloss:0.016164 
## [1760]   train-logloss:0.016162 
## [1761]   train-logloss:0.016160 
## [1762]   train-logloss:0.016155 
## [1763]   train-logloss:0.016148 
## [1764]   train-logloss:0.016143 
## [1765]   train-logloss:0.016137 
## [1766]   train-logloss:0.016126 
## [1767]   train-logloss:0.016120 
## [1768]   train-logloss:0.016115 
## [1769]   train-logloss:0.016106 
## [1770]   train-logloss:0.016102 
## [1771]   train-logloss:0.016094 
## [1772]   train-logloss:0.016088 
## [1773]   train-logloss:0.016081 
## [1774]   train-logloss:0.016075 
## [1775]   train-logloss:0.016070 
## [1776]   train-logloss:0.016069 
## [1777]   train-logloss:0.016066 
## [1778]   train-logloss:0.016062 
## [1779]   train-logloss:0.016059 
## [1780]   train-logloss:0.016056 
## [1781]   train-logloss:0.016049 
## [1782]   train-logloss:0.016043 
## [1783]   train-logloss:0.016040 
## [1784]   train-logloss:0.016036 
## [1785]   train-logloss:0.016033 
## [1786]   train-logloss:0.016029 
## [1787]   train-logloss:0.016025 
## [1788]   train-logloss:0.016022 
## [1789]   train-logloss:0.016018 
## [1790]   train-logloss:0.016015 
## [1791]   train-logloss:0.016014 
## [1792]   train-logloss:0.016012 
## [1793]   train-logloss:0.016008 
## [1794]   train-logloss:0.016006 
## [1795]   train-logloss:0.016003 
## [1796]   train-logloss:0.016000 
## [1797]   train-logloss:0.015997 
## [1798]   train-logloss:0.015995 
## [1799]   train-logloss:0.015992 
## [1800]   train-logloss:0.015988 
## [1801]   train-logloss:0.015979 
## [1802]   train-logloss:0.015975 
## [1803]   train-logloss:0.015974 
## [1804]   train-logloss:0.015969 
## [1805]   train-logloss:0.015967 
## [1806]   train-logloss:0.015962 
## [1807]   train-logloss:0.015958 
## [1808]   train-logloss:0.015956 
## [1809]   train-logloss:0.015955 
## [1810]   train-logloss:0.015953 
## [1811]   train-logloss:0.015949 
## [1812]   train-logloss:0.015946 
## [1813]   train-logloss:0.015943 
## [1814]   train-logloss:0.015942 
## [1815]   train-logloss:0.015936 
## [1816]   train-logloss:0.015935 
## [1817]   train-logloss:0.015933 
## [1818]   train-logloss:0.015929 
## [1819]   train-logloss:0.015921 
## [1820]   train-logloss:0.015915 
## [1821]   train-logloss:0.015911 
## [1822]   train-logloss:0.015906 
## [1823]   train-logloss:0.015901 
## [1824]   train-logloss:0.015897 
## [1825]   train-logloss:0.015893 
## [1826]   train-logloss:0.015892 
## [1827]   train-logloss:0.015890 
## [1828]   train-logloss:0.015886 
## [1829]   train-logloss:0.015885 
## [1830]   train-logloss:0.015882 
## [1831]   train-logloss:0.015880 
## [1832]   train-logloss:0.015878 
## [1833]   train-logloss:0.015876 
## [1834]   train-logloss:0.015870 
## [1835]   train-logloss:0.015864 
## [1836]   train-logloss:0.015859 
## [1837]   train-logloss:0.015855 
## [1838]   train-logloss:0.015849 
## [1839]   train-logloss:0.015847 
## [1840]   train-logloss:0.015845 
## [1841]   train-logloss:0.015839 
## [1842]   train-logloss:0.015834 
## [1843]   train-logloss:0.015829 
## [1844]   train-logloss:0.015824 
## [1845]   train-logloss:0.015818 
## [1846]   train-logloss:0.015813 
## [1847]   train-logloss:0.015808 
## [1848]   train-logloss:0.015805 
## [1849]   train-logloss:0.015804 
## [1850]   train-logloss:0.015800 
## [1851]   train-logloss:0.015795 
## [1852]   train-logloss:0.015790 
## [1853]   train-logloss:0.015787 
## [1854]   train-logloss:0.015784 
## [1855]   train-logloss:0.015781 
## [1856]   train-logloss:0.015778 
## [1857]   train-logloss:0.015774 
## [1858]   train-logloss:0.015769 
## [1859]   train-logloss:0.015767 
## [1860]   train-logloss:0.015758 
## [1861]   train-logloss:0.015755 
## [1862]   train-logloss:0.015751 
## [1863]   train-logloss:0.015749 
## [1864]   train-logloss:0.015745 
## [1865]   train-logloss:0.015741 
## [1866]   train-logloss:0.015731 
## [1867]   train-logloss:0.015723 
## [1868]   train-logloss:0.015716 
## [1869]   train-logloss:0.015712 
## [1870]   train-logloss:0.015708 
## [1871]   train-logloss:0.015706 
## [1872]   train-logloss:0.015703 
## [1873]   train-logloss:0.015699 
## [1874]   train-logloss:0.015696 
## [1875]   train-logloss:0.015691 
## [1876]   train-logloss:0.015687 
## [1877]   train-logloss:0.015685 
## [1878]   train-logloss:0.015683 
## [1879]   train-logloss:0.015682 
## [1880]   train-logloss:0.015679 
## [1881]   train-logloss:0.015677 
## [1882]   train-logloss:0.015677 
## [1883]   train-logloss:0.015673 
## [1884]   train-logloss:0.015670 
## [1885]   train-logloss:0.015667 
## [1886]   train-logloss:0.015661 
## [1887]   train-logloss:0.015658 
## [1888]   train-logloss:0.015653 
## [1889]   train-logloss:0.015650 
## [1890]   train-logloss:0.015644 
## [1891]   train-logloss:0.015640 
## [1892]   train-logloss:0.015634 
## [1893]   train-logloss:0.015630 
## [1894]   train-logloss:0.015625 
## [1895]   train-logloss:0.015622 
## [1896]   train-logloss:0.015619 
## [1897]   train-logloss:0.015616 
## [1898]   train-logloss:0.015615 
## [1899]   train-logloss:0.015614 
## [1900]   train-logloss:0.015612 
## [1901]   train-logloss:0.015609 
## [1902]   train-logloss:0.015605 
## [1903]   train-logloss:0.015604 
## [1904]   train-logloss:0.015599 
## [1905]   train-logloss:0.015596 
## [1906]   train-logloss:0.015592 
## [1907]   train-logloss:0.015589 
## [1908]   train-logloss:0.015586 
## [1909]   train-logloss:0.015583 
## [1910]   train-logloss:0.015579 
## [1911]   train-logloss:0.015575 
## [1912]   train-logloss:0.015573 
## [1913]   train-logloss:0.015572 
## [1914]   train-logloss:0.015571 
## [1915]   train-logloss:0.015570 
## [1916]   train-logloss:0.015568 
## [1917]   train-logloss:0.015564 
## [1918]   train-logloss:0.015561 
## [1919]   train-logloss:0.015557 
## [1920]   train-logloss:0.015555 
## [1921]   train-logloss:0.015552 
## [1922]   train-logloss:0.015548 
## [1923]   train-logloss:0.015544 
## [1924]   train-logloss:0.015539 
## [1925]   train-logloss:0.015535 
## [1926]   train-logloss:0.015533 
## [1927]   train-logloss:0.015529 
## [1928]   train-logloss:0.015525 
## [1929]   train-logloss:0.015523 
## [1930]   train-logloss:0.015519 
## [1931]   train-logloss:0.015517 
## [1932]   train-logloss:0.015512 
## [1933]   train-logloss:0.015506 
## [1934]   train-logloss:0.015504 
## [1935]   train-logloss:0.015498 
## [1936]   train-logloss:0.015495 
## [1937]   train-logloss:0.015492 
## [1938]   train-logloss:0.015489 
## [1939]   train-logloss:0.015484 
## [1940]   train-logloss:0.015482 
## [1941]   train-logloss:0.015478 
## [1942]   train-logloss:0.015477 
## [1943]   train-logloss:0.015475 
## [1944]   train-logloss:0.015474 
## [1945]   train-logloss:0.015471 
## [1946]   train-logloss:0.015467 
## [1947]   train-logloss:0.015460 
## [1948]   train-logloss:0.015458 
## [1949]   train-logloss:0.015455 
## [1950]   train-logloss:0.015449 
## [1951]   train-logloss:0.015444 
## [1952]   train-logloss:0.015440 
## [1953]   train-logloss:0.015436 
## [1954]   train-logloss:0.015434 
## [1955]   train-logloss:0.015431 
## [1956]   train-logloss:0.015430 
## [1957]   train-logloss:0.015427 
## [1958]   train-logloss:0.015423 
## [1959]   train-logloss:0.015418 
## [1960]   train-logloss:0.015415 
## [1961]   train-logloss:0.015412 
## [1962]   train-logloss:0.015409 
## [1963]   train-logloss:0.015406 
## [1964]   train-logloss:0.015398 
## [1965]   train-logloss:0.015392 
## [1966]   train-logloss:0.015382 
## [1967]   train-logloss:0.015378 
## [1968]   train-logloss:0.015373 
## [1969]   train-logloss:0.015368 
## [1970]   train-logloss:0.015363 
## [1971]   train-logloss:0.015361 
## [1972]   train-logloss:0.015359 
## [1973]   train-logloss:0.015356 
## [1974]   train-logloss:0.015351 
## [1975]   train-logloss:0.015348 
## [1976]   train-logloss:0.015346 
## [1977]   train-logloss:0.015343 
## [1978]   train-logloss:0.015341 
## [1979]   train-logloss:0.015337 
## [1980]   train-logloss:0.015333 
## [1981]   train-logloss:0.015330 
## [1982]   train-logloss:0.015327 
## [1983]   train-logloss:0.015324 
## [1984]   train-logloss:0.015320 
## [1985]   train-logloss:0.015317 
## [1986]   train-logloss:0.015312 
## [1987]   train-logloss:0.015306 
## [1988]   train-logloss:0.015303 
## [1989]   train-logloss:0.015299 
## [1990]   train-logloss:0.015294 
## [1991]   train-logloss:0.015290 
## [1992]   train-logloss:0.015289 
## [1993]   train-logloss:0.015288 
## [1994]   train-logloss:0.015285 
## [1995]   train-logloss:0.015283 
## [1996]   train-logloss:0.015282 
## [1997]   train-logloss:0.015280 
## [1998]   train-logloss:0.015276 
## [1999]   train-logloss:0.015272 
## [2000]   train-logloss:0.015271</code></pre>
<pre class="r"><code>print(xgb2)</code></pre>
<pre><code>## ##### xgb.Booster
## raw: 7.4 Mb 
## call:
##   xgb.train(params = params, data = dtrain, nrounds = nrounds, 
##     watchlist = watchlist, verbose = verbose, print_every_n = print_every_n, 
##     early_stopping_rounds = early_stopping_rounds, maximize = maximize, 
##     save_period = save_period, save_name = save_name, xgb_model = xgb_model, 
##     callbacks = callbacks, booster = &quot;gbtree&quot;, max.depth = 10, 
##     objective = &quot;binary:logistic&quot;)
## params (as set within xgb.train):
##   booster = &quot;gbtree&quot;, max_depth = &quot;10&quot;, objective = &quot;binary:logistic&quot;, validate_parameters = &quot;TRUE&quot;
## xgb.attributes:
##   niter
## callbacks:
##   cb.print.evaluation(period = print_every_n)
##   cb.evaluation.log()
## # of features: 24 
## niter: 2000
## nfeatures : 24 
## evaluation_log:
##     iter train_logloss
##        1    0.59510259
##        2    0.53372440
## ---                   
##     1999    0.01527200
##     2000    0.01527055</code></pre>
<p>We choose the gbtree booster and binary:logistic objective for
training XGBoost model and comparing with Logistic regression model
later.</p>
</div>
<div id="testing-3" class="section level4">
<h4>Testing</h4>
<pre class="r"><code>xgbpred2 = round(predict(xgb2, dtest))
tablog = table(data_testy$label, xgbpred2, dnn=c(&quot;true&quot;,&quot;pre&quot;))
tablog</code></pre>
<pre><code>##     pre
## true   0   1
##    0  75 287
##    1 201 970</code></pre>
<p>From the table, we find: 1. the logistic model works well on
sufficient sleep. <span class="math inline">\(\frac{970}{970+201} =
82.84\%\)</span> 2. the logistic model works not well on sufficient
sleep. <span class="math inline">\(\frac{75}{75+287} = 20.72\%\)</span>
3. the total accuracy of the model prediction is <span
class="math inline">\(\frac{75+970}{75+287+970+201} =
68.17\%\)</span></p>
<p>Compared with logistic regression model, we find although the
correctly prediction ability of XGBoost under gbtree booster and
binary:logistic objective for sufficient sleep and the total are lower,
the model actually is much reasonable as it could predict larger
proportions of insufficient sleep cases with no extreme gap between the
prediction ability between two groups.</p>
<pre class="r"><code>xgb_imp2 = xgb.importance(feature_names = xgb2$feature_names, model = xgb2)
xgb.ggplot.importance(xgb_imp2, top_n =10, n_clusters = 2)</code></pre>
<p><img src="Data_files/figure-html/unnamed-chunk-32-1.png" width="768" /></p>
<p>The feature importance plot of XGBoost model indicates two extremely
important factors, income_poverty_ratio and age. Which differs from
linear models so that we want to explore the decision tree
structure.</p>
</div>
<div id="tree-plot" class="section level4">
<h4>Tree plot</h4>
<pre class="r"><code>xgb.plot.tree(model = xgb2, trees=1999, plot_width = 900)</code></pre>
<div id="htmlwidget-e23bcc9aa2b13a882bad" style="width:900px;height:576px;" class="grViz html-widget"></div>
<script type="application/json" data-for="htmlwidget-e23bcc9aa2b13a882bad">{"x":{"diagram":"digraph {\n\ngraph [layout = \"dot\",\n       rankdir = \"LR\"]\n\nnode [color = \"DimGray\",\n      style = \"filled\",\n      fontname = \"Helvetica\"]\n\nedge [color = \"DimGray\",\n     arrowsize = \"1.5\",\n     arrowhead = \"vee\",\n     fontname = \"Helvetica\"]\n\n  \"1\" [label = \"Tree 1999\nincome_poverty_ratio\nCover: 62.9733429\nGain: 0.000239511981\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"2\" [label = \"genderfemale\nCover: 2.51974988\nGain: 0.000889236573\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"3\" [label = \"income_poverty_ratio\nCover: 60.4535942\nGain: 0.00042200755\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"4\" [label = \"Leaf\nCover: 1.1346997\nValue: -0.00649295142\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"5\" [label = \"Leaf\nCover: 1.38505018\nValue: 0.00218060869\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"6\" [label = \"age\nCover: 4.25633192\nGain: 0.000606495887\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"7\" [label = \"income_poverty_ratio\nCover: 56.1972618\nGain: 0.000454759545\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"8\" [label = \"age\nCover: 2.50290895\nGain: 0.000761303178\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"9\" [label = \"Leaf\nCover: 1.75342321\nValue: 0.00573912729\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"10\" [label = \"age\nCover: 9.80618763\nGain: 0.0018956901\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"11\" [label = \"income_poverty_ratio\nCover: 46.3910713\nGain: 0.00111477321\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"12\" [label = \"Leaf\nCover: 1.29169559\nValue: 0.00342856953\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"13\" [label = \"Leaf\nCover: 1.21121323\nValue: -0.00438651629\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"14\" [label = \"income_poverty_ratio\nCover: 7.31453371\nGain: 0.000999105978\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"15\" [label = \"income_poverty_ratio\nCover: 2.4916544\nGain: 0.000509388454\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"16\" [label = \"Leaf\nCover: 1.26789522\nValue: 0.00674024131\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"17\" [label = \"income_poverty_ratio\nCover: 45.1231766\nGain: 0.000223552066\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"18\" [label = \"income_poverty_ratio\nCover: 4.25301218\nGain: 0.00269462331\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"19\" [label = \"education_levelcollege or AA degree\nCover: 3.06152129\nGain: 0.0018697253\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"20\" [label = \"Leaf\nCover: 1.44835281\nValue: 0.00656974083\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"21\" [label = \"Leaf\nCover: 1.04330146\nValue: -0.000733739464\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"22\" [label = \"income_poverty_ratio\nCover: 2.11389351\nGain: 0.00037438955\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"23\" [label = \"income_poverty_ratio\nCover: 43.009285\nGain: 0.000888135633\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"24\" [label = \"age\nCover: 2.74063253\nGain: 0.0015762226\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"25\" [label = \"Leaf\nCover: 1.51237965\nValue: 0.00692419615\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"26\" [label = \"Leaf\nCover: 1.72548771\nValue: -0.0121403635\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"27\" [label = \"Leaf\nCover: 1.33603358\nValue: 0.000926949724\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"28\" [label = \"Leaf\nCover: 1.11202657\nValue: 0.00106493745\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"29\" [label = \"Leaf\nCover: 1.00186682\nValue: -0.00505874353\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"30\" [label = \"Leaf\nCover: 1.14088988\nValue: 0.00606213277\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"31\" [label = \"sleepy_freq5-15 per month\nCover: 41.8683929\nGain: 0.000464290031\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"32\" [label = \"Leaf\nCover: 1.44494832\nValue: 0.00120314583\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"33\" [label = \"Leaf\nCover: 1.29568422\nValue: -0.010720876\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"34\" [label = \"income_poverty_ratio\nCover: 35.5099678\nGain: 0.00142374518\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"35\" [label = \"income_poverty_ratio\nCover: 6.35842705\nGain: 0.00171111012\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"36\" [label = \"Leaf\nCover: 1.0064764\nValue: -0.00840273593\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"37\" [label = \"education_levelhigh school graduate\nCover: 34.5034904\nGain: 0.00113918772\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"38\" [label = \"Leaf\nCover: 1.16683412\nValue: 0.00908000674\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"39\" [label = \"age\nCover: 5.19159269\nGain: 0.000993509195\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"40\" [label = \"income_poverty_ratio\nCover: 31.5898361\nGain: 0.00263882405\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"41\" [label = \"age\nCover: 2.91365457\nGain: 0.00704009179\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"42\" [label = \"genderfemale\nCover: 2.16317415\nGain: 0.000108649401\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"43\" [label = \"age\nCover: 3.02841854\nGain: 0.00141703419\", shape = \"rectangle\", fontcolor = \"black\", fillcolor = \"Beige\"] \n  \"44\" [label = \"Leaf\nCover: 1.33340657\nValue: 0.0101325326\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"45\" [label = \"Leaf\nCover: 30.2564297\nValue: -0.000329456729\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"46\" [label = \"Leaf\nCover: 1.04955447\nValue: -0.0176226739\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"47\" [label = \"Leaf\nCover: 1.8641001\nValue: 0.00576730724\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"48\" [label = \"Leaf\nCover: 1.10688221\nValue: 0.000441855722\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"49\" [label = \"Leaf\nCover: 1.05629182\nValue: 0.00459684944\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"50\" [label = \"Leaf\nCover: 1.09522605\nValue: -0.00930902828\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n  \"51\" [label = \"Leaf\nCover: 1.93319261\nValue: 0.0013697088\", shape = \"oval\", fontcolor = \"black\", fillcolor = \"Khaki\"] \n\"1\"->\"2\" [label = \"< 0.504999995\", style = \"bold\"] \n\"2\"->\"4\" [label = \"< 0.5\", style = \"bold\"] \n\"3\"->\"6\" [label = \"< 0.954999983\", style = \"bold\"] \n\"6\"->\"8\" [label = \"< 57.5\", style = \"bold\"] \n\"7\"->\"10\" [label = \"< 1.91499996\", style = \"bold\"] \n\"8\"->\"12\" [label = \"< 38.5\", style = \"bold\"] \n\"10\"->\"14\" [label = \"< 66.5\", style = \"bold\"] \n\"11\"->\"16\" [label = \"< 2.09499979\", style = \"bold\"] \n\"14\"->\"18\" [label = \"< 1.43499994\", style = \"bold\"] \n\"15\"->\"20\" [label = \"< 1.53499997\", style = \"bold\"] \n\"17\"->\"22\" [label = \"< 2.41499996\", style = \"bold\"] \n\"18\"->\"24\" [label = \"< 1.20500004\", style = \"bold\"] \n\"19\"->\"26\" [label = \"< 0.5\", style = \"bold\"] \n\"22\"->\"28\" [label = \"< 2.30499983\", style = \"bold\"] \n\"23\"->\"30\" [label = \"< 2.63500023\", style = \"bold\"] \n\"24\"->\"32\" [label = \"< 43.5\", style = \"bold\"] \n\"31\"->\"34\" [label = \"< 0.5\", style = \"bold\"] \n\"34\"->\"36\" [label = \"< 2.81500006\", style = \"bold\"] \n\"35\"->\"38\" [label = \"< 4.15499973\", style = \"bold\"] \n\"37\"->\"40\" [label = \"< 0.5\", style = \"bold\"] \n\"39\"->\"42\" [label = \"< 55.5\", style = \"bold\"] \n\"40\"->\"44\" [label = \"< 3.29500008\", style = \"bold\"] \n\"41\"->\"46\" [label = \"< 50.5\", style = \"bold\"] \n\"42\"->\"48\" [label = \"< 0.5\", style = \"bold\"] \n\"43\"->\"50\" [label = \"< 72.5\", style = \"bold\"] \n\"1\"->\"3\" [style = \"bold\", style = \"solid\"] \n\"2\"->\"5\" [style = \"solid\", style = \"solid\"] \n\"3\"->\"7\" [style = \"solid\", style = \"solid\"] \n\"6\"->\"9\" [style = \"solid\", style = \"solid\"] \n\"7\"->\"11\" [style = \"solid\", style = \"solid\"] \n\"8\"->\"13\" [style = \"solid\", style = \"solid\"] \n\"10\"->\"15\" [style = \"solid\", style = \"solid\"] \n\"11\"->\"17\" [style = \"solid\", style = \"solid\"] \n\"14\"->\"19\" [style = \"solid\", style = \"solid\"] \n\"15\"->\"21\" [style = \"solid\", style = \"solid\"] \n\"17\"->\"23\" [style = \"solid\", style = \"solid\"] \n\"18\"->\"25\" [style = \"solid\", style = \"solid\"] \n\"19\"->\"27\" [style = \"solid\", style = \"solid\"] \n\"22\"->\"29\" [style = \"solid\", style = \"solid\"] \n\"23\"->\"31\" [style = \"solid\", style = \"solid\"] \n\"24\"->\"33\" [style = \"solid\", style = \"solid\"] \n\"31\"->\"35\" [style = \"solid\", style = \"solid\"] \n\"34\"->\"37\" [style = \"solid\", style = \"solid\"] \n\"35\"->\"39\" [style = \"solid\", style = \"solid\"] \n\"37\"->\"41\" [style = \"solid\", style = \"solid\"] \n\"39\"->\"43\" [style = \"solid\", style = \"solid\"] \n\"40\"->\"45\" [style = \"solid\", style = \"solid\"] \n\"41\"->\"47\" [style = \"solid\", style = \"solid\"] \n\"42\"->\"49\" [style = \"solid\", style = \"solid\"] \n\"43\"->\"51\" [style = \"solid\", style = \"solid\"] \n}","config":{"engine":"dot","options":null}},"evals":[],"jsHooks":[]}</script>
<p>According to the XGBoost tree plot above, we can clearly discover how
the final tree model decide the values on each leaf which can directly
lead to probability estimators based on the logistic function.</p>
</div>
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
